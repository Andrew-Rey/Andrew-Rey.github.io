[{"title":"算法 | 快速傅里叶变换FFT","url":"/2022/04/06/Algorithm/fft/","content":"\"对称，万变不离其中\"\r\n\r\n多项式乘积问题\r\n首先来思考这样的一个问题:\r\n\r\nQuestion 1\r\n你有两个多项式函数:\r\n\r\n\r\n应该如何计算它们的乘积?\r\n\r\n当然, 我不是说要用笔算的方式, 而是用计算机.\r\n显然这个问题我们在小学二年级就写过的,\r\n当初正在学习\"数据结构\"这门课, 如果没记错, 应该是用链表实现的.\r\n但是, 就算是用链表实现, 那不也是和手算一样的原理吗?\r\n\r\n将二者相乘\r\n分配律\r\n合并同类项\r\n\r\n例如上面那个例子:\r\n\r\nsolution 1\r\n\r\n\r\n(???是什么动力让我深夜在这里口算多项式乘法???)\r\n显然, 如果一个 n 次多项式乘上一个 m 次多项式, 在合并同类项前应该有\r\n 次多项式, 这谁顶得住?\r\n对于正常人类而言显然顶不住, 对于计算机而言, 时间复杂度是, 也是算比较大的开销了吧.\r\n咋办?\r\n点表示法\r\n开始\r\n有谁规定, 我多项式一定是用系数表示的?\r\n好家伙, 你这样说我就摸不着头脑了,\r\n难道除了系数表示还有其他表示方法吗?\r\n首先, 多项式集合其实是构成了一个线性空间, 也就是说,\r\n任意两个多项式进行线性运算 (加法和数乘) 后, 结果仍然是多项式. 事实上\r\n\r\n构成了该空间的一组基, 将函数展开成 Taylor 级数便用了这组基作为基底,\r\n基前面的系数也就是坐标.\r\n其次, 对于一个 n 次多项式而言, 只要我们确定了它的坐标,\r\n就能唯一确定这个多项式. 现在的问题是不知道坐标,\r\n如何确定多项式. 这里的巧妙之处就在于, 多项式函数是一个映射,\r\n对于一个特定的 x, 总是能给出唯一一个值与之对应, 这不就是一个方程吗?\r\n我给你一个 x, 你输出一个值, 同时由于多项式系数全部未知,\r\n这就是一个关于  个系数的方程\r\n显然, 我需要 \r\n个不同的点来唯一确定我的系数. 这就是所谓的点表示法. 这样一来, 我们将这\r\n 个方程写成矩阵形式:\r\n\r\n看到这里我终于理解了为什么在学高等代数时要突然讲一个范德蒙德(Vandermonde)行列式,\r\n也就是这里的\r\n\r\n将上述矩阵定义为我们最喜欢的字母.\r\n好, 既然这东西是范德蒙德行列式, 那我们可以知道它行列式不为 0, 也就是说, 这个矩阵是可逆的, 也就是当我们取\r\n 个不同点时,\r\n确实是可以使方程组有唯一解, 也就是  个点可以唯一表示一个 n\r\n次多项式.\r\n乘法\r\n问题来了, 如何做乘法?\r\n我们有 n 次多项式和 m 次多项式做乘法, 得到的是一个  次多项式, 那么我们只要找到  个点即可, 也就是只要在 n 次多项式和\r\nm 次多项式中分别找  个点,\r\n这些点的横坐标 x 相等, 再将对应的函数值相乘即可.\r\n进一步\r\n现在, 我们知道了如何用点表示多项式, 以及如何用点表示进行乘法运算.\r\n但是仔细一想, 这种方法需要求解线性方程组, 这里的计算复杂度并不低.\r\n也就是从系数表示法到点表示法的转化过程带来的计算复杂度还是很高的.\r\n有什么方法可以进行简化吗?\r\n先等一等, 我们先来梳理我们用点表示求多项式乘法的思路:\r\n\r\nMainIdea\r\n\r\n将 n 次多项式和 m 次多项式分别从系数表示转化为点表示\r\n对应点相乘\r\n将得到的 \r\n个点表示的多项式转化为系数表示\r\n\r\n\r\n奇偶\r\n先来考虑简单的情况:\r\n\r\nQuestion 2\r\n多项式\r\n\r\n和多项式\r\n\r\n用点表示法相乘\r\n\r\n那我们当然是按部就班地进行乘法啦~ - 由于结果是 5 次多项式,\r\n因此对取 5 个点, 对取 5 个点.\r\n取点, 说得轻巧, 做起来倒是挺犹豫的.\r\n取什么样的点能满足要求呢? 或者得寸进尺地说,\r\n什么样的点能让效率更高呢? 注意到二次函数是对称的,\r\n那我们是不是只要取正的 2 个点, 就能知道负的 2 个点, 另外加一个原点?\r\n确实如此.\r\n那三次多项式呢? 照理来说,\r\n我们同样也是只要取一半的点就能知道另一半点的值(这里的\"一半\"针对正负而言),\r\n只不过要在函数值上添加负号, 何必呢? 还不如干脆 提出一个\r\nx, 然后不也变成了二次函数?\r\n事实上, 一般而言, 我们要用点表示法表示多项式, 可以用如下方法:\r\n\r\nMethod1\r\n 其中, 表示只含偶次的多项式函数, 表示只含奇次的多项式.\r\n\r\n这样, 我们只要在非负轴上取值就可以确定整个多项式, 取点的个数是\r\n原来的一半.\r\n甚至, 这里形成了一个 递归 算法: 分解后的不也是一个关于 x 的多项式吗?!\r\n那我继续啊, 把继续分解啊,\r\n大事化小, 小事化了.\r\n等等!\r\n我们的其实是, 这里每个都是非负的啊.\r\n未来我们只能在非负轴取值了, 也就是说, 分解为偶次多项式后,\r\n递归停止了.\r\n完蛋.\r\n复数域分解\r\n\"山重水复疑无路, 柳暗花明又一村\"\r\n看到标题就已经知道要怎么做了. 既然在实数域上无法继续分解,\r\n那为何不去复数域呢?\r\n在复数域上我们可以快乐地进行递归.\r\n如何个快乐法呢? 我们来细品:\r\n\r\n偶次多项式在复平面上的根\r\n为什么突然变成了 求根?\r\n从第二节中\"奇偶\", 我们可以选取对称的点,\r\n来减少选取点的个数(即原来的一半). 接着我们把任意 n\r\n次多项式分解成两个偶次多项式, 偶次多项式的好处在于容易选取对称的点.\r\n但是由于在实数范围内, 在对偶次多项式进行递归时会发生中断,\r\n于是我们扩展至复数域讨论分解.\r\n方便起见: 对于, 我们取作为特征点, 对于, 我们取作为三个特征点, 那对于, 我们应该怎样取点, 抛开不谈, 令, 由 代数基本定理,\r\n该方程在复数域上有 4 个 根,\r\n对于其它偶次多项式我们以此类推.\r\n就这样, 我们找到了一个简单的方法寻找所有需要的点, 进行递归.\r\n单位根\r\n写到这里, 我也感觉有点吃力, 关键是为什么一定就取了令呢?\r\n虽然但是, 确实是所谓的\"方便起见\", 这是因为, 取了\"1\",\r\n我们可以在复平面上的单位圆上讨论这个问题.\r\n在小学二年级我们就知道,\r\n的根可以用我们熟悉的的幂来表示, 即  这些个点在复平面单位圆上\r\n对称分布. 每递归一次, 单位根的数量减少一半,\r\n但保持对称性不变.\r\n确实方便.\r\n\r\n快速傅里叶变换(FFT)\r\n终于能正式地介绍世界上最美丽的算法了:\r\n快速傅里叶变换(FFT).\r\nFFT解决的是多项式从系数表示到点表示的过程中, 计算复杂度的问题.\r\n框架\r\n分解:\r\n\r\n递归:\r\n\r\n\r\n加和: \r\n\r\n\r\n返回\r\n时间复杂度为: \r\n一些数学\r\n\r\n\r\n我们在复数域上考虑, 令  (这是因为, 我们希望多项式在复数域上考虑时,\r\n我们可以在单位圆周上讨论. 其中表示我们取的第 k 个点, 刚好与 是对应的.)\r\n则线性方程组可以化为:\r\n\r\n其中\r\n 称为离散傅里叶变换矩阵(DFT)显然该矩阵是\r\n对称的 且 可逆, 其逆矩阵为: \r\n并且, 该逆矩阵看起来和原矩阵 一模一样! .\r\n\r\n结束了?\r\n当我们乐呵呵地把FFT转化为代码时, 开心的分解多项式, 然后选点, 相乘,\r\n等等! 你还没告诉我, 怎么从点表示转化回系数表示呢!\r\n这就是FFT对称的魅力了. 由点求系数,\r\n不过是矩阵求逆的过程:\r\n\r\n显然, 由于DFT和DFT逆矩阵具有相似的形式,\r\n我们完全可以用同一个函数完成快速傅里叶的正反变换!\r\n\r\n","categories":["Algorithm"],"tags":["CS","Algorithm"]},{"title":"C++编程 | 类模板","url":"/2023/02/28/CS/class-template/","content":"\"C++17类模板\"\r\n\r\n\r\n编译器用于创建类的模板: 自动生成类\r\n\r\n标准库\r\n类模板不是类, 是创建类的一种方式\r\n\r\n\r\n\r\n实例\r\n\r\n编译器从类模板中生成的类, 在第一次使用模板类型声明变量是,\r\n会创建类模板的一个实例, 以后定义同类型的变量时,\r\n会使用已创建的第一个实例. 在创建类模板时, 也可以不同时声明变量.\r\n数据的组织 独立于 对象类型\r\n\r\n\r\n类模板的定义\r\ntemplate&lt;typename T1, typename T2, Type Arg1&gt;class ClassName {    // template class definition};\r\n\r\n模板参数\r\n\r\n类型参数 typename\r\n\r\n实参总是类型: int, float...\r\n\r\n非类型参数 Type\r\n\r\n实参是整数类型的字面量: 200, 10...\r\n整数常量表达式\r\n指向对象的指针或引用, 函数指针或空指针\r\n\r\n模板\r\n\r\n实参是类模板的一个实例\r\n\r\n\r\n\r\n\r\n在模板定义中, 不需要使用完整的ID, 例如构造函数\r\nClassName&lt;T1&gt;();可以写成ClassName();\r\n不过在模板体的外部标识模板, 则必须使用模板ID\r\n(即在模板类外定义模板中的成员函数时需要显式写出ID)\r\n\r\n一个例子\r\ntemplate&lt;typename T1&gt;class PythonList {private:    int len_;    int size_;    T1* elements_;public:    explicit PythonList&lt;T1&gt;(size_t list_len);    PythonList&lt;T1&gt;(const PythonList&lt;T1&gt;&amp; python_list);    ~PythonList();    T1&amp; operator[](size_t index);    const T1&amp; operator[](size_t index) const;    PythonList&lt;T1&gt;&amp; operator=(const PythonList&lt;T1&gt;&amp; rhs_list);    size_t get_len() const { return len_; }    void allocate_double();};\r\n类模板成员函数的定义\r\n\r\n若在模板类的内部定义, 实则为 内联\r\n\r\n\r\n如何理解该语法\r\n\r\n类模板的成员函数的外部定义本身就是函数模板,\r\n即使成员函数不依赖类型参数.\r\n若函数没有在类内定义, 则它需要一个模板定义.\r\n定义函数模板中的参数列表必须与类模板参数列表相同.\r\n\r\n\r\n例如\r\n// 析构函数template &lt;typename T1&gt;PythonList&lt;T1&gt;::~PythonList&lt;T1&gt;() {    delete [] elements_;}// 构造函数template &lt;typename T1&gt;PythonList&lt;T1&gt;::PythonList(size_t list_len)    : len_(list_len), size_(FOLD * list_len), elements_(new T1(list_len)) {}template &lt;typename T1&gt;PythonList&lt;T1&gt;::PythonList(const PythonList&lt;T1&gt; &amp;python_list)    : PythonList{python_list.len_} {  for (size_t i{}; i &lt; len_; ++i) {    elements_[i] = python_list.elements_[i];  }}// 下标运算符template &lt;typename T1&gt; T1 &amp;PythonList&lt;T1&gt;::operator[](size_t index) {  if (index &gt;= len_) {    throw std::out_of_range{\"Index out of range: \" + std::to_string(index)};  }  return elements_[index];}template &lt;typename T1&gt;const T1 &amp;PythonList&lt;T1&gt;::operator[](size_t index) const {  if (index &gt;= len_) {    throw std::out_of_range{\"Index out of range: \" + std::to_string(index)};  }  return elements_[index];}// 赋值运算符template &lt;typename T1&gt;PythonList&lt;T1&gt; PythonList&lt;T1&gt;::operator=(const PythonList&lt;T1&gt;&amp; rhs_list) {    if (&amp;rhs_list != this) {        delete [] elements_;        len_ = rhs_list.len_;        size_ = rhs_list.size_;        elements_ = new T1[len_];        for (size_t i {}; i &lt; size_; ++i) {            elements_[i] = rhs_list.elements_[i];        }    }    return *this;}\r\n\r\n第一行说明该函数为模板函数; 在限定成员函数时,\r\n作用域需要带上模板ID\r\n有时候需要提供自己的拷贝构造(或析构),\r\n因为涉及到动态内存分配时, 默认拷贝构造(或析构)有可能会出现负面效应\r\n在赋值重载时, 需要 检查左右操作数是否相等,\r\n否则会释放this指向的对象后再进行复制.\r\n\r\n代码重复\r\n\r\n在上述的定义中,\r\nconst的重载和非const的重载模板函数代码重复,\r\n代码重复不利于后续的维护\r\n\r\n对抗重复的方法: 函数, 模板, 基类\r\n\r\n\r\n传统方法:\r\n用const实现非const\r\ntemplate &lt;typename T1&gt;T1&amp; PythonList&lt;T1&gt;::operator[](size_t index) {    return const_cast&lt;T1&amp;&gt;(static_cast&lt;const PythonList&lt;T1&gt;&amp;&gt;(*this) [index]);}\r\nC++17:\r\nstd::as_const()(utility头文件)\r\ntemplate &lt;typename T1&gt;T1&amp; PythonList&lt;T1&gt;::operator[](size_t index) {    return const_cast&lt;T1&amp;&gt;(std::as_const(*this)[index]);}\r\n异常安全性\r\n\r\n在赋值运算符重载的时候, 由于使用了new,\r\n可能会出现std::bad_alloc异常\r\n在elements_[i] = rhs_list.elements_[i];可能会出现关于类型T1的赋值异常\r\n\r\n\r\n当声明了noexcept后, 表示代码内部不发生异常,\r\n使得编译器能做更多的优化,\r\n例如大部分析构都隐式声明了noexcept cppreference noexcept\r\n\r\n\r\n在以上的赋值运算符中使用 复制后交换\r\n\r\n定义模板类注意\r\n\r\n\r\n成员函数模板与类模板的定义放在同一个文件中: 当编译器生成类模板时,\r\n需要去使用函数模板, 所以在使用模板的源文件中,\r\n这些成员函数的定义必须可用.\r\n\r\n\r\n类模板实例化\r\nPythonList&lt;double&gt; data {10};\r\n\r\n编译器只编译程序使用的成员函数,\r\n不会为某个模板参数的实例而一次性编译整个类:\r\n例如上述代码编译后的类中只有构造函数和析构函数.\r\n\r\n\r\n声明对象类型的指针 不会 创建模板实例:\r\nPythonList&lt;std::string&gt;* data_p;\r\n\r\n非类型的类模板参数\r\n\r\n主要用于定义指定容器有效的值, 如数组的维数\r\n\r\n非类型参数只能是整数类型 (size_t,\r\nlong), 枚举类型, 对象的指针或引用, 函数的指针或引用,\r\n类成员的指针\r\n当作常量\r\n\r\n\r\ntemplate&lt;typename T1, size_t size&gt;class ClassName {    // definition};// 还有一些比较无语的template&lt;typename T1, T1 value&gt; ... // 此时T1只能是模板的非类型参数所允许的类型\r\n\r\n注意\r\n只有模板参数完全相同的情况下, 编译器才不会再次编译模板类;\r\n任意一个不同, 编译器都会认为是不同的类, 后果是代码膨胀\r\n\r\n解决方法 (待定)\r\n\r\n\r\n模板参数的默认值\r\n\r\n与函数的默认参数类似\r\n\r\n如果某个模板参数有默认值, 则后续的参数也必须有默认值\r\n如果某个模板参数的实参被省略, 则后续的所有实参也必须省略\r\n不需要在成员函数的模板中指定默认值\r\n\r\n\r\ntemplate&lt;typename T1 = int, int value = 10&gt; ...\r\n模板的显式实例化\r\ntemplate class ClassName&lt;T1, 10, ...&gt;\r\n\r\n编译器会从模板中实例化所有的成员函数, 无论是否调用\r\n\r\n类模板特化\r\n模板的使用中有时候只对 某些类型 有用,\r\n而不支持其他类型; 因此使用 特化 来处理某些特殊情况.\r\n例如整型变量的相等和浮点型的比较并不相同,\r\n这时可以使用模板的特化来处理.\r\n\r\n对于类模板中的成员函数:\r\n\r\n如果成员函数是在类模板的外部定义的, 而不是在类模板体中定义的,\r\n则可以提供函数模板的特化\r\n\r\n\r\n全特化\r\n即规定模板实现的所有模板参数\r\ntemplate&lt;&gt;class PythonList&lt;const char*&gt;{};\r\n\r\n特化的定义必须放在原有的定义或声明后面.\r\n因为指定了所有参数, 所以是 全特化\r\n\r\n偏特化\r\n即只规定模板参数列表中的一部分模板参数\r\ntemplate&lt;Type value&gt;class PythonList&lt;const char*, value&gt; {};\r\n\r\ntemplate后的参数列表包含的是为这个模板特化的实例所指定的参数,\r\n即实例化时需要指定value\r\n模板名后面的尖括号指定原有类模板定义中的参数如何特化.\r\n该参数列表必须与原来未特化的类模板个数相同\r\n\r\n\r\n指针类型的偏特化\r\n例如下面代码: template的第一个参数仍是T1,\r\n但模板名后面可以跟着T*\r\n\r\ntemplate&lt;typename T1, Type value&gt;class ClassName&lt;T1*, value&gt; {};\r\n\r\n特化的选择\r\n当匹配给定特化的每个实参匹配多个特化时, 编译器会选择\r\n最特殊 的一个特化.\r\n\r\n特殊是指有多个匹配, 如果符合A特化, 也符合B特化, 但反过来不行时,\r\n则A比B更特殊 (A含于B)\r\n\r\n\r\n在类模板中使用static_assert()\r\nstatic_assert()接受两个参数,\r\n第一个参数为false时, 输出第二个参数指定的消息.\r\n第一个实参使用type_traits.h中的模板\r\n\r\n\r\ntype_traits\r\n\r\n类模板的友元\r\n对于友元函数和友元类的情况与一般情况相同\r\n\r\n模板友元\r\n\r\n类模板的参数列表一般包含定义友元模板的所有参数\r\n如果类模板的一些参数在友元模板中没有,\r\n则友元模板的实例会用于类模板的几个实例\r\n普通类若有友元模板, 则友元的每一个实例都是这个类的友元\r\n\r\n\r\n","categories":["CS"],"tags":["CS","C++"]},{"title":"C++编程 | CMake Tutorial","url":"/2022/08/10/CS/cmake/","content":"CMake version: 3.x\r\n\r\nCommand Line\r\n# (configure step) create build dir, and generate build/Makefile -&gt; generate Makefilecmake -B build# (build step) invoke building system and build the project in different OS -&gt; generate executable filecmake --build build -j4# invoke building system to execute target &quot;install&quot;cmake --build build --target install# define configure variables, only use in configure step# use -D# set build type in configure step, the value will remain when invoked the second time unless delete build dircmake -B build -DCMAKE_BUILD_TYPE=Release# Specify generator (generator: generate build system build rule from CMakeLists.txt)# use -G# generator Ninja, faster than Unix Makefile, generate *.ninjacmake -B build -G Ninja\r\nCMakeLists.txt\r\nadd source file\r\n(1). single file: main.cpp\r\nadd_executable(main main.cpp)\r\nor\r\nadd_executable(main)target_sources(main PUBLIC main.cpp)\r\n(2). multiple files: main.cpp | other.cpp | other.h\r\nadd_executable(main)target_sources(main PUBLIC main.cpp other.cpp)\r\nor set a new variable\r\nadd_executable(main)set(sources main.cpp other.cpp other.h)  # other.h can deletetarget_sources(main PUBLIC $&#123;sources&#125;)\r\nor use GLOB to search all files in current dir\r\nadd_executable(main)file(GLOB sources CONFIGURE_DEPENDS *.cpp *.h)  # add CONFIGURE_DEPENDS to detect any change when next buildtarget_sources(main PUBLIC $&#123;sources&#125;)\r\nwhen we have a dir structure:\r\nmylib  +----*.cpp  +----*.h*.cpp*.h\r\nno need to write all files:\r\n# add all file in current dir and mylib diradd_executable(main)aux_source_directory(. sources)aux_source_directory(mylib sources)target_sources(main PUBLIC $&#123;sources&#125;)\r\nor use GLOB_RECURSE to find all files recursely:\r\nadd_executable(main)file(GLOB_RECURSE sources CONFIGURE_DEPENDS *.cpp *.h)target_sources(main PUBLIC $&#123;sources&#125;)\r\nERROR: use GLOB_RECURSE will include *.cpp files in\r\nbuild dir.\r\nsolution: Add all source files in a dir named\r\nsrc\r\nConfigure variables\r\nCMAKE_BUILD_TYPE: type of build,\r\nRelease, Debug,\r\nMinSizeRel and RelWithDebInfo,\r\ndefualt: none (debug).\r\nset(CMAKE_BUILD_TYPE Release)\r\nset default build type as Release to reach high performance: in the\r\nfirst three lines:\r\nif (NOT CMAKE_BUILD_TYPE)    set(CMAKE_BUILD_TYPE Release)endif()\r\n# Specify version of cmakecmake_minimum_required(VERSION 3.22)# set c++ standard# don&#x27;t modify CMAKE_CXX_FLAGS to add -std=c++17set(CMAKE_CXX_STANDARD 17)# if use the needed CXX standard defined.set(CMAKE_CXX_STANDARD_REQUIRED ON)  # OFF default# prevent features GCC onlyset(CMAKE_CXX_EXTENSIONS OFF)# set project infoproject(project_name LANGUAGES language_list(such as C CXX ASM...))\r\nLinkable library\r\nadd_executable(main mian.cpp mylib.cpp)\r\nor generate a static library\r\nadd_library(mylib STATIC mylib.cpp)  # create libmylib.aadd_executable(main main.cpp)target_link_libraries(main PUBLIC mylib)\r\nor generate dynamic lib\r\nadd_library(mylib SHARED mylib.cpp)add_executable(main main.cpp)target_link_libraries(main PUBLIC mylib)\r\nor use object lib, no *.a file, let CMake remember which objects\r\nfiles are created\r\nadd_library(mylib OBJECT mylib.cpp)add_executable(main main.cpp)target_link_libraries(main PUBLIC mylib)\r\n静态库问题: GCC会自行剔除没有引用符号的对象, 此时使用对象库避免,\r\n从而不会自动剔除没引用到的对象文件, 绕开编译器不统一问题.\r\n动态库也可以避免剔除没引用的对象文件, 但引入了运行时链接的麻烦.\r\n# no specify variable in add_library()set(BUILD_SHARED_LIBS ON)  # default OFFadd_library(mylib mylib.cpp)\r\nHINT 静态库常常被认为直接链接到可执行文件上.\r\n因此在动态库中不要链接静态库. 很呆. 地址会变.\r\n当然解决方法是: 要么转化为对象库,\r\n要么让静态库变成地址无关的代码PIC\r\n# set global propertyset(CMAKE_POSITION_INDEPENDENT_CODE ON)add_library(otherlib STATIC otherlib.cpp)add_library(mylib SHARED mylib.cpp)target_link_libraries(mylib PUBLIC otherlib)add_executable(main main.cpp)target_link_libraries(main PUBLIC mylib)\r\nor set local property\r\n# set local propertyadd_library(otherlib STATIC otherlib.cpp)set_property(TARGET otherlib PROPERTY POSITION_INDEPENDENT_CODE ON)add_library(mylib SHARED mylib.cpp)target_link_libraries(mylib PUBLIC otherlib)add_execuable(main main.cpp)target_link_libraries(main PUBLIC mylib)\r\nAttributes of objects\r\n设置单属性: set_property(TARGET ... PROPERTY ...);\r\n设置多属性:\r\nset_target_properties(file_name PROPERTIES properties_list)\r\nHINT:\r\n以上命令在add_executable后有效.\r\n设置全局属性 (改变属性的默认值): set(CMAKE_XXX),\r\n在add_executable前设置.\r\n\r\n如果需要在Windows下面使用动态库 (Windows对动态链接不友好),\r\n则需要在定义和声明添加: Deffinition: #include &lt;cstdio&gt;#ifdef _MSC_VER__declspec(dllexport)#endifvoid sayy_hello()&#123;&#125;\r\nDeclaration: #pragma once#ifdef _MSC_VER__declspec(dllimport)#endifvoid say_hello(); 然后CMakeLists.txt这样写:\r\n# In Main dircmake_minimum_required(VERSION 3.22)add_subdirectory(mylib)  # add sub moduleadd_executable(main main.cpp)target_link_libraries(main PUBLIC mylib)# In sub module diradd_library(mylib SHARED mylib.cpp mylib.h) 然后Windows极有可能会报错: 运行时找不到dll;\r\n原因是dll和exe不在同一目录 (Windows只会查找exe所在目录和PATH). -\r\n把dll添加到PATH环境变量 - 或者dll和dll其他的所有依赖dll,\r\n全部拷贝到exe同一目录\r\n这是因为CMake把main放在build下, 而mylib放在build/mylib/mylib.dll\r\n\r\n因此重定向输出路径, 改变mylib属性, 让.dll文件输出到\r\nPROJECT_BINARY_DIR 里面. set_property(TARGET mylib PROPERTY RUNTIME_OUTPUT_DIRECTORY_(DEBUG | RELEASE | NONE) | ARCHIVE_OUTPUT_DIRECTORY | LIBRARY_OUTPUT_DIRECTORY $&#123;PROJECT_BINARY_DIR&#125;)\r\nExternel library\r\nIn Linux: feel free to link externel libraries. (/usr/lib/...) But\r\nWindows can't. Linux can also include head file directly\r\n(/usr/include/...).\r\nHINT: CMake 的分隔符永远是 \"/\", 即使是Windows,\r\nCMake会自动转化.\r\nMore general method: find_package(package_name REQUIRED)\r\n没听懂, 以后补, 以后也不想补.\r\nVariables and Outputs\r\noutput some log infomation when running cmake -B build,\r\nused for debugging. message(&quot;log info&quot;)\r\nmessage(STATUS &quot;status info&quot;)  # -- prefix\r\nmessage(WARNING &quot;warning info&quot;)  # yellow\r\nmessage(SEND_ERROR &quot;error info&quot;)  # send error log but continue to runmessage(FATAL_ERROR &quot;error info&quot;)  # print error and stop running\r\nVariable and Cache\r\n重复执行cmake -B build: 第一次较慢,\r\n将环境的检测存入缓存, 第二次以及以后直接查看缓存内容.\r\n因此某些错误可以通过删除 ./build/CMakeCache.txt解决.\r\n当然也可以删了整个build文件夹重新编译, 慢一点而已.\r\n","categories":["CS"],"tags":["C++","Programming"]},{"title":"深度学习 | Image Semantic Segmentation based on UNet","url":"/2022/08/21/ML/UNet/","content":"\"Semantic segmentation of images, use UNet model.\"\r\n\r\nAbstract\r\nIn this project, we realize an basic UNet model and UNet++ model,\r\nthen we apply them on image semantic segmentation. We show our basic\r\ntheory of UNet and an improvement of it, and we provide main code of\r\nthis program. Finally, we give the result of segmentation images,\r\nloss-curve and accuracy-curve on both training and validation set.\r\nThe copyright of this program is owned by our team mentioned on the\r\nend of this blog.\r\nUNet Structure\r\nThe paper published in\r\n2015 propose a noval network structure, whose shape is similar with the\r\ncaptal \"U\". The idea comes from FCNN. U-Net is one of the classes of\r\n\"Encoder-Decoder\" structure.\r\n\r\n\r\nU-Net Structure\r\n\r\nThe front half of the network is \"encoder\". The input image passes\r\ncovolutional kernel, and then passes the pooling layer (or other\r\ndimension-decreasing layer). The opposite of that is the back part of\r\nUNet, the \"decoder\". The input of decoder is a sequence of feature maps\r\nwith highly contracted pixels. The output of the decoder (or the whole\r\nnetwork) is an image with the same shape of input image, where each\r\npixel has its own class.\r\nIn this project, we decrease the number of convolutional layers so\r\nthat there are only two convolutional layers in each convolutional\r\nkernel as the dataset includes images with shape .\r\nOperator Definitions\r\nConvolutional Kernel:\r\nWe define the basic convolutional kernel as follow:\r\nself.layer = nn.Sequential(    # in_channel, out_channel, kernel_size, stride, padding    # batch size * channel * height * weight    nn.Conv2d(C_in, C_out, kernel_size=(3, 3), stride=(1, 1), padding=1),  # 64 64 128 256    nn.BatchNorm2d(C_out),    nn.Dropout(0.2),    nn.LeakyReLU(),    nn.Conv2d(C_out, C_out, kernel_size=(3, 3), stride=(1, 1), padding=1),  # 64 64 128 256    nn.BatchNorm2d(C_out),    nn.Dropout(0.5),    nn.LeakyReLU(),\r\nIt includes two convolution operations.\r\nDown Sampling Kernel:\r\nAs for downsampling kernel, we replace conditional pooling layer to\r\nconvolutional layer with stride equaling to 2, which means the shape\r\nwill be shrunk to  while\r\nremaining the same channels.\r\nself.Down = nn.Sequential(    nn.Conv2d(C, C, kernel_size=(3, 3), stride=(2, 2), padding=1),  # 64 64 64 128    nn.LeakyReLU()        )\r\nUp Sampling Kernel:\r\nThe basic structure of up-sampling contains only one convolutional\r\nlayer with  convolutional\r\nkernel size and half out-channel. The feature map should pass an\r\ninterpolation layer before getting into the convolutional layer.\r\ndef __init__(self, C):    super(UpSampling, self).__init__()    # out-channel = 1/2 in-channel    self.Up = nn.Conv2d(C, C // 2, kernel_size=(1, 1), stride=(1, 1))    def forward(self, x, r):    # neighbor interpolation    up = F.interpolate(x, scale_factor=2, mode=\"nearest\")    x = self.Up(up)    # concatenate the feature map in encoder and     # the feature map in corrsponding decoder layer, in channel dimension    res = torch.cat((x, r), 1)    return res\r\nThe interpolation mode we choose is \"nearest\". The function\r\ntorch.cat(dim=1) is used to concatenate two feature maps in\r\nchannel dimension.\r\nNetwork Definition\r\nBased on the operators defined above, we link these blocks together\r\nlike UNet structure.\r\ndef __init__(self):    super(UNet, self).__init__()    # down sampling    self.C1 = Conv(3, 64)    self.D1 = DownSampling(64)    self.C2 = Conv(64, 128)    self.D2 = DownSampling(128)    self.C3 = Conv(128, 256)    self.D3 = DownSampling(256)    self.C4 = Conv(256, 512)    self.D4 = DownSampling(512)    self.C5 = Conv(512, 1024)    # up sampling    self.U1 = UpSampling(1024)    self.C6 = Conv(1024, 512)    self.U2 = UpSampling(512)    self.C7 = Conv(512, 256)    self.U3 = UpSampling(256)    self.C8 = Conv(256, 128)    self.U4 = UpSampling(128)    self.C9 = Conv(128, 64)    self.C10 = torch.nn.Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=1)    self.pred = torch.nn.Conv2d(3, 34, kernel_size=(1, 1), stride=(1, 1))    self.Th = torch.nn.Sigmoid()\r\nLike U-Net mentioned in that paper, we designed 4 layer deep\r\nnetwork.\r\ndef forward(self, x):        # part 1: down sampling, decreasing dimension        R1 = self.C1(x)        R2 = self.C2(self.D1(R1))        R3 = self.C3(self.D2(R2))        R4 = self.C4(self.D3(R3))        Y1 = self.C5(self.D4(R4))        # part 2: up sampling, connect priori knowledge        O1 = self.C6(self.U1(Y1, R4))        O2 = self.C7(self.U2(O1, R3))        O3 = self.C8(self.U3(O2, R2))        O4 = self.C9(self.U4(O3, R1))        # part 3: active function        return self.Th(self.pred(self.C10(O4)))\r\nAs you can see, the difference between U-Net and other networks\r\nbefore U-Net is that U-Net conbines the former information from encoder\r\nand current information from decoder.\r\nCode\r\nDuring the training process, we want to keep some information of loss\r\nvalues and accuracy values on training set and validation set so that we\r\ncan analyze the variance.\r\nIn the function named train(), we take\r\noptimizer and loss as two parameters used in\r\ntraining process. The outputs of this function are loss and accuracy on\r\nboth training set and validation set. If we get the data about training\r\nset and validation set, we can draw the curves. If both training and\r\nvalidation loss values decrease during training process, we can conclude\r\nthat our model converges and does not overfit on training set.\r\nThe training code is shown as follow:\r\nself.model.train()for batch in self.train_loader:    batch_num += 1    optimizer.zero_grad()    rgbs, segs = batch    s, _, m, n = segs.shape    segs = torch.reshape(segs, (s, m, n))    pred_segs = self.model(rgbs).to(self.device)    loss_val = loss(pred_segs, segs)    loss_val.backward()    optimizer.step()\r\nThe data collecting code can be written as follow:\r\nStatistic data of training set\r\nfor ... :    with torch.no_grad():        if batch_num % 5 == 0:            logging.info(f\"batch num {batch_num}, loss {loss_val}\")        # delete or add comments when needed        train_loss += loss_val        # statistic valid classified samples        total_pix += s * m * n        idx = torch.argmax(pred_segs, dim=1)        train_valid_pix += torch.eq(idx, segs).sum().float().item()torch.cuda.empty_cache()epoch_acc = train_valid_pix / total_pixtrain_epoch_loss.append(train_loss / batch_num)train_epoch_acc.append(epoch_acc)\r\nStatistic data of validation set\r\nself.model.eval()with torch.no_grad():    for valid_batch in self.valid_loader:        valid_batch_num += 1        rgbs, segs = valid_batch        s, _, m, n = segs.shape        segs = torch.reshape(segs, (s, m, n))        pred_segs = self.model(rgbs).to(self.device)        loss_val = loss(pred_segs, segs)        valid_loss += loss_val        valid_total_pix += s * m * n        idx = torch.argmax(pred_segs, dim=1)        valid_valid_pix += torch.eq(idx, segs).sum().float().item()epoch_acc = valid_valid_pix / valid_total_pixvalid_epoch_loss.append(valid_loss / valid_batch_num)valid_epoch_acc.append(epoch_acc)\r\nThe point you should pay attention to is that you should use\r\nwith torch.no_grad() before you do some work that have no\r\nrelation with training process, otherwise your GPU memory will be full\r\nor even overflow.\r\nResult\r\nAfter a long time training, we get the satisfying result with U-Net\r\nmodel.\r\nFormer Model\r\nThe \"former model\" infers the U-Net model, and you will see we use\r\nother upgraded model named \"UNet++\" which will be introduced later.\r\nWe output the segmentation results and their uncertainties.\r\n\r\n\r\npicture 1 result-UNet\r\n\r\nModel Upgrade\r\nFor some reasons, we try another U-Net-like model, Nested UNet,\r\nnamely UNet++. It has a nested convolutional blocks like a pyramid and\r\nthere is a chain passing connectivity between each convolutional block\r\nevery layer.\r\n\r\n\r\nNeseted UNet\r\n\r\nThe black nodes are the same with U-Net model. The green nodes are\r\nwhat Nested UNet newly added. Both green and blue lines are skip\r\npathways that pass connectivities from encoder to decoder.\r\nThe use of Nested UNet gives us a little improvement on final\r\nresults.\r\n\r\nAnalysis\r\nU-Net\r\nWe analyze the loss value and accuracy on both training and\r\nvalidation set:\r\n\r\n\r\nunet loss\r\n\r\nWe find that after 100 epochs, the model has not convergenced yet,\r\nbut the loss on validation decreases to the bottom.\r\n\r\n\r\nunet accuracy\r\n\r\nFrom the accuracy curves, we find that both training set and\r\nvalidation set have increasing accuracy, which means our model does not\r\noverfit.\r\nNested UNet\r\nMeanwhile, we analyze the loss and accuracy of Nested UNet model on\r\nboth training and validation set.\r\n\r\n\r\nnested loss\r\n\r\nWe find that Nested UNet has a faster convergency speed than UNet. It\r\nuses only about 60 epochs. But to our surprise, we find that Neseted\r\nUNet overfit after about only 20 epochs because the validation loss does\r\nnot decrease anymore.\r\n\r\n\r\nnested accuracy\r\n\r\nThe performance on validation accuracy stays the same with UNet\r\nmodel.\r\n","categories":["ML"],"tags":["CS","Deep Learning"]},{"title":"PCG | Perlin Noise","url":"/2023/09/21/Math/PerlinNoise/","content":"“程序化内容生成——地形/材质等的程序化生成”\r\n——关于一些剑走偏锋的项目历程\r\n\r\nPerlin Noise\r\n\r\nabout the Perlin noise: ref\r\n\r\n\r\n4 types of noise that are similar and that are often confused with\r\none another\r\n\r\nclassic Perlin noise\r\nimproved Perlin noise\r\nsimplex noise\r\nvaluse noise\r\n\r\n\r\n\r\nabout the range of Perlin noise: ref\r\n\r\n// mapping a 2D position to a random number range from -1 to 1var perlinValue = PerlinNoise(float x, float y);\r\n\r\nfor texture: x, y stand for pixel position, but multiplied by a small\r\nnumber called the frequency\r\n\r\n\r\nParameters\r\n\r\nfrequency of 2D waves\r\namplitude of 2D waves\r\noctaves: the amount of waves to be generated\r\npersistence: amount of change in size between one curve and the\r\nnext\r\noffset: provide variation in the output\r\nheight scale: scaling factor to accentuate the output generated\r\n\r\n\r\n\r\nProperty if 2 inputs are near to each other, the\r\nresults of the noise function will be near to each other too.\r\n\r\nguarantee continuity\r\n\r\n\r\nGeneration\r\n\r\ngiven a 2D grid as following, the input of Perlin noise is each\r\npixel.\r\n\r\n\r\n\r\n1695262810353\r\n\r\n\r\nassign each gird point a random constant vector. (note:\r\ngridVector[4])\r\nget the vectors pointing from the grid point to the input\r\npoint(target pixel). (note: inputVector[4])\r\nfor each of the 4 corners of the square where the target pixel lies,\r\ncalculate the dot products:\r\nfor i in range(4): calculate dot(gridVector[i], inputVector[i])\r\n\r\nthe dot product means the effects corners value to target\r\npixels\r\n\r\n\r\n\r\n\r\n1695264135606\r\n\r\n\r\ninterpolate between those 4 values and the result is the value of\r\nthe target pixel.\r\n\r\n\r\ndifference between Perlin noise and value noise: Perlin noise use\r\ndot product between 2 vectors to get 4 corners' values\r\nwhile value noise use a pseudo-random number.\r\n\r\nDiscussion\r\n\r\ngradient constant vectors\r\n\r\nwhy we need permutation table(noted as P) &amp;\r\ngradient table(noted as G): P is used to\r\nselect a random gradient from G. P provides randomness and\r\nrepeatability(???)4\r\nhow to generate a permutation table: the core is\r\ndouble and shuffle. we have known that permutation\r\ntable is used to select a gradient from gradient table and one gradient\r\nis defined by (x,y) (which is the grid point position). so one tuple\r\n(x,y) defines one permutation value. so the size of permutation table is\r\n\\(len(X)\\times len(Y)\\)\r\n(double). to guarantee the randomness, we can do shuffle for\r\n\\(0-255\\). the code to generate\r\npermutation table is (where \\(len(X) =\r\nlen(Y)\\)):\r\n\r\nvar permutationTable = new int[2 * len(X)];for (var i = 0; i &lt; len(X); i += 1) permutationTable[i] = i;permutationTable = Shuffle(permutationTable);for (var i = 0; i &lt; len(X); i += 1) permutationTable[len(X) + i] = permutationTable[i];// visit the table given (x,y)var valueTopRight = P[P[x+1]+y+1];var valueTopLeft = P[P[x]+y+1];var valueBottomRight = P[P[x+1]+y];var valueBottomLeft = P[P[x]+y];\r\n\r\nhow to generate a gradient table: use 4 constant\r\nvectors: \\((1f,1f), (1f, -1f), (-1f, 1f),\r\n(-1f, -1f)\\). so just do modulo with the permutation value given\r\n(x,y) can get one gradient vector.\r\n\r\nVector2 GetConstantVector(v) &#123;  switch v % 4:    case 0: return Vector2(1f,1f);    case 1: return Vector2(1f,-1f);    case 2: return Vector2(-1f,1f);    case 3: return Vector2(-1f,-1f);    default: throw undefined error;&#125;\r\n\r\n\r\ninterpolation\r\n\r\nhow to interpotate between such 4 values: 4 values\r\n(a1,a2,b1,b2), firstly interpolate between a1 and a2 which produces v1,\r\nsecondly interpolate between b1 and b2 which produces v2, finally\r\ninterpolate v1 and v2 which produces v, the interpolated value.\r\nwhich interpolation function should be used: if we\r\nuse linear interplation to get our \\(t\\) in \\(v_p =\r\na_1 + t (a_2 - a_1)\\), there will be a \"hard transition\" between\r\n3 points (x=0,1,2, while y=2,0,1.5)\r\n\r\n\r\n\r\n1695314437553\r\n\r\nbut if we use an unlinear method, it will be smoothed\r\n\r\n\r\n1695314441924\r\n\r\nthe normally used interpolation function is \\(6t^5 - 15t^4 + 10t^3\\), the image is:\r\n\r\n\r\n1695314653794\r\n\r\n\r\n\r\nfrequency\r\n\r\nwhat dose frequency means in Perlin noise: consider\r\nthis situation: what is the interpolate value when our target pixel\r\nhappens to be the bottom left grid point? ZERO. because the inputVector\r\nis zero and thus all dot products are zero. to solve ths issue, we\r\ngenerallt multiply the inputs target pixel by a small value called\r\nfrequency.\r\n\r\n\r\n\r\namplitude\r\n\r\nwhat dose amplitude means in Perlin noise: this\r\nwill be used in following section. amplitude is the multiplier before\r\none item.\r\n\r\n\r\n\r\noctave\r\n\r\nwhat dose octave means in Perlin noise: this will\r\nalso be used in following section. when one layer has a frequency that\r\nis double the frequency of the previous layer, this layer is called an\r\noctave.\r\n\r\n\r\nMore?\r\n\r\nFBM: Fractal brownian motion\r\n\r\n\r\n1695315613281\r\n\r\nobviously, the left is better.\r\n\r\nthe left image uses FBM to simulate the terrains in real world.\r\nbut...how?\r\n\r\n\r\n\r\n1695317070446\r\n\r\n\r\nso the high frequencies and low amplitudes generate more details\r\nthan just one single layer, we can keep changing the frequencies and\r\namplitudes in a for-loop, and add them together.\r\n\r\n\r\n","categories":["Math"],"tags":["Unity","PCG","Terrain"]},{"title":"第一篇博客 | Hello World","url":"/2022/01/11/Life/first/","content":"Finished! My First Blog!\r\nAfter a long time deploying my blog webpage and a lot of other\r\nborthering settings, I finally finished it! I mean, FINALLY!!!\r\n:laughing: :laughing: :laughing:\r\n\r\nOriginal Intention\r\nCan a programmer has no personal blog? I have seen many blogers\r\nwriting their own blogs no metter answering a question or just taking\r\nnotes from time to time on websites such as zhihu and csdn, but among which I prefer is to\r\nestablish a personal website where I can put my blogs on.\r\nSo, at first I have no intention about what to do with my site, maybe\r\nI just feel that it's really cool to have such a lovely home for oneself\r\nto \"lie down and rest\".\r\nBut when it was finally established by myself, experencing a lot of\r\nconfusing problems and taking amount of time to debug, I must to say\r\nthat, I love here, and I believe I will take after it like taking after\r\na baby, a baby who are growing up. :blush:\r\nThanks\r\nI would not finish my work without the help of JerryYang, whose\r\nhelpful blog is the guidance of mine (though there are still some\r\nmistakes maybe? :dizzy_face:). Based on it, I have known some basic\r\ncommand with Linux, Git and Github,\r\nwhich is also beneficial for my lessons next term. Except him I want to\r\nlink some videos there to thank for another ups from bilibili:\r\nusing\r\nhexo to start blog\r\nhow\r\nto writing blogs\r\n","categories":["Life"],"tags":["Blog","Life"]},{"title":"数学杂记 | 空间","url":"/2022/03/20/Math/space/","content":"\"内积空间和度量空间有什么区别? Hilbert空间是什么?\r\n它与线性空间的关系是什么?\"\r\n\"我已经晕了.\"\r\n\r\n数域\r\n是包含0, 1的数集, 且对 中任意两个数的加减乘除运算封闭, 则称\r\n是一个数域.\r\n线性空间\r\n在数域的基础上, 我们提出线性空间的概念:\r\n给定数域  , 和集合 . 有如下映射:   且 () 满足八条基本性质,\r\n则称为一个线性空间.\r\n赋范空间\r\n赋范空间是定义在线性空间之上的.\r\n定义在数域  的线性空间  存在如下映射:  且该映射满足: 正定, 齐次, 三角不等式. 则  是一个赋范空间,\r\n其中映射  称为范数.\r\n内积空间\r\n内积空间是定义在线性空间之上的.\r\n定义在数域  的线性空间  存在如下映射:  则  是一个内积空间.\r\n定义了内积后, 我们可以讨论向量 (即线性空间的元素) 间的长度和夹角,\r\n并进一步讨论正交性等.\r\n注意: 内积本身具有自然定义的范数, 即内积可以诱导出范数, ,\r\n因此内积空间含于赋范空间.\r\n度量空间\r\n度量空间是某个具有距离函数的集合. 该函数定义的是集合内所有元素的距离,\r\n即集合上的某种度量, 即:\r\n给定集合, 有映射:  满足:\r\n\r\n[\r\n\r\n]\r\n\r\n注意: 此处并未要求线性结构.\r\n注意: 赋范空间一定可以诱导出度量空间,\r\n因此赋范空间含于度量空间\r\n完备空间\r\n完备空间又称 Cauchy 空间. 完备空间是定义在度量空间之上的.\r\n若度量空间  中所有的柯西序列都收敛在  中的一点, 则 \r\n是一个完备空间.\r\nHilbert空间\r\n在内积空间的基础上增添完备性条件,\r\n即得到Hilbert空间.\r\n总结\r\n范数运算+向量空间=(线性)赋范空间\r\n(线性)赋范空间 + 内积运算=内积空间\r\n(线性)赋范空间 + 完备性 = Banach 空间\r\n内积空间 + 完备性 = Hilbert 空间\r\n内积空间 + 完备性 + 有限维 = Euclidean 空间\r\nReferences\r\nzhihu:\r\nhttps://www.zhihu.com/question/332144499/answer/731866608\r\nhttps://www.zhihu.com/question/42312263/answer/699451330\r\nwikipedia:\r\nhttps://en.wikipedia.org/wiki/Complete_metric_space\r\nhttps://en.wikipedia.org/wiki/Metric_space\r\nhttps://en.wikipedia.org/wiki/Cauchy_sequence\r\nhttps://en.wikipedia.org/wiki/Cauchy_sequence\r\nhttps://en.wikipedia.org/wiki/Cauchy_sequence\r\nhttps://en.wikipedia.org/wiki/Normed_vector_space\r\n","categories":["Math"],"tags":["Math"]},{"title":"项目开发 | CSW-Microservice","url":"/2023/10/05/Project/csw-microservice/","content":"\"Microservice platform of Chien-Shiung Wu Colledge\"\r\n\r\nRequirements\r\n\r\n功能性分析\r\n\r\n\r\n用户权限管理\r\n\r\n管理人员权限\r\n普通用户权限\r\n运维人员权限\r\n\r\n书院通知管理\r\n\r\n发布功能\r\n置顶功能\r\n招聘管理\r\n轮播功能\r\n静态内容展示\r\n书院文化展示（lt最爱）\r\n\r\n会议室预约管理\r\n\r\n会议室预约时间展示：展示近几天所有会议室的空闲时间、占用时间\r\n会议审核功能：查看审核协议；审核通过/拒绝用户在会议室预约的会议\r\n会议预约功能：填写会议室、使用时间、会议主题、预约人；提供增添预约、删除预约、修改预约、查看预约，撤回预约等功能\r\n\r\n大厅座位预约管理\r\n\r\n大厅座位空间时间展示：展示座位空间分布 -&gt;\r\n展示某个选中座位的预约时间\r\n座位预约功能：预约时间、预约人\r\n\r\n咖啡预约管理\r\n\r\n咖啡厅餐品展示\r\n咖啡预约功能：口味选择、预期提取时间、预约人\r\n\r\n器材出借管理\r\n\r\n器材管理：增删查改，器材描述\r\n器材出借功能：出借人、出借/归还时间、审批人\r\n\r\n问题上报管理\r\n\r\n问题收集管理：增删查改\r\n问题审批功能：审批人、问题描述、上报人（可选）\r\n\r\n操作日志管理\r\n\r\n日志查看功能\r\n\r\n个人账号管理\r\n\r\n个人信息维护\r\n\r\n密码管理（修改密码，找回密码）\r\n\r\n个人权限维护\r\n个人诚信点管理\r\n\r\n\r\n\r\n非功能性分析\r\n\r\n\r\n用户界面的具体细节\r\n\r\n参照东大信息化（？）\r\n有吴健雄学院的特色\r\n\r\n对稳定性要求要高\r\n\r\n旧的书院系统容易崩溃\r\n\r\n响应时间要尽量快\r\n\r\n实际中会经常面对临时预约会议的情况，此时需要快速响应\r\n\r\n不存在网上交易平台，但仍然需要保证用户权限的分离和安全性\r\n\r\n\r\n设计约束\r\n\r\n\r\n进度约束：本学期结束前有测试版，争取寒假能上线正式版\r\n\r\n\r\n初步讨论使用maven进行项目管理，Springboot+Vue技术栈（相关版本未定）\r\n\r\n\r\n相关use case图\r\n\r\n\r\n预约模块\r\n\r\n\r\n\r\n1696654551214\r\n\r\n\r\n个人信息管理\r\n\r\n\r\n\r\n1696654600025\r\n\r\n\r\n问题反馈\r\n\r\n\r\n\r\n1696654633120\r\n\r\n\r\n浏览模块\r\n\r\n\r\n\r\n1696654672936\r\n\r\n\r\n\r\n1696654662518\r\n\r\n","categories":["Project"],"tags":["CS","Project","SE","Microservice"]},{"title":"Full Stack | Quick Start","url":"/2023/10/12/SE/full-stack/","content":"Full-Stack Quick Start\r\nSpringBoot + Vue development\r\n\r\nJavaEE: SpringBoot + MyBatis Plus\r\nWeb front end: Vue + ElementUI\r\n\r\n\r\nconfigure environment\r\n\r\njdk\r\nIDE: JetBrain IDEA\r\nauto build tool: maven\r\n\r\nprojects auto build\r\ndependencies manegement\r\nstandard development structure\r\n\r\n\r\nmaven usage\r\n\r\nedit local repo (default is under user dir)\r\n(optional) configure mirrors\r\nconfigure self downloaded maven with IDEA\r\n\r\n\r\n\r\n1697109448690\r\n\r\nspringboot\r\n\r\nintro\r\n\r\nframework provided by Pivotal Team\r\nconvention Over Configuration(约定优于配置)\r\nembedded server(Tomcat, Jetty; no war file, just jar)\r\nsimplify maven configuration\r\npure Java\r\n\r\n\r\nspringboot\r\n\r\nSSM\r\n\r\nSpring, Spring mvc, MyBatis\r\ndifficult to configure\r\n\r\nadv\r\n\r\nquickly, simplely\r\n\r\n\r\ninitialize Springboot\r\n\r\nIDEA\r\n\r\nSpring Initializer(maven based infact)\r\n\r\nfill project information(group id &amp; artifact id)\r\ngenerate project on the web,\r\ndownload it and load in\r\nselect jdk\r\n\r\n\r\n\r\ninitialize Springboot\r\n\r\nchoose Maven project\r\nchoose springboot version\r\n\r\n\r\n\r\n1697114528195\r\n\r\ninitialize Springboot\r\n\r\nconfigure project information\r\n\r\n\r\n\r\n1697114538516\r\n\r\ninitialize Springboot\r\n\r\nadd dependencies (web app, choose spring web)\r\n\r\n\r\n\r\n1697114557959\r\n\r\ncoding springboot\r\n\r\nbackend project: receive requests from brower\r\n\r\nuse Components\r\n\r\n\r\n@RestControllerpublic class DemoController&#123;&#125;\r\ncoding springboot\r\n\r\nadd controller member function\r\n\r\n// https://localhost:8080/demo@GetMapping(&quot;/demo&quot;)public String demo()&#123;    return &quot;hello world&quot;;&#125;\r\ncoding springboot\r\n\r\nstart the project(yes, no additional code in main)\r\n\r\n\r\n\r\n1697116224333\r\n\r\nhot deployment\r\n\r\nnormal case: every time you modify the code, restart the\r\nproject\r\nneed hot-deployment\r\n\r\nspring-boot-devtools component\r\n\r\nlisten the variations of classpath, trigger Restart\r\nclass loader to reload the class\r\nnot every change needs to restart app (static res, view templates),\r\nuse spring.devtools.restart.exclude to exclude the\r\ndirs/files\r\n\r\n\r\nadd dev-tools dependency in pow.xml\r\n\r\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;    &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;\r\n\r\nadd properties in application.properties\r\n\r\nspring.devtools.restart.enabled = truespring.devtools.restart.additional-paths = src/main/javaspring-devtools.restart.additional.exclude = static/**\r\nweb development\r\n\r\nbasic\r\n\r\nrequest\r\nmethods: get request: usualy send request using address\r\nbar, only for aquiring resources; post request: commit\r\nentity to pointed resources, which changes the state or has sideffects\r\non server, more secure (however both unsecure on the view of data\r\ntransmition, using https to encrypt)\r\nentity: usually the object\r\n\r\nspring-boot-starter-web: springboot provides an integrated frame of\r\nmvc, json, tomcat\r\n\r\nwebmvc: basic frame\r\njson: data parser\r\ntomcat: container dependency\r\nauto-configure when ticking spring web\r\n\r\n\r\n&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;\r\ncontroller\r\n\r\nSpringboot provides @Controller and\r\n@RestController tags\r\n\r\nreceive &amp; handle HTTP requests\r\n@Controller tag: when requesting page\r\nand data, usually with Thymeleaf template\r\nengine (no seperation between front and back ends)\r\n@RestController tag: when requesting only\r\ndata, usually converting object into json formmat in default\r\nwhen return\r\n\r\n\r\nMVC frame\r\n\r\n\r\n1697121109192\r\n\r\n\r\ncontroller receives users' requests, fetches data from model and\r\nreture data to view\r\n\r\nrouter mapping\r\n\r\nreceive user requests\r\n@RequestMapping tag: URL router mapping, defines http\r\nrequest-mapping rule, can be added on controller class or method\r\n\r\nfor class: whole router mapping will add this mapping rule\r\nfor method: only the method itself\r\nproperties:\r\n\r\nvalue: url path, supports url templates, reg-expr,\r\n@RequestMapping(\"/user\")\r\nmethod: http request methods (get, post), also use:\r\n@GetMapping, @PostMapping\r\nconsumes: content-type for requests, e.g.\r\napplication/json\r\nproducer: content-type for resposes (html, json)\r\nparams, headers: requests parameters and requests\r\nheader\r\n\r\n\r\n\r\nparameters passing\r\n\r\nget parameters, such as\r\nhttp://localhost:8080/demo?nickname=zhangsan passing\r\nzhangsan to param: nickname\r\n@RequestParam tag: receiving parameters from http\r\nrequests body or QueryString of url, omitted when request-parameter's\r\nname is the same with controller method\r\n@PathVariable tag: handle dynamic url, the value of url\r\ncan be the parameters of controller handler\r\n@RequestBody tag: receives parameters from requestBody,\r\nhandling data such as\r\napplication/json, application/xml\r\n\r\n@RestControllerpublic class DemoController &#123;  @RequestMapping(value=&quot;/demo&quot;, method=RequestMethod.GET)  public String myDemo(String nickname, String phone) &#123;    return &quot;nickname: &quot; + nickname + &quot; phone: &quot; + phone;  &#125;&#125;\r\n\r\nmy request:\r\nhttp://localhost:8080/demo?nickname=xr&amp;phone=123\r\noutput:\r\n\r\n\r\n\r\n1697123584376\r\n\r\n\r\nnow parameter mapping handles the situation where request-parameters\r\nconflict with method parameters\r\n\r\n@RestControllerpublic class DemoController &#123;  @RequestMapping(value=&quot;/demo&quot;, method=RequestMethod.GET)  public String myDemo(@RequestParam(&quot;nickname&quot;, required = false) String nickname, String phone) &#123;    return &quot;nickname: &quot; + nickname + &quot; phone: &quot; + phone;  &#125;&#125;\r\n","categories":["SE"],"tags":["SE","Full Stack","Springboot","Vue"]},{"title":"Unity3D | Movements with CharacterController","url":"/2023/08/06/Unity/Tutorial-Movement/","content":"\r\nCharacter Controller\r\n\r\n角色控制器不同于刚体+碰撞体，但角色控制器可以控制角色的移动并进行碰撞检测。不过神奇的地方在于被它绑定的角色并不会受到力的作用，角色只有在\r\nMove函数里面才能进行运动（当然会受到碰撞检测的制约）。然后就基于这个组件做了一个人物移动的demo。\r\n-&gt; 官方文档\r\n\r\n场景搭建\r\n\r\n\r\nscene\r\n\r\n\r\n模型：Bronya-次生银翼\r\n场景：Unity商店\r\n\r\n绑定角色控制器\r\n添加Physics/Character\r\nController组件，调整大小和位置，使其刚好包围人的主体部分。\r\n\r\n\r\n1691338767159\r\n\r\n绑定玩家输入\r\n\r\n这里要使用键盘AWSD作为人物walk的输入，按住shift以后改为run。输入的管理使用InputSystem插件。\r\n\r\n\r\n创建Input Action文件\r\n创建动作映射： Action map -&gt; Actions -&gt;\r\nAction Propertites / Binding Propertites\r\n\r\n\r\n\r\n1691339763299\r\n\r\n\r\n自动生成C#文件（放在scripts目录下面），里面自动生成以上的映射关系，在其它地方使用时需要\r\nImport UnityEngine.InputSystem\r\n\r\n角色移动\r\n\r\n接下来自己创建一个用于角色动作控制的脚本\r\n\r\n\r\n创建好以后是如下形式的\r\n\r\nusing UnityEngine;namespace Bronya &#123;  public class BronyaActionController : MonoBehaviour &#123;    // Start is called before the first frame update    private void Start() &#123; &#125;    // Update is called once per frame    private void Update() &#123; &#125;  &#125;&#125;\r\n添加成员变量：\r\n// 创建InputAction的文件名，用于管理角色输入private BronyaInput         _bronyaInput;// 角色控制器组件private CharacterController _characterController;// AWSD方向键输入private Vector2             _movementInput;\r\n在 Awake()函数中实例化上述中的两个成员变量：\r\nprivate void Awake() &#123;  _bronyaInput         = new BronyaInput();  _characterController = GetComponent&lt;CharacterController&gt;();&#125;\r\n并创建enable和disable函数用于控制角色控制器是否启用：\r\nprivate void OnEnable() &#123;   _bronyaInput.BronyaActionController.Enable(); &#125; private void OnDisable() &#123;   _bronyaInput.BronyaActionController.Disable(); &#125;\r\n行走\r\n\r\n行走的实现需要包含速度，方向；\r\n由于用到了InputSystem这个插件，自己实现行走的回调函数\r\n\r\npublic static void OnWalk(InputAction.CallbackContext context) &#123;  // movement input has been normalized  MovementInput = context.ReadValue&lt;Vector2&gt;();  IsWalkPressed = MovementInput.x != 0 || MovementInput.y != 0;  // assign to walk-variables  WalkSpeedVector.x = MovementInput.x * WalkSpeedFactor;  WalkSpeedVector.z = MovementInput.y * WalkSpeedFactor;  WalkSpeedVector.y = GlobalVariables.ZeroF;&#125;\r\n在Awake函数中绑定回调函数。回调函数中主要是计算角色行走时的速度。\r\n_inputSystem.BronyaActionController.Walk.started  += MovementController.OnWalk;_inputSystem.BronyaActionController.Walk.canceled += MovementController.OnWalk;\r\n另外编写控制角色行走的函数MovementHandler()用于帧更新（放在Update()中）\r\n private void MovementHandler() &#123;      _characterController.Move(MovementController.WalkSpeedVector * Time.deltaTime);&#125;\r\n此时等待Unity编译后可以操控角色行走（移动）\r\n\r\n\r\n1691924618535\r\n\r\n添加动画\r\n下面添加行走动画：首先要找动画资源，拖入unity中修改骨骼为Humanoid（这里略）；其次为角色添加AnimationController，可以发现动画是由状态机控制的，创建初始状态为Idle，再创建新状态为Walk（后续的跑步亦如此）\r\n\r\n\r\n1691926979364\r\n\r\n为状态之间添加上图所示的Transition，三个状态之间的转换用两个布尔型的变量来控制即可\r\n\r\n\r\n1692006048905\r\n\r\n在创建用于控制角色动画的函数\r\nprivate void AnimationHandler() &#123;  var isWalking = _bronya.GetIsWalking();  var isRunning = _bronya.GetIsRunning();  if (_characterController.isGrounded) &#123;    if (MovementController.IsWalkPressed &amp;&amp; !isWalking) &#123;      _bronya.PlayWalkAnimation();    &#125;    else if (!MovementController.IsWalkPressed &amp;&amp; isWalking) &#123;      _bronya.StopWalkAnimation();    &#125;    if ((MovementController.IsWalkPressed &amp;&amp; MovementController.IsRunPressed) &amp;&amp; !isRunning) &#123;      _bronya.PlaRunAnimation();    &#125;    else if ((!MovementController.IsWalkPressed || !MovementController.IsRunPressed) &amp;&amp; isRunning) &#123;      _bronya.StopRunAnimation();    &#125;  &#125;&#125;\r\n相关的角色动作都记录在如下的类中，和上面的移动代码分开管理\r\nnamespace Bronya &#123;  public class BronyaActionController : MonoBehaviour &#123;    private Animator _animator;    private int _isWalkingHash;    private int _isRunningHash;    private void Awake() &#123;      _animator = GetComponentInChildren&lt;Animator&gt;();      _isWalkingHash = Animator.StringToHash(&quot;isWalking&quot;);      _isRunningHash = Animator.StringToHash(&quot;isRunning&quot;);    &#125;    public bool GetIsWalking() &#123;      return _animator.GetBool(_isWalkingHash);    &#125;    public bool GetIsRunning() &#123;      return _animator.GetBool(_isRunningHash);    &#125;    public void PlayWalkAnimation() &#123;      _animator.SetBool(_isWalkingHash, true);    &#125;    public void StopWalkAnimation() &#123;      _animator.SetBool(_isWalkingHash, false);    &#125;      public void PlaRunAnimation() &#123;      _animator.SetBool(_isRunningHash, true);    &#125;    public void StopRunAnimation() &#123;      _animator.SetBool(_isRunningHash, false);    &#125;  &#125;&#125;\r\n目前的移动如下所示\r\n\r\n\r\n1692012104234\r\n\r\n添加转向\r\n可以发现目前的问题是角色不会根据移动的方向进行转向，因此继续添加控制角色转向的代码，如下\r\nprivate void RotationHandler() &#123;  Vector3 lookAtPosition;  lookAtPosition.x = MovementController.WalkSpeedVector.x;  lookAtPosition.z = MovementController.WalkSpeedVector.z;  lookAtPosition.y = GlobalVariables.ZeroF;  if (MovementController.IsWalkPressed) &#123;    transform.rotation = Quaternion.Slerp(transform.rotation, Quaternion.LookRotation(lookAtPosition),      MovementController.RotationSpeedFactor);  &#125;&#125;\r\n可以看到角色已经可以正常进行转向了\r\n\r\n\r\n1692012118894\r\n\r\n添加重力\r\n但此时还可以发现一个致命问题是角色没有重力\r\n\r\n这是因为在CharacterController中控制的物体是不受重力的，需要自己添加向下的力\r\n\r\n于是添加GravityHandler如下\r\nprivate void GravityHandler() &#123;  if (_characterController.isGrounded) &#123;    MovementController.WalkSpeedVector.y = GlobalVariables.GravityAccelerate;    MovementController.RunSpeedVector.y  = GlobalVariables.GravityAccelerate;  &#125;  else &#123;    MovementController.WalkSpeedVector.y += GlobalVariables.GravityAccelerate;    MovementController.RunSpeedVector.y  += GlobalVariables.GravityAccelerate;  &#125;&#125;\r\n即：如果此时CharacterController检测到角色与地面有接触时，角色的重力直接等于9.8，方向向下（相当于给角色一个向下的速度，让角色“压”住地面）；如果没有与地面有接触，则不断增加角色的下落速度（模拟自由落体时速度的变化）\r\n跑步\r\n跑步的代码类似与走路，唯一不同之处是我们选择用button来控制角色是否应该开始跑步\r\n角色移动的演示\r\n\r\n\r\n1692012129227\r\n\r\n","categories":["Unity3D"],"tags":["CS","Unity3D","CharacterController"]},{"title":"逆向分析 | 需求分析","url":"/2023/10/05/SE/reverse-analysis-0/","content":"\"项目负责人对用户需求的理解程度，在很大程度上决定了项目的成败\"\r\n\r\n需求分析的目的\r\n\r\n更好地了解、分析、明确用户需求，并能够准确清晰地以文档形式表达给参与项目开发的每个成员，保证项目开发按照用户需求的方向进行。\r\n\r\n\r\n需求分析的物质性结果是\r\n软件功能描述书（此外，一般还需要编写用户调查报告和市场调研报告）\r\n\r\n需求分析的内容\r\n\r\n功能性分析\r\n\r\n必须实现哪些功能\r\n向用户提供功能时需要执行的动作\r\n形成软件需求规格说明书\r\n\r\n非功能性分析\r\n\r\n用户界面具体细节\r\n软件性能、可靠性、响应时间需求\r\n运行环境需求\r\n相关标准、规范\r\n安全需求\r\n架构需求\r\n未来可能的扩充需求\r\n\r\n设计约束\r\n\r\n进度约束\r\n预算约束\r\n资源约束\r\n其它\r\n\r\n\r\n需求分析的人员分工\r\n\r\n项目管理者：组织\r\n人员与用户进行交流，组织 人员编写项目功能描述书\r\n\r\n\r\n开发人员：与用户一起进行需求分析\r\n\r\n\r\n美术和技术骨干代表或者全体成员（与用户讨论）编写项目的功能描述书初稿\r\n\r\n\r\n相关人员对功能书的初稿进行修改和完善，形成正式文档\r\n\r\n\r\n用户若有能力，可以参与至功能描述书的编写和修改中\r\n\r\n用户调查报告\r\n\r\n用户的充分配合，必要时需要对用户进行培训。\r\n\r\n\r\n调查的形式：发需求调查表、开需求调查座谈会或者现场调研\r\n\r\n调查内容\r\n\r\n网站当前以及日后可能出现的功能需求\r\n客户对网站的性能(如访问速度)的要求和可靠性的要求\r\n确定网站维护的要求\r\n网站的实际运行环境\r\n网站页面总体风格以及美工效果(必要的时候用户可以提供参考站点或者由公司向用户提供)\r\n主页面和次级页面数量，是否需要多种语言版本等\r\n内容管理及录入任务的分配\r\n各种页面特殊效果及其数量(js，flash等)\r\n项目完成时间及进度(可以根据合同)\r\n明确项目完成后的维护责任\r\n\r\n调查报告的重点内容\r\n\r\n调查概要说明：网站项目的名称;用户单位;参与调查人员;调查开始终止的时间;调查的工作安排。\r\n调查内容说明：用户的基本情况;用户的主要业务;信息化建设现状;网站当前和将来潜在的功能需求、性能需求、可靠性需求、实际运行环境;用户对新网站的期望等。\r\n调查资料汇编：将调查得到的资料分类汇总(如调查问卷，会议记录等等)\r\n\r\n市场调研报告\r\n\r\n市场调研的目的：清晰地分析相似网站的性能和运行情况，帮助项目负责人清楚地构想出自己开发的网站的大体架构和模样，总结同类网站优势和缺点；明确并引导用户需求\r\n\r\n\r\n应尽可能调研到所有比较出名或优秀的同类网站，了解同类网站的使用环境和用户的诧异点。\r\n\r\n调研内容\r\n\r\n市场中同类网站作品的确定。\r\n调研作品的使用范围和访问人群。\r\n调研产品的功能设计(主要模块构成，特色功能，性能情况等等)\r\n简单评价所调研的网站情况。\r\n\r\n调研报告的重点内容\r\n\r\n调研概要说明：调研计划;网站项目名称、调研单位、参与调研、调研开始终止时间。\r\n调研内容说明：调研的同类网站作品名称、网址、设计公司、网站相关说明、开发背景、主要适用访问对象、功能描述、评价等项目管理者联盟\r\n可采用借鉴的调研网站的功能设计：功能描述、用户界面、性能需求、可采用的原因。\r\n不可采用借鉴的调研网站的功能设计：功能描述、用户界面、性能需求、不可采用的原因。\r\n分析同类网站作品和主要竞争对手产品的弱点和缺陷以及本公司产品在这些方面的优势。\r\n调研资料汇编：将调研得到的资料进行分类汇总。\r\n\r\n软件功能描述书\r\n描述书的重点内容\r\n\r\n网站功能\r\n网站用户界面(初步)\r\n网站运行的软硬件环境\r\n网站系统性能定义\r\n网站系统的软件和硬件接口\r\n确定网站维护的要求\r\n确定网站系统空间租赁要求\r\n网站页面总体风格及美工效果。\r\n主页面及次页面大概数量。\r\n管理及内容录入任务分配。\r\n各种页面特殊效果及其数量。\r\n项目完成时间及进度(根据合同)\r\n明确项目完成后的维护责任。\r\n\r\n","categories":["SE"],"tags":["CS","SE","reverse analysis"]},{"title":"Paper Reading | Top Transactions and Top Conferences","url":"/2023/10/10/VR/paper-look-through/","content":"Summarize some top transactions and conferences in VR (and the\r\nrelated tech) field.\r\n\r\nTop Transactions\r\n\r\nTOCHI: ACM Transactions on\r\nComputer-Human Interaction\r\nIJHCS: International\r\nJournal of Human Computer Studies\r\nTOMCCAP: ACM Transactions on Multimedia\r\nComputing, Communications and Application\r\nHCI(IJHCI): Human Computer\r\nInteraction\r\n\r\nTop Conferences\r\n\r\nCSCW: ACM\r\nConference on Computer Supported Cooperative Work and Social\r\nComputing\r\nACM CHI: ACM\r\nConference on Human Factors in Computing Systems\r\nUbiComp: ACM International\r\nConference on Ubiquitous Computing\r\nVR: IEEE Virtual\r\nReality\r\n\r\n","categories":["VR"],"tags":["VR","paper","HCI"]},{"title":"【VR项目练习】诗歌场景生成项目","url":"/2023/08/26/VR/vr-poemGeneration/","content":"“练手项目（内心惊恐”\r\n\r\nTODO List\r\n\r\n\r\n优化场景的光照\r\n\r\n优化人物/动物的行为\r\n\r\n优化语言输入中的数据库匹配\r\n\r\n添加主角本体\r\n\r\n优化/丰富NPC/场景交互\r\n\r\n整体的思路\r\n\r\n语音识别模块：使用流式语音处理的预训练模型，用爬虫爬取古诗文网中的古诗和语音作为训练集训练模型\r\n场景生成模块：根据语音识别结果生成对应场景\r\n\r\n困难\r\n\r\n语音识别模块\r\n\r\n模型的选择\r\n数据集的构建（第一周实践后发现有部分古诗词原文有注释，和录音不能很好匹配，需要手动调整）\r\n关键词如何匹配（语音识别后如何快速将识别结果和关键词进行匹配，由于中文一个读音有多种对应的汉字，如何进行匹配）\r\n\r\n场景生成模块\r\n\r\n不会真要为每一首诗单独创建一个场景吧\r\n目前的方案是把场景分为主要场景和次要场景，主要场景根据诗词里面的特色单独人工搭建，次要场景随机生成\r\n\r\n\r\n第五/六周\r\n\r\n主要还是重构代码的工作：发现了一些更利于扩展的项目结构\r\n问题\r\n\r\n由近及远生成内容\r\n作用于全局的内容、作用于局部的内容生成方式不同\r\n地形的功能划分（山地、丘陵、平原、湖泊）\r\n\r\n\r\n\r\n论文\r\n\r\nIEEE VR：Towards Virtual\r\nReality Infinite Walking: Dynamic Saccadic Redirection(ACM\r\nTransactions on Graphics, 2018), An Open Framework\r\nfor Infinite Walking With Saccadic Redirection(ARST)\r\n\r\n\r\n第四周\r\n\r\n重写了地形生成的代码\r\n\r\n之前的地形生成HeightMap和Terrain的大小必须一致（HeightMap的每一个像素强制对应Terrain的每一个整数坐标）\r\n修改后二者可以不相等\r\n当HeightMap的length大于terrain的length时，可以实现更好的平滑\r\n\r\n\r\n\r\n完成了树、草、花的随机生成\r\n\r\n多种类型的decoration\r\n\r\n\r\n\r\n\r\n1695816544892\r\n\r\n第三周\r\n\r\n用Perlin Noise生成随机heightMap再生成地形\r\n\r\n不同起伏程度的地形\r\n\r\n山区地形\r\n居民区地形\r\n湖区地形\r\n\r\n\r\n实现区域划分和不同起伏程度\r\n\r\n区域划分的参数用结构体保存 -&gt; 以后转为解析出来的对应数据\r\n起伏程度 -&gt;\r\n改变柏林噪声中的随机梯度的模（或者改变内积以后的值）\r\n\r\n\r\ndemo演示\r\n第二周\r\n项目框架\r\n\r\n\r\n1693908290231\r\n\r\n\r\n任务1：原模型转化为low poly模型\r\n\r\n使用插件（LowPoly Mesh\r\nGenerator）将模型逐个转化（已完成）并保存为prefab\r\n初步对预制体进行分类\r\n\r\n任务2：json生成地形（思路是语音识别的结果保存为json格式，然后根据json参数生成场景；目前先实现地形生成）\r\n\r\n手动生成：尝试了LowPoly Terrain Generator插件\r\n代码生成：解析json数据-&gt;读取Terrain有关的参数-&gt;根据不同的mode生成不同的HeightMap-&gt;将HeightMap用于插件内的成员变量-&gt;在自己的脚本里面调用插件\r\n\r\n难点是如何生成HeightMap，目前考虑的是先生成png形式的HeightMap，例如\r\n\r\n\r\n\r\n\r\n\r\n1693906658523\r\n\r\n在unity内生成的地形如下\r\n\r\n\r\n1693907088483\r\n\r\n第一周\r\n\r\n寻找语音识别模型\r\n\r\n流式语音处理\r\n对中文准确率高\r\n最好是预训练的，可以将进行古诗文/语音的适配\r\n\r\n学习/熟悉Unity的VR开发\r\n\r\n先了解了无设备开发\r\n在连接设备的情况下如何进行开发（目前找到了网课讲解SteamVR和HTCVive）\r\n场景的搭建（之前在实验室主机上面查看了整个项目的场景，配色偏暗，想修改，但是不是当务之急）\r\n\r\n\r\n目前做的事情\r\n\r\n爬虫能爬取古诗词原文（还不能爬取语音）\r\n预训练模型选择的是MASR框架，项目地址，内部提供了多个预训练模型，并且能支持流式的语音处理\r\n\r\n论文\r\n讲解\r\n\r\n\r\n\r\n\r\n1693294494575\r\n\r\n\r\n浅学了一下Unity的VR开发（没有借设备，只是听课）\r\n\r\n","categories":["VR"],"tags":["VR","c#"]},{"title":"【VR】头显等设备的模拟（无设备开发）","url":"/2023/08/26/VR/vr-simulation/","content":"无设备开发VR\r\n\r\n插件\r\n\r\nVive Input Utility\r\nVive Registry Tool\r\nSteamVR Plugin\r\n\r\n\r\n"}]