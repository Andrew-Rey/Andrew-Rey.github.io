[{"title":"Algorithm | FFT","url":"/2022/04/06/Algorithm/fft/","content":"\"对称，万变不离其中\"\r\n\r\n多项式乘积问题\r\n首先来思考这样的一个问题:\r\n\r\nQuestion 1\r\n你有两个多项式函数:\r\n\r\n\r\n应该如何计算它们的乘积?\r\n\r\n当然, 我不是说要用笔算的方式, 而是用计算机.\r\n显然这个问题我们在小学二年级就写过的,\r\n当初正在学习\"数据结构\"这门课, 如果没记错, 应该是用链表实现的.\r\n但是, 就算是用链表实现, 那不也是和手算一样的原理吗?\r\n\r\n将二者相乘\r\n分配律\r\n合并同类项\r\n\r\n例如上面那个例子:\r\n\r\nsolution 1\r\n\r\n\r\n(???是什么动力让我深夜在这里口算多项式乘法???)\r\n显然, 如果一个 n 次多项式乘上一个 m 次多项式, 在合并同类项前应该有\r\n 次多项式, 这谁顶得住?\r\n对于正常人类而言显然顶不住, 对于计算机而言, 时间复杂度是, 也是算比较大的开销了吧.\r\n咋办?\r\n点表示法\r\n开始\r\n有谁规定, 我多项式一定是用系数表示的?\r\n好家伙, 你这样说我就摸不着头脑了,\r\n难道除了系数表示还有其他表示方法吗?\r\n首先, 多项式集合其实是构成了一个线性空间, 也就是说,\r\n任意两个多项式进行线性运算 (加法和数乘) 后, 结果仍然是多项式. 事实上\r\n\r\n构成了该空间的一组基, 将函数展开成 Taylor 级数便用了这组基作为基底,\r\n基前面的系数也就是坐标.\r\n其次, 对于一个 n 次多项式而言, 只要我们确定了它的坐标,\r\n就能唯一确定这个多项式. 现在的问题是不知道坐标,\r\n如何确定多项式. 这里的巧妙之处就在于, 多项式函数是一个映射,\r\n对于一个特定的 x, 总是能给出唯一一个值与之对应, 这不就是一个方程吗?\r\n我给你一个 x, 你输出一个值, 同时由于多项式系数全部未知,\r\n这就是一个关于  个系数的方程\r\n显然, 我需要 \r\n个不同的点来唯一确定我的系数. 这就是所谓的点表示法. 这样一来, 我们将这\r\n 个方程写成矩阵形式:\r\n\r\n看到这里我终于理解了为什么在学高等代数时要突然讲一个范德蒙德(Vandermonde)行列式,\r\n也就是这里的\r\n\r\n将上述矩阵定义为我们最喜欢的字母.\r\n好, 既然这东西是范德蒙德行列式, 那我们可以知道它行列式不为 0, 也就是说, 这个矩阵是可逆的, 也就是当我们取\r\n 个不同点时,\r\n确实是可以使方程组有唯一解, 也就是  个点可以唯一表示一个 n\r\n次多项式.\r\n乘法\r\n问题来了, 如何做乘法?\r\n我们有 n 次多项式和 m 次多项式做乘法, 得到的是一个  次多项式, 那么我们只要找到  个点即可, 也就是只要在 n 次多项式和\r\nm 次多项式中分别找  个点,\r\n这些点的横坐标 x 相等, 再将对应的函数值相乘即可.\r\n进一步\r\n现在, 我们知道了如何用点表示多项式, 以及如何用点表示进行乘法运算.\r\n但是仔细一想, 这种方法需要求解线性方程组, 这里的计算复杂度并不低.\r\n也就是从系数表示法到点表示法的转化过程带来的计算复杂度还是很高的.\r\n有什么方法可以进行简化吗?\r\n先等一等, 我们先来梳理我们用点表示求多项式乘法的思路:\r\n\r\nMainIdea\r\n\r\n将 n 次多项式和 m 次多项式分别从系数表示转化为点表示\r\n对应点相乘\r\n将得到的 \r\n个点表示的多项式转化为系数表示\r\n\r\n\r\n奇偶\r\n先来考虑简单的情况:\r\n\r\nQuestion 2\r\n多项式\r\n\r\n和多项式\r\n\r\n用点表示法相乘\r\n\r\n那我们当然是按部就班地进行乘法啦~ - 由于结果是 5 次多项式,\r\n因此对取 5 个点, 对取 5 个点.\r\n取点, 说得轻巧, 做起来倒是挺犹豫的.\r\n取什么样的点能满足要求呢? 或者得寸进尺地说,\r\n什么样的点能让效率更高呢? 注意到二次函数是对称的,\r\n那我们是不是只要取正的 2 个点, 就能知道负的 2 个点, 另外加一个原点?\r\n确实如此.\r\n那三次多项式呢? 照理来说,\r\n我们同样也是只要取一半的点就能知道另一半点的值(这里的\"一半\"针对正负而言),\r\n只不过要在函数值上添加负号, 何必呢? 还不如干脆 提出一个\r\nx, 然后不也变成了二次函数?\r\n事实上, 一般而言, 我们要用点表示法表示多项式, 可以用如下方法:\r\n\r\nMethod1\r\n 其中, 表示只含偶次的多项式函数, 表示只含奇次的多项式.\r\n\r\n这样, 我们只要在非负轴上取值就可以确定整个多项式, 取点的个数是\r\n原来的一半.\r\n甚至, 这里形成了一个 递归 算法: 分解后的不也是一个关于 x 的多项式吗?!\r\n那我继续啊, 把继续分解啊,\r\n大事化小, 小事化了.\r\n等等!\r\n我们的其实是, 这里每个都是非负的啊.\r\n未来我们只能在非负轴取值了, 也就是说, 分解为偶次多项式后,\r\n递归停止了.\r\n完蛋.\r\n复数域分解\r\n\"山重水复疑无路, 柳暗花明又一村\"\r\n看到标题就已经知道要怎么做了. 既然在实数域上无法继续分解,\r\n那为何不去复数域呢?\r\n在复数域上我们可以快乐地进行递归.\r\n如何个快乐法呢? 我们来细品:\r\n\r\n偶次多项式在复平面上的根\r\n为什么突然变成了 求根?\r\n从第二节中\"奇偶\", 我们可以选取对称的点,\r\n来减少选取点的个数(即原来的一半). 接着我们把任意 n\r\n次多项式分解成两个偶次多项式, 偶次多项式的好处在于容易选取对称的点.\r\n但是由于在实数范围内, 在对偶次多项式进行递归时会发生中断,\r\n于是我们扩展至复数域讨论分解.\r\n方便起见: 对于, 我们取作为特征点, 对于, 我们取作为三个特征点, 那对于, 我们应该怎样取点, 抛开不谈, 令, 由 代数基本定理,\r\n该方程在复数域上有 4 个 根,\r\n对于其它偶次多项式我们以此类推.\r\n就这样, 我们找到了一个简单的方法寻找所有需要的点, 进行递归.\r\n单位根\r\n写到这里, 我也感觉有点吃力, 关键是为什么一定就取了令呢?\r\n虽然但是, 确实是所谓的\"方便起见\", 这是因为, 取了\"1\",\r\n我们可以在复平面上的单位圆上讨论这个问题.\r\n在小学二年级我们就知道,\r\n的根可以用我们熟悉的的幂来表示, 即  这些个点在复平面单位圆上\r\n对称分布. 每递归一次, 单位根的数量减少一半,\r\n但保持对称性不变.\r\n确实方便.\r\n\r\n快速傅里叶变换(FFT)\r\n终于能正式地介绍世界上最美丽的算法了:\r\n快速傅里叶变换(FFT).\r\nFFT解决的是多项式从系数表示到点表示的过程中, 计算复杂度的问题.\r\n框架\r\n分解:\r\n\r\n递归:\r\n\r\n\r\n加和: \r\n\r\n\r\n返回\r\n时间复杂度为: \r\n一些数学\r\n\r\n\r\n我们在复数域上考虑, 令  (这是因为, 我们希望多项式在复数域上考虑时,\r\n我们可以在单位圆周上讨论. 其中表示我们取的第 k 个点, 刚好与 是对应的.)\r\n则线性方程组可以化为:\r\n\r\n其中\r\n 称为离散傅里叶变换矩阵(DFT)显然该矩阵是\r\n对称的 且 可逆, 其逆矩阵为: \r\n并且, 该逆矩阵看起来和原矩阵 一模一样! .\r\n\r\n结束了?\r\n当我们乐呵呵地把FFT转化为代码时, 开心的分解多项式, 然后选点, 相乘,\r\n等等! 你还没告诉我, 怎么从点表示转化回系数表示呢!\r\n这就是FFT对称的魅力了. 由点求系数,\r\n不过是矩阵求逆的过程:\r\n\r\n显然, 由于DFT和DFT逆矩阵具有相似的形式,\r\n我们完全可以用同一个函数完成快速傅里叶的正反变换!\r\n\r\n","categories":["Algorithm"],"tags":["CS","Algorithm"]},{"title":"C++ Programming | class template","url":"/2023/02/28/CS/class-template/","content":"\"C++17类模板\"\r\n\r\n\r\n编译器用于创建类的模板: 自动生成类\r\n\r\n标准库\r\n类模板不是类, 是创建类的一种方式\r\n\r\n\r\n\r\n实例\r\n\r\n编译器从类模板中生成的类, 在第一次使用模板类型声明变量是,\r\n会创建类模板的一个实例, 以后定义同类型的变量时,\r\n会使用已创建的第一个实例. 在创建类模板时, 也可以不同时声明变量.\r\n数据的组织 独立于 对象类型\r\n\r\n\r\n类模板的定义\r\ntemplate&lt;typename T1, typename T2, Type Arg1&gt;\nclass ClassName {\n    // template class definition\n};\r\n\r\n模板参数\r\n\r\n类型参数 typename\r\n\r\n实参总是类型: int, float...\r\n\r\n非类型参数 Type\r\n\r\n实参是整数类型的字面量: 200, 10...\r\n整数常量表达式\r\n指向对象的指针或引用, 函数指针或空指针\r\n\r\n模板\r\n\r\n实参是类模板的一个实例\r\n\r\n\r\n\r\n\r\n在模板定义中, 不需要使用完整的ID, 例如构造函数\r\nClassName&lt;T1&gt;();可以写成ClassName();\r\n不过在模板体的外部标识模板, 则必须使用模板ID\r\n(即在模板类外定义模板中的成员函数时需要显式写出ID)\r\n\r\n一个例子\r\ntemplate&lt;typename T1&gt;\nclass PythonList {\nprivate:\n    int len_;\n    int size_;\n    T1* elements_;\n\npublic:\n    explicit PythonList&lt;T1&gt;(size_t list_len);\n    PythonList&lt;T1&gt;(const PythonList&lt;T1&gt;&amp; python_list);\n    ~PythonList();\n    T1&amp; operator[](size_t index);\n    const T1&amp; operator[](size_t index) const;\n    PythonList&lt;T1&gt;&amp; operator=(const PythonList&lt;T1&gt;&amp; rhs_list);\n    size_t get_len() const { return len_; }\n    void allocate_double();\n};\r\n类模板成员函数的定义\r\n\r\n若在模板类的内部定义, 实则为 内联\r\n\r\n\r\n如何理解该语法\r\n\r\n类模板的成员函数的外部定义本身就是函数模板,\r\n即使成员函数不依赖类型参数.\r\n若函数没有在类内定义, 则它需要一个模板定义.\r\n定义函数模板中的参数列表必须与类模板参数列表相同.\r\n\r\n\r\n例如\r\n// 析构函数\ntemplate &lt;typename T1&gt;\nPythonList&lt;T1&gt;::~PythonList&lt;T1&gt;() {\n    delete [] elements_;\n}\n\n// 构造函数\ntemplate &lt;typename T1&gt;\nPythonList&lt;T1&gt;::PythonList(size_t list_len)\n    : len_(list_len), size_(FOLD * list_len), elements_(new T1(list_len)) {}\n\ntemplate &lt;typename T1&gt;\nPythonList&lt;T1&gt;::PythonList(const PythonList&lt;T1&gt; &amp;python_list)\n    : PythonList{python_list.len_} {\n  for (size_t i{}; i &lt; len_; ++i) {\n    elements_[i] = python_list.elements_[i];\n  }\n}\n\n// 下标运算符\ntemplate &lt;typename T1&gt; T1 &amp;PythonList&lt;T1&gt;::operator[](size_t index) {\n  if (index &gt;= len_) {\n    throw std::out_of_range{\"Index out of range: \" + std::to_string(index)};\n  }\n  return elements_[index];\n}\n\ntemplate &lt;typename T1&gt;\nconst T1 &amp;PythonList&lt;T1&gt;::operator[](size_t index) const {\n  if (index &gt;= len_) {\n    throw std::out_of_range{\"Index out of range: \" + std::to_string(index)};\n  }\n  return elements_[index];\n}\n\n// 赋值运算符\ntemplate &lt;typename T1&gt;\nPythonList&lt;T1&gt; PythonList&lt;T1&gt;::operator=(const PythonList&lt;T1&gt;&amp; rhs_list) {\n    if (&amp;rhs_list != this) {\n        delete [] elements_;\n        len_ = rhs_list.len_;\n        size_ = rhs_list.size_;\n        elements_ = new T1[len_];\n        for (size_t i {}; i &lt; size_; ++i) {\n            elements_[i] = rhs_list.elements_[i];\n        }\n    }\n    return *this;\n}\r\n\r\n第一行说明该函数为模板函数; 在限定成员函数时,\r\n作用域需要带上模板ID\r\n有时候需要提供自己的拷贝构造(或析构),\r\n因为涉及到动态内存分配时, 默认拷贝构造(或析构)有可能会出现负面效应\r\n在赋值重载时, 需要 检查左右操作数是否相等,\r\n否则会释放this指向的对象后再进行复制.\r\n\r\n代码重复\r\n\r\n在上述的定义中,\r\nconst的重载和非const的重载模板函数代码重复,\r\n代码重复不利于后续的维护\r\n\r\n对抗重复的方法: 函数, 模板, 基类\r\n\r\n\r\n传统方法:\r\n用const实现非const\r\ntemplate &lt;typename T1&gt;\nT1&amp; PythonList&lt;T1&gt;::operator[](size_t index) {\n    return const_cast&lt;T1&amp;&gt;(static_cast&lt;const PythonList&lt;T1&gt;&amp;&gt;(*this) [index]);\n}\r\nC++17:\r\nstd::as_const()(utility头文件)\r\ntemplate &lt;typename T1&gt;\nT1&amp; PythonList&lt;T1&gt;::operator[](size_t index) {\n    return const_cast&lt;T1&amp;&gt;(std::as_const(*this)[index]);\n}\r\n异常安全性\r\n\r\n在赋值运算符重载的时候, 由于使用了new,\r\n可能会出现std::bad_alloc异常\r\n在elements_[i] = rhs_list.elements_[i];可能会出现关于类型T1的赋值异常\r\n\r\n\r\n当声明了noexcept后, 表示代码内部不发生异常,\r\n使得编译器能做更多的优化,\r\n例如大部分析构都隐式声明了noexcept cppreference noexcept\r\n\r\n\r\n在以上的赋值运算符中使用 复制后交换\r\n\r\n定义模板类注意\r\n\r\n\r\n成员函数模板与类模板的定义放在同一个文件中: 当编译器生成类模板时,\r\n需要去使用函数模板, 所以在使用模板的源文件中,\r\n这些成员函数的定义必须可用.\r\n\r\n\r\n类模板实例化\r\nPythonList&lt;double&gt; data {10};\r\n\r\n编译器只编译程序使用的成员函数,\r\n不会为某个模板参数的实例而一次性编译整个类:\r\n例如上述代码编译后的类中只有构造函数和析构函数.\r\n\r\n\r\n声明对象类型的指针 不会 创建模板实例:\r\nPythonList&lt;std::string&gt;* data_p;\r\n\r\n非类型的类模板参数\r\n\r\n主要用于定义指定容器有效的值, 如数组的维数\r\n\r\n非类型参数只能是整数类型 (size_t,\r\nlong), 枚举类型, 对象的指针或引用, 函数的指针或引用,\r\n类成员的指针\r\n当作常量\r\n\r\n\r\ntemplate&lt;typename T1, size_t size&gt;\nclass ClassName {\n    // definition\n};\n\n// 还有一些比较无语的\ntemplate&lt;typename T1, T1 value&gt; ... // 此时T1只能是模板的非类型参数所允许的类型\r\n\r\n注意\r\n只有模板参数完全相同的情况下, 编译器才不会再次编译模板类;\r\n任意一个不同, 编译器都会认为是不同的类, 后果是代码膨胀\r\n\r\n解决方法 (待定)\r\n\r\n\r\n模板参数的默认值\r\n\r\n与函数的默认参数类似\r\n\r\n如果某个模板参数有默认值, 则后续的参数也必须有默认值\r\n如果某个模板参数的实参被省略, 则后续的所有实参也必须省略\r\n不需要在成员函数的模板中指定默认值\r\n\r\n\r\ntemplate&lt;typename T1 = int, int value = 10&gt; ...\r\n模板的显式实例化\r\ntemplate class ClassName&lt;T1, 10, ...&gt;\r\n\r\n编译器会从模板中实例化所有的成员函数, 无论是否调用\r\n\r\n类模板特化\r\n模板的使用中有时候只对 某些类型 有用,\r\n而不支持其他类型; 因此使用 特化 来处理某些特殊情况.\r\n例如整型变量的相等和浮点型的比较并不相同,\r\n这时可以使用模板的特化来处理.\r\n\r\n对于类模板中的成员函数:\r\n\r\n如果成员函数是在类模板的外部定义的, 而不是在类模板体中定义的,\r\n则可以提供函数模板的特化\r\n\r\n\r\n全特化\r\n即规定模板实现的所有模板参数\r\ntemplate&lt;&gt;\nclass PythonList&lt;const char*&gt;{};\r\n\r\n特化的定义必须放在原有的定义或声明后面.\r\n因为指定了所有参数, 所以是 全特化\r\n\r\n偏特化\r\n即只规定模板参数列表中的一部分模板参数\r\ntemplate&lt;Type value&gt;\nclass PythonList&lt;const char*, value&gt; {};\r\n\r\ntemplate后的参数列表包含的是为这个模板特化的实例所指定的参数,\r\n即实例化时需要指定value\r\n模板名后面的尖括号指定原有类模板定义中的参数如何特化.\r\n该参数列表必须与原来未特化的类模板个数相同\r\n\r\n\r\n指针类型的偏特化\r\n例如下面代码: template的第一个参数仍是T1,\r\n但模板名后面可以跟着T*\r\n\r\ntemplate&lt;typename T1, Type value&gt;\nclass ClassName&lt;T1*, value&gt; {};\r\n\r\n特化的选择\r\n当匹配给定特化的每个实参匹配多个特化时, 编译器会选择\r\n最特殊 的一个特化.\r\n\r\n特殊是指有多个匹配, 如果符合A特化, 也符合B特化, 但反过来不行时,\r\n则A比B更特殊 (A含于B)\r\n\r\n\r\n在类模板中使用static_assert()\r\nstatic_assert()接受两个参数,\r\n第一个参数为false时, 输出第二个参数指定的消息.\r\n第一个实参使用type_traits.h中的模板\r\n\r\n\r\ntype_traits\r\n\r\n类模板的友元\r\n对于友元函数和友元类的情况与一般情况相同\r\n\r\n模板友元\r\n\r\n类模板的参数列表一般包含定义友元模板的所有参数\r\n如果类模板的一些参数在友元模板中没有,\r\n则友元模板的实例会用于类模板的几个实例\r\n普通类若有友元模板, 则友元的每一个实例都是这个类的友元\r\n\r\n\r\n","categories":["CS"],"tags":["CS","C++"]},{"title":"first blog | Hello World","url":"/2022/01/11/Life/first/","content":"Finished! My First Blog!\r\nAfter a long time deploying my blog webpage and a lot of other\r\nborthering settings, I finally finished it! I mean, FINALLY!!!\r\n:laughing: :laughing: :laughing:\r\n\r\nOriginal Intention\r\nCan a programmer has no personal blog? I have seen many blogers\r\nwriting their own blogs no metter answering a question or just taking\r\nnotes from time to time on websites such as zhihu and csdn, but among which I prefer is to\r\nestablish a personal website where I can put my blogs on.\r\nSo, at first I have no intention about what to do with my site, maybe\r\nI just feel that it's really cool to have such a lovely home for oneself\r\nto \"lie down and rest\".\r\nBut when it was finally established by myself, experencing a lot of\r\nconfusing problems and taking amount of time to debug, I must to say\r\nthat, I love here, and I believe I will take after it like taking after\r\na baby, a baby who are growing up. :blush:\r\nThanks\r\nI would not finish my work without the help of JerryYang, whose\r\nhelpful blog is the guidance of mine (though there are still some\r\nmistakes maybe? :dizzy_face:). Based on it, I have known some basic\r\ncommand with Linux, Git and Github,\r\nwhich is also beneficial for my lessons next term. Except him I want to\r\nlink some videos there to thank for another ups from bilibili:\r\nusing\r\nhexo to start blog\r\nhow\r\nto writing blogs\r\n","categories":["Life"],"tags":["Blog","Life"]},{"title":"Profile | Welcome","url":"/2023/11/01/Life/profile/","content":"Welcome to my Site\r\nimport seu.chien_shiung_wu as csw\n\nprofile = csw.register(\"Andrew-Rey\").time(\"2020-9\")\n\nprofile.study = \"cs-vr\"\nprofile.current_work = [\"poem2scene\"]\nprofile.published = []  # but will be more and more\n\ncsw.graduate(profile).time(\"2024-6\")\r\n\r\nfind me on github\r\nfind me on\r\nbilibili\r\n","categories":["Profile"],"tags":["welcome"]},{"title":"C++ Programming | CMake Tutorial","url":"/2022/08/10/CS/cmake/","content":"CMake version: 3.x\r\n\r\nCommand Line\r\n# (configure step) create build dir, and generate build/Makefile -> generate Makefile\ncmake -B build\n\n# (build step) invoke building system and build the project in different OS -> generate executable file\ncmake --build build -j4\n\n# invoke building system to execute target \"install\"\ncmake --build build --target install\n\n# define configure variables, only use in configure step\n# use -D\n# set build type in configure step, the value will remain when invoked the second time unless delete build dir\ncmake -B build -DCMAKE_BUILD_TYPE=Release\n\n# Specify generator (generator: generate build system build rule from CMakeLists.txt)\n# use -G\n# generator Ninja, faster than Unix Makefile, generate *.ninja\ncmake -B build -G Ninja\r\nCMakeLists.txt\r\nadd source file\r\n(1). single file: main.cpp\r\nadd_executable(main main.cpp)\r\nor\r\nadd_executable(main)\ntarget_sources(main PUBLIC main.cpp)\r\n(2). multiple files: main.cpp | other.cpp | other.h\r\nadd_executable(main)\ntarget_sources(main PUBLIC main.cpp other.cpp)\r\nor set a new variable\r\nadd_executable(main)\nset(sources main.cpp other.cpp other.h)  # other.h can delete\ntarget_sources(main PUBLIC $&#123;sources&#125;)\r\nor use GLOB to search all files in current dir\r\nadd_executable(main)\nfile(GLOB sources CONFIGURE_DEPENDS *.cpp *.h)  # add CONFIGURE_DEPENDS to detect any change when next build\ntarget_sources(main PUBLIC $&#123;sources&#125;)\r\nwhen we have a dir structure:\r\nmylib\n  +----*.cpp\n  +----*.h\n*.cpp\n*.h\r\nno need to write all files:\r\n# add all file in current dir and mylib dir\nadd_executable(main)\naux_source_directory(. sources)\naux_source_directory(mylib sources)\ntarget_sources(main PUBLIC $&#123;sources&#125;)\r\nor use GLOB_RECURSE to find all files recursely:\r\nadd_executable(main)\nfile(GLOB_RECURSE sources CONFIGURE_DEPENDS *.cpp *.h)\ntarget_sources(main PUBLIC $&#123;sources&#125;)\r\nERROR: use GLOB_RECURSE will include *.cpp files in\r\nbuild dir.\r\nsolution: Add all source files in a dir named\r\nsrc\r\nConfigure variables\r\nCMAKE_BUILD_TYPE: type of build,\r\nRelease, Debug,\r\nMinSizeRel and RelWithDebInfo,\r\ndefualt: none (debug).\r\nset(CMAKE_BUILD_TYPE Release)\r\nset default build type as Release to reach high performance: in the\r\nfirst three lines:\r\nif (NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release)\nendif()\r\n# Specify version of cmake\ncmake_minimum_required(VERSION 3.22)\n\n# set c++ standard\n# don't modify CMAKE_CXX_FLAGS to add -std=c++17\nset(CMAKE_CXX_STANDARD 17)\n\n# if use the needed CXX standard defined.\nset(CMAKE_CXX_STANDARD_REQUIRED ON)  # OFF default\n\n# prevent features GCC only\nset(CMAKE_CXX_EXTENSIONS OFF)\n\n# set project info\nproject(project_name LANGUAGES language_list(such as C CXX ASM...))\r\nLinkable library\r\nadd_executable(main mian.cpp mylib.cpp)\r\nor generate a static library\r\nadd_library(mylib STATIC mylib.cpp)  # create libmylib.a\n\nadd_executable(main main.cpp)\n\ntarget_link_libraries(main PUBLIC mylib)\r\nor generate dynamic lib\r\nadd_library(mylib SHARED mylib.cpp)\n\nadd_executable(main main.cpp)\n\ntarget_link_libraries(main PUBLIC mylib)\r\nor use object lib, no *.a file, let CMake remember which objects\r\nfiles are created\r\nadd_library(mylib OBJECT mylib.cpp)\n\nadd_executable(main main.cpp)\n\ntarget_link_libraries(main PUBLIC mylib)\r\n静态库问题: GCC会自行剔除没有引用符号的对象, 此时使用对象库避免,\r\n从而不会自动剔除没引用到的对象文件, 绕开编译器不统一问题.\r\n动态库也可以避免剔除没引用的对象文件, 但引入了运行时链接的麻烦.\r\n# no specify variable in add_library()\nset(BUILD_SHARED_LIBS ON)  # default OFF\n\nadd_library(mylib mylib.cpp)\r\nHINT 静态库常常被认为直接链接到可执行文件上.\r\n因此在动态库中不要链接静态库. 很呆. 地址会变.\r\n当然解决方法是: 要么转化为对象库,\r\n要么让静态库变成地址无关的代码PIC\r\n# set global property\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\nadd_library(otherlib STATIC otherlib.cpp)\n\nadd_library(mylib SHARED mylib.cpp)\ntarget_link_libraries(mylib PUBLIC otherlib)\n\nadd_executable(main main.cpp)\ntarget_link_libraries(main PUBLIC mylib)\r\nor set local property\r\n# set local property\nadd_library(otherlib STATIC otherlib.cpp)\nset_property(TARGET otherlib PROPERTY POSITION_INDEPENDENT_CODE ON)\n\nadd_library(mylib SHARED mylib.cpp)\ntarget_link_libraries(mylib PUBLIC otherlib)\n\nadd_execuable(main main.cpp)\ntarget_link_libraries(main PUBLIC mylib)\r\nAttributes of objects\r\n设置单属性: set_property(TARGET ... PROPERTY ...);\r\n设置多属性:\r\nset_target_properties(file_name PROPERTIES properties_list)\r\nHINT:\r\n以上命令在add_executable后有效.\r\n设置全局属性 (改变属性的默认值): set(CMAKE_XXX),\r\n在add_executable前设置.\r\n\r\n如果需要在Windows下面使用动态库 (Windows对动态链接不友好),\r\n则需要在定义和声明添加: Deffinition: #include &lt;cstdio>\n\n#ifdef _MSC_VER\n__declspec(dllexport)\n#endif\nvoid sayy_hello()&#123;&#125;\r\nDeclaration: #pragma once\n\n#ifdef _MSC_VER\n__declspec(dllimport)\n#endif\nvoid say_hello(); 然后CMakeLists.txt这样写:\r\n# In Main dir\ncmake_minimum_required(VERSION 3.22)\nadd_subdirectory(mylib)  # add sub module\nadd_executable(main main.cpp)\ntarget_link_libraries(main PUBLIC mylib)\n\n# In sub module dir\nadd_library(mylib SHARED mylib.cpp mylib.h) 然后Windows极有可能会报错: 运行时找不到dll;\r\n原因是dll和exe不在同一目录 (Windows只会查找exe所在目录和PATH). -\r\n把dll添加到PATH环境变量 - 或者dll和dll其他的所有依赖dll,\r\n全部拷贝到exe同一目录\r\n这是因为CMake把main放在build下, 而mylib放在build/mylib/mylib.dll\r\n\r\n因此重定向输出路径, 改变mylib属性, 让.dll文件输出到\r\nPROJECT_BINARY_DIR 里面. set_property(TARGET mylib PROPERTY RUNTIME_OUTPUT_DIRECTORY_(DEBUG | RELEASE | NONE) | ARCHIVE_OUTPUT_DIRECTORY | LIBRARY_OUTPUT_DIRECTORY $&#123;PROJECT_BINARY_DIR&#125;)\r\nExternel library\r\nIn Linux: feel free to link externel libraries. (/usr/lib/...) But\r\nWindows can't. Linux can also include head file directly\r\n(/usr/include/...).\r\nHINT: CMake 的分隔符永远是 \"/\", 即使是Windows,\r\nCMake会自动转化.\r\nMore general method: find_package(package_name REQUIRED)\r\n没听懂, 以后补, 以后也不想补.\r\nVariables and Outputs\r\noutput some log infomation when running cmake -B build,\r\nused for debugging. message(\"log info\")\r\nmessage(STATUS \"status info\")  # -- prefix\r\nmessage(WARNING \"warning info\")  # yellow\r\nmessage(SEND_ERROR \"error info\")  # send error log but continue to run\n\nmessage(FATAL_ERROR \"error info\")  # print error and stop running\r\nVariable and Cache\r\n重复执行cmake -B build: 第一次较慢,\r\n将环境的检测存入缓存, 第二次以及以后直接查看缓存内容.\r\n因此某些错误可以通过删除 ./build/CMakeCache.txt解决.\r\n当然也可以删了整个build文件夹重新编译, 慢一点而已.\r\n","categories":["CS"],"tags":["C++","Programming"]},{"title":"Math in a Mess | Space","url":"/2022/03/20/Math/space/","content":"\"内积空间和度量空间有什么区别? Hilbert空间是什么?\r\n它与线性空间的关系是什么?\"\r\n\"我已经晕了.\"\r\n\r\n数域\r\n是包含0, 1的数集, 且对 中任意两个数的加减乘除运算封闭, 则称\r\n是一个数域.\r\n线性空间\r\n在数域的基础上, 我们提出线性空间的概念:\r\n给定数域  , 和集合 . 有如下映射:   且 () 满足八条基本性质,\r\n则称为一个线性空间.\r\n赋范空间\r\n赋范空间是定义在线性空间之上的.\r\n定义在数域  的线性空间  存在如下映射:  且该映射满足: 正定, 齐次, 三角不等式. 则  是一个赋范空间,\r\n其中映射  称为范数.\r\n内积空间\r\n内积空间是定义在线性空间之上的.\r\n定义在数域  的线性空间  存在如下映射:  则  是一个内积空间.\r\n定义了内积后, 我们可以讨论向量 (即线性空间的元素) 间的长度和夹角,\r\n并进一步讨论正交性等.\r\n注意: 内积本身具有自然定义的范数, 即内积可以诱导出范数, ,\r\n因此内积空间含于赋范空间.\r\n度量空间\r\n度量空间是某个具有距离函数的集合. 该函数定义的是集合内所有元素的距离,\r\n即集合上的某种度量, 即:\r\n给定集合, 有映射:  满足:\r\n\r\n[\r\n\r\n]\r\n\r\n注意: 此处并未要求线性结构.\r\n注意: 赋范空间一定可以诱导出度量空间,\r\n因此赋范空间含于度量空间\r\n完备空间\r\n完备空间又称 Cauchy 空间. 完备空间是定义在度量空间之上的.\r\n若度量空间  中所有的柯西序列都收敛在  中的一点, 则 \r\n是一个完备空间.\r\nHilbert空间\r\n在内积空间的基础上增添完备性条件,\r\n即得到Hilbert空间.\r\n总结\r\n范数运算+向量空间=(线性)赋范空间\r\n(线性)赋范空间 + 内积运算=内积空间\r\n(线性)赋范空间 + 完备性 = Banach 空间\r\n内积空间 + 完备性 = Hilbert 空间\r\n内积空间 + 完备性 + 有限维 = Euclidean 空间\r\nReferences\r\nzhihu:\r\nhttps://www.zhihu.com/question/332144499/answer/731866608\r\nhttps://www.zhihu.com/question/42312263/answer/699451330\r\nwikipedia:\r\nhttps://en.wikipedia.org/wiki/Complete_metric_space\r\nhttps://en.wikipedia.org/wiki/Metric_space\r\nhttps://en.wikipedia.org/wiki/Cauchy_sequence\r\nhttps://en.wikipedia.org/wiki/Cauchy_sequence\r\nhttps://en.wikipedia.org/wiki/Cauchy_sequence\r\nhttps://en.wikipedia.org/wiki/Normed_vector_space\r\n","categories":["Math"],"tags":["Math"]},{"title":"PCG | Perlin Noise","url":"/2023/09/21/Math/PerlinNoise/","content":"“程序化内容生成——地形/材质等的程序化生成”\r\n——关于一些剑走偏锋的项目历程\r\n\r\nPerlin Noise\r\n\r\nabout the Perlin noise: ref\r\n\r\n\r\n4 types of noise that are similar and that are often confused with\r\none another\r\n\r\nclassic Perlin noise\r\nimproved Perlin noise\r\nsimplex noise\r\nvaluse noise\r\n\r\n\r\n\r\nabout the range of Perlin noise: ref\r\n\r\n// mapping a 2D position to a random number range from -1 to 1\nvar perlinValue = PerlinNoise(float x, float y);\r\n\r\nfor texture: x, y stand for pixel position, but multiplied by a small\r\nnumber called the frequency\r\n\r\n\r\nParameters\r\n\r\nfrequency of 2D waves\r\namplitude of 2D waves\r\noctaves: the amount of waves to be generated\r\npersistence: amount of change in size between one curve and the\r\nnext\r\noffset: provide variation in the output\r\nheight scale: scaling factor to accentuate the output generated\r\n\r\n\r\n\r\nProperty if 2 inputs are near to each other, the\r\nresults of the noise function will be near to each other too.\r\n\r\nguarantee continuity\r\n\r\n\r\nGeneration\r\n\r\ngiven a 2D grid as following, the input of Perlin noise is each\r\npixel.\r\n\r\n\r\n\r\n1695262810353\r\n\r\n\r\nassign each gird point a random constant vector. (note:\r\ngridVector[4])\r\nget the vectors pointing from the grid point to the input\r\npoint(target pixel). (note: inputVector[4])\r\nfor each of the 4 corners of the square where the target pixel lies,\r\ncalculate the dot products:\r\nfor i in range(4): calculate dot(gridVector[i], inputVector[i])\r\n\r\nthe dot product means the effects corners value to target\r\npixels\r\n\r\n\r\n\r\n\r\n1695264135606\r\n\r\n\r\ninterpolate between those 4 values and the result is the value of\r\nthe target pixel.\r\n\r\n\r\ndifference between Perlin noise and value noise: Perlin noise use\r\ndot product between 2 vectors to get 4 corners' values\r\nwhile value noise use a pseudo-random number.\r\n\r\nDiscussion\r\n\r\ngradient constant vectors\r\n\r\nwhy we need permutation table(noted as P) &amp;\r\ngradient table(noted as G): P is used to\r\nselect a random gradient from G. P provides randomness and\r\nrepeatability(???)4\r\nhow to generate a permutation table: the core is\r\ndouble and shuffle. we have known that permutation\r\ntable is used to select a gradient from gradient table and one gradient\r\nis defined by (x,y) (which is the grid point position). so one tuple\r\n(x,y) defines one permutation value. so the size of permutation table is\r\n\r\n(double). to guarantee the randomness, we can do shuffle for\r\n. the code to generate\r\npermutation table is (where ):\r\n\r\nvar permutationTable = new int[2 * len(X)];\nfor (var i = 0; i &lt; len(X); i += 1) permutationTable[i] = i;\npermutationTable = Shuffle(permutationTable);\nfor (var i = 0; i &lt; len(X); i += 1) permutationTable[len(X) + i] = permutationTable[i];\n// visit the table given (x,y)\nvar valueTopRight = P[P[x+1]+y+1];\nvar valueTopLeft = P[P[x]+y+1];\nvar valueBottomRight = P[P[x+1]+y];\nvar valueBottomLeft = P[P[x]+y];\r\n\r\nhow to generate a gradient table: use 4 constant\r\nvectors: . so just do modulo with the permutation value given\r\n(x,y) can get one gradient vector.\r\n\r\nVector2 GetConstantVector(v) {\n  switch v % 4:\n    case 0: return Vector2(1f,1f);\n    case 1: return Vector2(1f,-1f);\n    case 2: return Vector2(-1f,1f);\n    case 3: return Vector2(-1f,-1f);\n    default: throw undefined error;\n}\r\n\r\n\r\ninterpolation\r\n\r\nhow to interpotate between such 4 values: 4 values\r\n(a1,a2,b1,b2), firstly interpolate between a1 and a2 which produces v1,\r\nsecondly interpolate between b1 and b2 which produces v2, finally\r\ninterpolate v1 and v2 which produces v, the interpolated value.\r\nwhich interpolation function should be used: if we\r\nuse linear interplation to get our  in , there will be a \"hard transition\" between\r\n3 points (x=0,1,2, while y=2,0,1.5)\r\n\r\n\r\n\r\n1695314437553\r\n\r\nbut if we use an unlinear method, it will be smoothed\r\n\r\n\r\n1695314441924\r\n\r\nthe normally used interpolation function is , the image is:\r\n\r\n\r\n1695314653794\r\n\r\n\r\n\r\nfrequency\r\n\r\nwhat dose frequency means in Perlin noise: consider\r\nthis situation: what is the interpolate value when our target pixel\r\nhappens to be the bottom left grid point? ZERO. because the inputVector\r\nis zero and thus all dot products are zero. to solve ths issue, we\r\ngenerallt multiply the inputs target pixel by a small value called\r\nfrequency.\r\n\r\n\r\n\r\namplitude\r\n\r\nwhat dose amplitude means in Perlin noise: this\r\nwill be used in following section. amplitude is the multiplier before\r\none item.\r\n\r\n\r\n\r\noctave\r\n\r\nwhat dose octave means in Perlin noise: this will\r\nalso be used in following section. when one layer has a frequency that\r\nis double the frequency of the previous layer, this layer is called an\r\noctave.\r\n\r\n\r\nMore?\r\n\r\nFBM: Fractal brownian motion\r\n\r\n\r\n1695315613281\r\n\r\nobviously, the left is better.\r\n\r\nthe left image uses FBM to simulate the terrains in real world.\r\nbut...how?\r\n\r\n\r\n\r\n1695317070446\r\n\r\n\r\nso the high frequencies and low amplitudes generate more details\r\nthan just one single layer, we can keep changing the frequencies and\r\namplitudes in a for-loop, and add them together.\r\n\r\n\r\n","categories":["Math"],"tags":["Unity","PCG","Terrain"]},{"title":"DeepLearning | Image Semantic Segmentation based on UNet","url":"/2022/08/21/ML/UNet/","content":"\"Semantic segmentation of images, use UNet model.\"\r\n\r\nAbstract\r\nIn this project, we realize an basic UNet model and UNet++ model,\r\nthen we apply them on image semantic segmentation. We show our basic\r\ntheory of UNet and an improvement of it, and we provide main code of\r\nthis program. Finally, we give the result of segmentation images,\r\nloss-curve and accuracy-curve on both training and validation set.\r\nThe copyright of this program is owned by our team mentioned on the\r\nend of this blog.\r\nUNet Structure\r\nThe paper published in\r\n2015 propose a noval network structure, whose shape is similar with the\r\ncaptal \"U\". The idea comes from FCNN. U-Net is one of the classes of\r\n\"Encoder-Decoder\" structure.\r\n\r\n\r\nU-Net Structure\r\n\r\nThe front half of the network is \"encoder\". The input image passes\r\ncovolutional kernel, and then passes the pooling layer (or other\r\ndimension-decreasing layer). The opposite of that is the back part of\r\nUNet, the \"decoder\". The input of decoder is a sequence of feature maps\r\nwith highly contracted pixels. The output of the decoder (or the whole\r\nnetwork) is an image with the same shape of input image, where each\r\npixel has its own class.\r\nIn this project, we decrease the number of convolutional layers so\r\nthat there are only two convolutional layers in each convolutional\r\nkernel as the dataset includes images with shape .\r\nOperator Definitions\r\nConvolutional Kernel:\r\nWe define the basic convolutional kernel as follow:\r\nself.layer = nn.Sequential(\n    # in_channel, out_channel, kernel_size, stride, padding\n    # batch size * channel * height * weight\n    nn.Conv2d(C_in, C_out, kernel_size=(3, 3), stride=(1, 1), padding=1),  # 64 64 128 256\n    nn.BatchNorm2d(C_out),\n    nn.Dropout(0.2),\n    nn.LeakyReLU(),\n\n    nn.Conv2d(C_out, C_out, kernel_size=(3, 3), stride=(1, 1), padding=1),  # 64 64 128 256\n    nn.BatchNorm2d(C_out),\n    nn.Dropout(0.5),\n    nn.LeakyReLU(),\r\nIt includes two convolution operations.\r\nDown Sampling Kernel:\r\nAs for downsampling kernel, we replace conditional pooling layer to\r\nconvolutional layer with stride equaling to 2, which means the shape\r\nwill be shrunk to  while\r\nremaining the same channels.\r\nself.Down = nn.Sequential(\n    nn.Conv2d(C, C, kernel_size=(3, 3), stride=(2, 2), padding=1),  # 64 64 64 128\n    nn.LeakyReLU()\n        )\r\nUp Sampling Kernel:\r\nThe basic structure of up-sampling contains only one convolutional\r\nlayer with  convolutional\r\nkernel size and half out-channel. The feature map should pass an\r\ninterpolation layer before getting into the convolutional layer.\r\ndef __init__(self, C):\n    super(UpSampling, self).__init__()\n    # out-channel = 1/2 in-channel\n    self.Up = nn.Conv2d(C, C // 2, kernel_size=(1, 1), stride=(1, 1))\n\n    def forward(self, x, r):\n    # neighbor interpolation\n    up = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n    x = self.Up(up)\n    # concatenate the feature map in encoder and \n    # the feature map in corrsponding decoder layer, in channel dimension\n    res = torch.cat((x, r), 1)\n    return res\r\nThe interpolation mode we choose is \"nearest\". The function\r\ntorch.cat(dim=1) is used to concatenate two feature maps in\r\nchannel dimension.\r\nNetwork Definition\r\nBased on the operators defined above, we link these blocks together\r\nlike UNet structure.\r\ndef __init__(self):\n    super(UNet, self).__init__()\n\n    # down sampling\n    self.C1 = Conv(3, 64)\n    self.D1 = DownSampling(64)\n    self.C2 = Conv(64, 128)\n    self.D2 = DownSampling(128)\n    self.C3 = Conv(128, 256)\n    self.D3 = DownSampling(256)\n    self.C4 = Conv(256, 512)\n    self.D4 = DownSampling(512)\n    self.C5 = Conv(512, 1024)\n\n    # up sampling\n    self.U1 = UpSampling(1024)\n    self.C6 = Conv(1024, 512)\n    self.U2 = UpSampling(512)\n    self.C7 = Conv(512, 256)\n    self.U3 = UpSampling(256)\n    self.C8 = Conv(256, 128)\n    self.U4 = UpSampling(128)\n    self.C9 = Conv(128, 64)\n\n    self.C10 = torch.nn.Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=1)\n    self.pred = torch.nn.Conv2d(3, 34, kernel_size=(1, 1), stride=(1, 1))\n    self.Th = torch.nn.Sigmoid()\r\nLike U-Net mentioned in that paper, we designed 4 layer deep\r\nnetwork.\r\ndef forward(self, x):\n        # part 1: down sampling, decreasing dimension\n        R1 = self.C1(x)\n        R2 = self.C2(self.D1(R1))\n        R3 = self.C3(self.D2(R2))\n        R4 = self.C4(self.D3(R3))\n        Y1 = self.C5(self.D4(R4))\n\n        # part 2: up sampling, connect priori knowledge\n        O1 = self.C6(self.U1(Y1, R4))\n        O2 = self.C7(self.U2(O1, R3))\n        O3 = self.C8(self.U3(O2, R2))\n        O4 = self.C9(self.U4(O3, R1))\n\n        # part 3: active function\n        return self.Th(self.pred(self.C10(O4)))\r\nAs you can see, the difference between U-Net and other networks\r\nbefore U-Net is that U-Net conbines the former information from encoder\r\nand current information from decoder.\r\nCode\r\nDuring the training process, we want to keep some information of loss\r\nvalues and accuracy values on training set and validation set so that we\r\ncan analyze the variance.\r\nIn the function named train(), we take\r\noptimizer and loss as two parameters used in\r\ntraining process. The outputs of this function are loss and accuracy on\r\nboth training set and validation set. If we get the data about training\r\nset and validation set, we can draw the curves. If both training and\r\nvalidation loss values decrease during training process, we can conclude\r\nthat our model converges and does not overfit on training set.\r\nThe training code is shown as follow:\r\nself.model.train()\nfor batch in self.train_loader:\n    batch_num += 1\n    optimizer.zero_grad()\n    rgbs, segs = batch\n    s, _, m, n = segs.shape\n    segs = torch.reshape(segs, (s, m, n))\n    pred_segs = self.model(rgbs).to(self.device)\n    loss_val = loss(pred_segs, segs)\n    loss_val.backward()\n    optimizer.step()\r\nThe data collecting code can be written as follow:\r\nStatistic data of training set\r\nfor ... :\n    with torch.no_grad():\n        if batch_num % 5 == 0:\n            logging.info(f\"batch num {batch_num}, loss {loss_val}\")\n        # delete or add comments when needed\n        train_loss += loss_val\n        # statistic valid classified samples\n        total_pix += s * m * n\n        idx = torch.argmax(pred_segs, dim=1)\n        train_valid_pix += torch.eq(idx, segs).sum().float().item()\ntorch.cuda.empty_cache()\nepoch_acc = train_valid_pix / total_pix\ntrain_epoch_loss.append(train_loss / batch_num)\ntrain_epoch_acc.append(epoch_acc)\r\nStatistic data of validation set\r\nself.model.eval()\nwith torch.no_grad():\n    for valid_batch in self.valid_loader:\n        valid_batch_num += 1\n        rgbs, segs = valid_batch\n        s, _, m, n = segs.shape\n        segs = torch.reshape(segs, (s, m, n))\n        pred_segs = self.model(rgbs).to(self.device)\n        loss_val = loss(pred_segs, segs)\n        valid_loss += loss_val\n        valid_total_pix += s * m * n\n        idx = torch.argmax(pred_segs, dim=1)\n        valid_valid_pix += torch.eq(idx, segs).sum().float().item()\nepoch_acc = valid_valid_pix / valid_total_pix\nvalid_epoch_loss.append(valid_loss / valid_batch_num)\nvalid_epoch_acc.append(epoch_acc)\r\nThe point you should pay attention to is that you should use\r\nwith torch.no_grad() before you do some work that have no\r\nrelation with training process, otherwise your GPU memory will be full\r\nor even overflow.\r\nResult\r\nAfter a long time training, we get the satisfying result with U-Net\r\nmodel.\r\nFormer Model\r\nThe \"former model\" infers the U-Net model, and you will see we use\r\nother upgraded model named \"UNet++\" which will be introduced later.\r\nWe output the segmentation results and their uncertainties.\r\n\r\n\r\npicture 1 result-UNet\r\n\r\nModel Upgrade\r\nFor some reasons, we try another U-Net-like model, Nested UNet,\r\nnamely UNet++. It has a nested convolutional blocks like a pyramid and\r\nthere is a chain passing connectivity between each convolutional block\r\nevery layer.\r\n\r\n\r\nNeseted UNet\r\n\r\nThe black nodes are the same with U-Net model. The green nodes are\r\nwhat Nested UNet newly added. Both green and blue lines are skip\r\npathways that pass connectivities from encoder to decoder.\r\nThe use of Nested UNet gives us a little improvement on final\r\nresults.\r\n\r\nAnalysis\r\nU-Net\r\nWe analyze the loss value and accuracy on both training and\r\nvalidation set:\r\n\r\n\r\nunet loss\r\n\r\nWe find that after 100 epochs, the model has not convergenced yet,\r\nbut the loss on validation decreases to the bottom.\r\n\r\n\r\nunet accuracy\r\n\r\nFrom the accuracy curves, we find that both training set and\r\nvalidation set have increasing accuracy, which means our model does not\r\noverfit.\r\nNested UNet\r\nMeanwhile, we analyze the loss and accuracy of Nested UNet model on\r\nboth training and validation set.\r\n\r\n\r\nnested loss\r\n\r\nWe find that Nested UNet has a faster convergency speed than UNet. It\r\nuses only about 60 epochs. But to our surprise, we find that Neseted\r\nUNet overfit after about only 20 epochs because the validation loss does\r\nnot decrease anymore.\r\n\r\n\r\nnested accuracy\r\n\r\nThe performance on validation accuracy stays the same with UNet\r\nmodel.\r\n","categories":["ML"],"tags":["CS","Deep Learning"]},{"title":"Racket | How to launch a rocket","url":"/2023/11/11/PL/Racket-How-to-Launch-a-Rocket/","content":"\"...Introductory books on programming tend to contain lots of\r\nmaterials about the authors' favorate application discipline: puzzles,\r\nmathematics, physics, music and so on. Such material is natural\r\nbecause programming is obviously useful in all these areas, but it also\r\ndstracts from the essential elements of programming. Hence, we\r\nhave made every attempt to minimize the use of knowledge from other\r\nareas so that we can focus on what computer science can teach\r\nyou about computational problems solving.\"\r\n\r\nshow an image\r\nZerothly, you want to show your favorate image on the screen, such as\r\nthe rocket.\r\nFirstly, maybe you in your DrRacket but I used to use my VSCode\r\nanyway, if you want to show an image, one convinient module is\r\nslideshow, just add #lang slideshow in the\r\nfirst line.\r\nSecondly, just look up the help manual and find a function called\r\nshow-pict, which can display and image. Meanwhile,\r\nbitmap/file can load an image based on the given\r\nstring-like path. So just use as what I did:\r\n#lang slideshow\n(show-pict (bitmap/file \"path/to/image\"))\r\nRun and you will see:\r\n\r\n\r\n1699773585285\r\n\r\nYes, and we will launch it into the sky.\r\ndefine a scene\r\nRacket provides an empty-scene where we can put our objects on it (of\r\ncourse, out of it is just ok), use empty-scene to define.\r\nNow we put our samll rocket overlay it but with a smaller\r\nsize using scale. Before that, you should\r\n(require 2htdp/image) to add package.\r\n(require 2htdp/image)\n(empty-scene 800 600)\n(overlay (scale 0.2 (bitmap/file \"path/to/image\")) scene)\r\nBut this expression maybe a little annoyed as the nested and confused\r\nexpressions. So it's necessary to define some constants and\r\nvariables.\r\n; constants\n(define image-scaler .2)\n(define scene-length 800)\n(define scene-width 600)\n(define x-offset 0)\n(define max-height (/ scene-width 2))\n(define min-height (- 0 (/ scene-width 2)))\n\n; variables\n(define scene (empty-scene scene-length scene-width))\n(define rocket (scale image-scaler (bitmap/file \"path/to/image\")))\n\n; interactions\n(show-pict (overlay rocket scene))\r\n\r\n\r\n1699774549772\r\n\r\n\r\nNote: the CENTER coordinate is (0, 0)\r\n\r\nchange positions\r\nWe use overlay/offset to define the relative position on\r\nour scene. Insert a xy-pair between rocket and\r\nscene to define the rocket's position on scene. If we want\r\nshow another little different positions, just modify the xy-pair to\r\nanother values.\r\nThere we use a function provided by 2htdp/universe named\r\nanimate, which consumes a function as an variable\r\nand apply it to time tick every frame.\r\nInaddition, we prefer to use function to do such\r\nwork.\r\n; functions\n(animate (λ (height) \n  (overlay/offset rocket x-offset height scene)))\r\n\r\n\r\n1699775538374\r\n\r\nrefine the program\r\nAnyway, the rocket can fly safely? But there is a little\r\nmore problems in our program.\r\n\r\nthe rocket flies out of our scene, should stop when flies too\r\nhigh\r\nthe rocket flies with a constant speed, should be accelerated\r\nthe rocket flies start on the scene center, not the land\r\n\r\n; add acceleration in our rocket\n(define acceleration .5)\n(define (get-height t)\n  (/ (* acceleration (* t t)) 2))\n\n; stop when reach to the highest use max or min height defined above\n(define (get-location t)\n  (min (+ min-height (get-height t)) max-height))\n\n; the finally animation\n(define (fly-up t)\n  (overlay/offset rocket x-offset (get-location t) scene))\r\n\r\n\r\n1699776100626\r\n\r\n\r\nalthough you can launch a rocket in some degrees, but however, you\r\nlearnt nothing. I mean you are just coding but with an image or with a\r\nnew programming language. The same problems appear when you construct a\r\nlarger project in your usual life. We not learn functional programming\r\nbut learn the essential elements of programming, learn the construction\r\nof projects and learn how to use these tools to solve computational\r\nproblems.\r\n\r\n","categories":["Programming Language"],"tags":["Racket","Functional Programming"]},{"title":"Paper Reading | Learning Transferable Visual Models From Natural Language Supervision","url":"/2023/10/30/Papers/clip-1/","content":"\r\nNatural Language Supervision\r\nLearning Transferable Visual Models\r\n\r\n\r\n利用自然语言的监督信号来训练一个泛化性能好的视觉模型\r\n\r\n\r\nabstract\r\n\r\n之前的视觉模型训练：先有一个固定的、提前定义好的物体类别集合，模型去预测这些定义好的类别来完成训练。\r\n\r\n收集数据集简单，模型训练简单\r\n有限制的监督信号，限制了模型本身的泛化性\r\n\r\nCLIP\r\n\r\n直接从自然语言得到监督信号\r\n范围广，语言描述过的物体就可以被识别到\r\n给定一张图片，给定一个句子，判断配对\r\n训练样本是图片和句子的配对\r\n数据集规模：4亿\r\n自监督\r\n多模态的对比学习\r\n自然语言用于引导模型做物体分类(prompt)\r\n能扩展到新的任务\r\n大多数迁移效果好\r\n开源了预训练模型，只公开了推理的代码，未公开预训练代码\r\n\r\n\r\nmy focus\r\n\r\ntext2image\r\n\r\nDownstream task\r\nforzen clip model and train generator\r\nencode text\r\n\r\ntext: poems\r\nimages: height maps, grey-scale\r\n\r\n","categories":["Paper"],"tags":["clip","paper","DL","Multimodule"]},{"title":"Project | CSW-Microservice","url":"/2023/10/05/Project/csw-microservice/","content":"\"Microservice platform of Chien-Shiung Wu Colledge\"\r\n\r\nRequirements\r\n\r\n功能性分析\r\n\r\n\r\n用户权限管理\r\n\r\n管理人员权限\r\n普通用户权限\r\n运维人员权限\r\n\r\n书院通知管理\r\n\r\n发布功能\r\n置顶功能\r\n招聘管理\r\n轮播功能\r\n静态内容展示\r\n书院文化展示（lt最爱）\r\n\r\n会议室预约管理\r\n\r\n会议室预约时间展示：展示近几天所有会议室的空闲时间、占用时间\r\n会议审核功能：查看审核协议；审核通过/拒绝用户在会议室预约的会议\r\n会议预约功能：填写会议室、使用时间、会议主题、预约人；提供增添预约、删除预约、修改预约、查看预约，撤回预约等功能\r\n\r\n大厅座位预约管理\r\n\r\n大厅座位空间时间展示：展示座位空间分布 -&gt;\r\n展示某个选中座位的预约时间\r\n座位预约功能：预约时间、预约人\r\n\r\n咖啡预约管理\r\n\r\n咖啡厅餐品展示\r\n咖啡预约功能：口味选择、预期提取时间、预约人\r\n\r\n器材出借管理\r\n\r\n器材管理：增删查改，器材描述\r\n器材出借功能：出借人、出借/归还时间、审批人\r\n\r\n问题上报管理\r\n\r\n问题收集管理：增删查改\r\n问题审批功能：审批人、问题描述、上报人（可选）\r\n\r\n操作日志管理\r\n\r\n日志查看功能\r\n\r\n个人账号管理\r\n\r\n个人信息维护\r\n\r\n密码管理（修改密码，找回密码）\r\n\r\n个人权限维护\r\n个人诚信点管理\r\n\r\n\r\n\r\n非功能性分析\r\n\r\n\r\n用户界面的具体细节\r\n\r\n参照东大信息化（？）\r\n有吴健雄学院的特色\r\n\r\n对稳定性要求要高\r\n\r\n旧的书院系统容易崩溃\r\n\r\n响应时间要尽量快\r\n\r\n实际中会经常面对临时预约会议的情况，此时需要快速响应\r\n\r\n不存在网上交易平台，但仍然需要保证用户权限的分离和安全性\r\n\r\n\r\n设计约束\r\n\r\n\r\n进度约束：本学期结束前有测试版，争取寒假能上线正式版\r\n\r\n\r\n初步讨论使用maven进行项目管理，Springboot+Vue技术栈（相关版本未定）\r\n\r\n\r\n相关use case图\r\n\r\n\r\n预约模块\r\n\r\n\r\n\r\n1696654551214\r\n\r\n\r\n个人信息管理\r\n\r\n\r\n\r\n1696654600025\r\n\r\n\r\n问题反馈\r\n\r\n\r\n\r\n1696654633120\r\n\r\n\r\n浏览模块\r\n\r\n\r\n\r\n1696654672936\r\n\r\n\r\n\r\n1696654662518\r\n\r\n","categories":["Project"],"tags":["CS","Project","SE","Microservice"]},{"title":"Design Pattern | OOD","url":"/2023/10/14/SE/design-pattern-1/","content":"\r\n项目的可维护性和可复用性，项目规范，团队开发\r\n\r\n\r\n单一职责原则\r\n\r\n一个对象应该只包含单一职责，并且该职责被完整地封装在一个类中 -\r\n用于控制类的粒度大小\r\n\r\nclass People(object):\n    def coding(): pass\n    def riding(): pass\n    def cooking(): pass\n    ...\r\n\r\n臃肿：修改任意行为都会修改People类\r\nPeople拥有\r\n不止一个引起它变化的原因\r\n\r\nclass Programmer(object): pass\nclass Rider(object): pass\nclass Chef(object): pass\r\n\r\n类的\r\n粒度更细，根据不同业务划分，采用单一职责原则，高内聚低耦合\r\n\r\n开闭原则\r\n\r\n软件实体应当对扩展开放，对修改关闭 - 扩展开放：对提供方而言 -\r\n修改关闭：对调用方而言\r\n\r\nfrom abc import ABCMeta, abstractmethod\n\n# define abstract class or interface\nclass IProgrammer(metaclass=ABCMeta):\n    @abstractmethod\n    def coding(): pass\n\nclass JavaProgrammer(IProgrammer): \n    # override\n    def coding():\n        java()\n\nclass PythonProgrammer(IProgrammer):\n    # override\n    def coding():\n        python()\r\n\r\n将不同程序员写代码抽象为一个统一的接口/抽象类，不进行实现\r\n\r\n扩展开放：开放了统一的接口coding，不同程序员自由决定如何编程\r\n修改关闭：具体程序员如何重写coding是自己负责，不受别人干扰\r\n\r\n\r\n里氏替换原则\r\n\r\n所有引用基类的地方必须能透明使用其子类的对象（子类能扩展父类的功能）\r\n-\r\n子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法（既然是继承于父类，子类已经不具有父类的原本行为了，如果有这样的需求，一是可以取消继承，二是可以重新写一个方法）\r\n- 子类可以增加自己特有的方法 -\r\n当子类方法重载父类方法时，方法的前置条件（入参）要比父类更加宽松 -\r\n当子类方法重载父类方法时，方法的后置条件（返回值）要比父类更加严格或一样\r\n\r\n依赖倒转原则\r\n\r\n高层模块不依赖于底层模块，它们都应该依赖抽线。抽象不应依赖于细节，细节应该依赖于抽象\r\n- Spring框架\r\n\r\nclass UserMapper(object): \n    # use CRUD\n    pass\nclass UserService(object):\n    # use methods in UserMapper\n    user_mapper = UserMapper()\nclass UserController(object):\n    # use methods in UserService\n    user_service = UserService()\r\n\r\n上述代码结构好，但是如果底层逻辑（例如UserService）发生改变，则上层逻辑也要跟着改变，层次分明但\r\n耦合度高\r\n\r\nfrom abc import ABCMeta, abstractmethod\n\nclass UserMapper(metaclass=ABCMeta): \n    # provides abstract interfaces only\n    pass\nclass UserMapperImpl(UserMapper):\n    # implements interfaces of UserMapper\n    pass\nclass UserService(metaclass=ABCMeta):\n    # provides abstract interfaces only\n    pass\nclass UserServiceImpl(UserService):\n    # use dependency injection to instantiate UserMapper such as:\n    self.user_mapper = None\n    def set_user_mapper(user_mapper: UserMapper): pass\nclass UserController():\n    # use dependency injection to instantiate UserService such as:\n    self.user_service = None\n    def set_user_service(user_service: UserService): pass\r\n\r\n如果修改了底层逻辑（例如UserService），只需要修改其实现类（UserServiceImpl）即可，上层逻辑由依赖注入完成\r\n上层只需要知道接口中定义了什么方法然后使用即可，具体操作内容由接口的实现类完成\r\n使用了 依赖注入\r\n降低了类之间的耦合度\r\n\r\n接口隔离原则\r\n\r\n客户端不应依赖那些它不需要的接口 - 对接口的细化 -\r\n定义接口时要注意接口的粒度\r\n\r\n合成复用原则\r\n\r\n优先使用对象组合，而不是通过继承来达到复用的目的 - 核心是委派 -\r\n在一个新的对象里面使用一些已有的对象，使之成为新对象的一部分，新的对象通过向这些对象的委派达到已有功能复用的目的\r\n\r\n\r\n继承可以实现复用，但\r\n\r\n耦合度高\r\n子类得到父类细节（某些字段或方法），不安全\r\n\r\n组合实现复用\r\n\r\n考虑函数传参，用的时候才传参\r\n考虑成为成员变量，构造时指定\r\n\r\n\r\n迪米特法则/最少知识原则\r\n\r\n每一个软件单位对其它单位都只有最少的知识，而且局限于那些与本单位密切相关的软件单位\r\n- 一个模块与其它模块交互越少越好 - 降低耦合度\r\n\r\ndef main_task():\n    my_secret = Secret()\n    test = Test()\n    # test_start may need one of the property of Secret\n    test.test_start(my_secret)\n\n# in Test\ndef test_start(my_secret: Secret):\n    print(my_secret.sth)\r\n\r\ntest_start参数改为sth: Something减少与其它功能模块的交互\r\n\r\n","categories":["SE"],"tags":["CS","SE","Java","Design Pattern"]},{"title":"Design Pattern | Creational","url":"/2023/10/15/SE/design-pattern-2/","content":"\r\n使用设计模式是为了可重用代码，让代码更容易被他人理解，保证程序的可靠性和重用性\r\n\r\n\r\n\r\n\r\n肯特·贝克和沃德·坎宁安：建筑设计思想 -&gt;\r\nSmalltalk的图形用户接口生成\r\nErich Gamma改为适用于软件开发\r\nJames Coplien致力于C++开发，Advanced C++ Idioms\r\nGang of Four (Erich Gamma, Richard Helm, Ralph Johnson, John\r\nVlissides) 出版 Design Patterns - Elements of Reusable Object-Oriented\r\nSoftware\r\n\r\n\r\nref\r\n工厂方法模式\r\n\r\n创建对象时，不直接使用new来创建对象，而是使用工厂方法模式\r\n-\r\n程序中使用大量的new时，当某个对象/构造方法发生变化时，难以维护\r\n- 将频繁出现的对象创建，封装到一个工厂类中\r\n\r\n简单工厂模式\r\npublic class FruitFactory() &#123;\n    public Fruit getFruit(ID id) &#123;\n        if (id == 0) &#123;return new Apple();&#125;\n        if (id == 1) &#123;return new Peach();&#125;\n        ...\n    &#125;\n&#125;\r\n\r\n当调用方添加新对象时，需要 修改 工厂代码\r\n上述代码不符合 开闭原则：对扩展开放，对修改关闭\r\n\r\nFruitFactory提供给调用方使用，应该对修改关闭，即不要修改工厂代码\r\n但是提供方可以对扩展开放\r\n\r\n\r\n工厂方法模式\r\npublic abstract class FruitFactory&lt;T extends Fruit> &#123;\n    public abstract T getFruit();\n&#125;\npublic class AppleFactory extends FruitFactory&lt;Apple> &#123;\n    @Override\n    public Apple getFruit() &#123;\n        return new Apple();\n    &#125;\n&#125;\n...\r\n\r\n如果新增了新对象，调用方只需要自己添加对应的工厂并继承最高的工厂即可\r\n使用者只需要关心如何使用对象，工厂屏蔽了对象的创建细节\r\n\r\n抽象工厂模式\r\n\r\n工厂方法模式只适用于简单对象，如果需要许多的产品族时会显得乏力\r\n\r\n建造者模式\r\n\r\nbuilder\r\n如果一个对象构造时用的参数过多，可以使用builder优雅地完成构造\r\n\r\npublic class Student &#123;\n    int id;\n    int age;\n    int grade;\n    String name;\n    String college;\n    String profession;\n    List&lt;String> awards;\n\n    public Student(int id, int age, int grade, String name, String college, String profession, List&lt;String> awards) &#123;\n        this.id = id;\n        this.age = age;\n        this.grade = grade;\n        this.name = name;\n        this.college = college;\n        this.profession = profession;\n        this.awards = awards;\n    &#125;\n&#125;\r\n\r\n参数错位，构造方法太长\r\n\r\npublic class Student &#123;\n\t\t...\n\n    //一律使用建造者来创建，不对外直接开放\n    private Student(int id, int age, int grade, String name, String college, String profession, List&lt;String> awards) &#123;\n        ...\n    &#125;\n\n    public static StudentBuilder builder()&#123;   //通过builder方法直接获取建造者\n        return new StudentBuilder();\n    &#125;\n\n    public static class StudentBuilder&#123;   //这里就直接创建一个内部类\n        //Builder也需要将所有的参数都进行暂时保存，所以Student怎么定义的这里就怎么定义\n        int id;\n        int age;\n        int grade;\n        String name;\n        String college;\n        String profession;\n        List&lt;String> awards;\n\n        public StudentBuilder id(int id)&#123;    //直接调用建造者对应的方法，为对应的属性赋值\n            this.id = id;\n            return this;   //为了支持链式调用，这里直接返回建造者本身，下同\n        &#125;\n\n        public StudentBuilder age(int age)&#123;\n            this.age = age;\n            return this;\n        &#125;\n      \n      \t...\n\n        public StudentBuilder awards(String... awards)&#123;\n            this.awards = Arrays.asList(awards);\n            return this;\n        &#125;\n        \n        public Student build()&#123;    //最后我们只需要调用建造者提供的build方法即可根据我们的配置返回一个对象\n            return new Student(id, age, grade, name, college, profession, awards);\n        &#125;\n    &#125;\n&#125;\r\n\r\n最后可以使用链式调用完成对象的创建\r\n\r\npublic static void main(String[] args) &#123;\n    Student student = Student.builder()   //获取建造者\n            .id(1)    //逐步配置各个参数\n            .age(18)\n            .grade(3)\n            .name(\"小明\")\n            .awards(\"ICPC-ACM 区域赛 金牌\", \"LPL 2022春季赛 冠军\")\n            .build();   //最后直接建造我们想要的对象\n&#125;\r\n\r\n还包括 协调者模式\r\n\r\n单例模式\r\n\r\n一个类始终只有一个实例对象/直接使用类的静态方法\r\n\r\n\r\n饿汉式单例\r\n\r\n类加载时将对象创建好\r\n\r\n懒汉式单例\r\n\r\n延迟加载，需要时才创建\r\n线程不安全\r\n\r\n多线程不能保证只创建一次\r\n在创建实例的方法上加锁synchronized（但高并发效率较低）\r\n可以在判断实例是否创建的if语句内加锁\r\nvolatile保证线程可见\r\n\r\n\r\n懒汉式+饿汉式\r\n在懒汉式的内部创建内部类\r\n\r\n由静态内部类持有单例对象，根据类加载特性，仅使用外层类时，不会对静态内部类进行初始化\r\n\r\n\r\n原型模式\r\n\r\n原型对象作为模板，通过复制该对象来创建新对象\r\n\r\n浅拷贝\r\n\r\n基本数据类型将值赋值，引用类型只复制地址，指向的还是原来的对象\r\n\r\n深拷贝\r\n\r\n拷贝为一个全新的对象\r\n\r\nJava拷贝机制\r\n\r\nCloneable接口\r\n\r\n重写clone方法\r\n内层对象依然是引用的复制，用==进行地址比较\r\n应该在clone中处理成员变量\r\n\r\n\r\n","categories":["SE"],"tags":["CS","SE","Java","Disgn Pattern"]},{"title":"Full Stack | Quick Start","url":"/2023/10/12/SE/full-stack/","content":"SpringBoot + Vue development\r\n\r\nJavaEE: SpringBoot + MyBatis Plus\r\nWeb front end: Vue + ElementUI\r\n\r\n\r\n\r\n\r\n\r\nconfigure environment\r\n\r\njdk\r\nIDE: JetBrain IDEA\r\nauto build tool: maven\r\n\r\nprojects auto build\r\ndependencies manegement\r\nstandard development structure\r\n\r\n\r\nmaven usage\r\n\r\nedit local repo (default is under user dir)\r\n(optional) configure mirrors\r\nconfigure self downloaded maven with IDEA\r\n\r\n\r\n\r\n1697109448690\r\n\r\nspringboot\r\n\r\nintro\r\n\r\nframework provided by Pivotal Team\r\nconvention Over Configuration(约定优于配置)\r\nembedded server(Tomcat, Jetty; no war file, just jar)\r\nsimplify maven configuration\r\npure Java\r\n\r\n\r\nspringboot\r\n\r\nSSM\r\n\r\nSpring, Spring mvc, MyBatis\r\ndifficult to configure\r\n\r\nadv\r\n\r\nquickly, simplely\r\n\r\n\r\ninitialize Springboot\r\n\r\nIDEA\r\n\r\nSpring Initializer(maven based infact)\r\n\r\nfill project information(group id &amp; artifact id)\r\ngenerate project on the web,\r\ndownload it and load in\r\nselect jdk\r\n\r\n\r\n\r\ninitialize Springboot\r\n\r\nchoose Maven project\r\nchoose springboot version\r\n\r\n\r\n\r\n1697114528195\r\n\r\ninitialize Springboot\r\n\r\nconfigure project information\r\n\r\n\r\n\r\n1697114538516\r\n\r\ninitialize Springboot\r\n\r\nadd dependencies (web app, choose spring web)\r\n\r\n\r\n\r\n1697114557959\r\n\r\ncoding springboot\r\n\r\nbackend project: receive requests from brower\r\n\r\nuse Components\r\n\r\n\r\n\n@RestController\npublic class DemoController&#123;&#125;\n\r\ncoding springboot\r\n\r\nadd controller member function\r\n\r\n// https://localhost:8080/demo\n@GetMapping(\"/demo\")\npublic String demo()&#123;\n    return \"hello world\";\n&#125;\r\ncoding springboot\r\n\r\nstart the project(yes, no additional code in main)\r\n\r\n\r\n\r\n1697116224333\r\n\r\nhot deployment\r\n\r\nnormal case: every time you modify the code, restart the\r\nproject\r\nneed hot-deployment\r\n\r\nspring-boot-devtools component\r\n\r\nlisten the variations of classpath, trigger Restart\r\nclass loader to reload the class\r\nnot every change needs to restart app (static res, view templates),\r\nuse spring.devtools.restart.exclude to exclude the\r\ndirs/files\r\n\r\n\r\nadd dev-tools dependency in pow.xml\r\n\r\n&lt;dependency>\n    &lt;groupId>org.springframework.boot&lt;/groupId>\n    &lt;artifactId>spring-boot-devtools&lt;/artifactId>\n    &lt;optional>true&lt;/optional>\n&lt;/dependency>\r\n\r\nadd properties in application.properties\r\n\r\nspring.devtools.restart.enabled = true\nspring.devtools.restart.additional-paths = src/main/java\nspring-devtools.restart.additional.exclude = static/**\r\nweb development\r\n\r\nbasic\r\n\r\nrequest\r\nmethods: get request: usualy send request using address\r\nbar, only for aquiring resources; post request: commit\r\nentity to pointed resources, which changes the state or has sideffects\r\non server, more secure (however both unsecure on the view of data\r\ntransmition, using https to encrypt)\r\nentity: usually the object\r\n\r\nspring-boot-starter-web: springboot provides an integrated frame of\r\nmvc, json, tomcat\r\n\r\nwebmvc: basic frame\r\njson: data parser\r\ntomcat: container dependency\r\nauto-configure when ticking spring web\r\n\r\n\r\n&lt;dependency>\n    &lt;groupId>org.springframework.boot&lt;/groupId>\n    &lt;artifactId>spring-boot-starter-web&lt;/artifactId>\n&lt;/dependency>\r\ncontroller\r\n\r\nSpringboot provides @Controller and\r\n@RestController tags\r\n\r\nreceive &amp; handle HTTP requests\r\n@Controller tag: when requesting page\r\nand data, usually with Thymeleaf template\r\nengine (no seperation between front and back ends)\r\n@RestController tag: when requesting only\r\ndata, usually converting object into json formmat in default\r\nwhen return\r\n\r\n\r\nMVC frame\r\n\r\n\r\n1697121109192\r\n\r\n\r\ncontroller receives users' requests, fetches data from model and\r\nreture data to view\r\n\r\nrouter mapping\r\n\r\nreceive user requests\r\n@RequestMapping tag: URL router mapping, defines http\r\nrequest-mapping rule, can be added on controller class or method\r\n\r\nfor class: whole router mapping will add this mapping rule\r\nfor method: only the method itself\r\nproperties:\r\n\r\nvalue: url path, supports url templates, reg-expr,\r\n@RequestMapping(\"/user\")\r\nmethod: http request methods (get, post), also use:\r\n@GetMapping, @PostMapping\r\nconsumes: content-type for requests, e.g.\r\napplication/json\r\nproducer: content-type for resposes (html, json)\r\nparams, headers: requests parameters and requests\r\nheader\r\n\r\n\r\n\r\nparameters passing\r\n\r\nget parameters, such as\r\nhttp://localhost:8080/demo?nickname=zhangsan passing\r\nzhangsan to param: nickname\r\n@RequestParam tag: receiving parameters from http\r\nrequests body or QueryString of url, omitted when request-parameter's\r\nname is the same with controller method\r\n@PathVariable tag: handle dynamic url, the value of url\r\ncan be the parameters of controller handler\r\n@RequestBody tag: receives parameters from requestBody,\r\nhandling data such as\r\napplication/json, application/xml\r\n\r\n@RestController\npublic class DemoController &#123;\n  @RequestMapping(value=\"/demo\", method=RequestMethod.GET)\n  public String myDemo(String nickname, String phone) &#123;\n    return \"nickname: \" + nickname + \" phone: \" + phone;\n  &#125;\n&#125;\r\n\r\nmy request:\r\nhttp://localhost:8080/demo?nickname=xr&amp;phone=123\r\noutput:\r\n\r\n\r\n\r\n1697123584376\r\n\r\n\r\nnow parameter mapping handles the situation where request-parameters\r\nconflict with method parameters\r\n\r\n@RestController\npublic class DemoController &#123;\n  @RequestMapping(value=\"/demo\", method=RequestMethod.GET)\n  public String myDemo(@RequestParam(\"nickname\", required = false) String nickname, String phone) &#123;\n    return \"nickname: \" + nickname + \" phone: \" + phone;\n  &#125;\n&#125;\r\nstatic resources visit\r\n\r\nput self static resources here\r\nvisit by http://localhost:8080/test.png\r\n\r\n\r\n\r\n1697173796277\r\n\r\n\r\nmodify static path pattern: add\r\nspring.mvc.static-path-pattern=/images/** to\r\napplication.properties\r\n\r\nupload files\r\n\r\nfront end form\r\n\r\nmodify enctype=\"multipart/form-data\", otherwise can't\r\nupload files\r\n\r\ntomcat limits the size of files by default:\r\n\r\nsingle file &lt; 1MB\r\nsingle request &lt; 10MB\r\nmodify the size\r\n\r\n\r\nspring.servlet.multipart.max-file-size=10MB\nspring.servlet.multipart.max-request-size=10MB\r\n\r\nreceiver data-type: MultipartFile\r\nreceiver writing method: transferTo\r\n\r\nattention: the parameter name should be same with\r\nfront end form name\r\ninterceptor\r\n\r\nsituation: use every function after login judge login state in ANY\r\ncontroller -&gt; code repeating extract to intercepter\r\n\r\n\r\nauthority checking\r\nperformance monitoring\r\ngeneral behaviour: reading cookie to get user information and put\r\nit to request\r\nSpringboot interface: HandlerInterceptor\r\n\r\npreHandle\r\npostHandle\r\nafterCompletion\r\n\r\n\r\n\r\n\r\n1697177213073\r\n\r\nhow to define\r\n\r\nextend from system HandlerInterceptor\r\noverride parents' method used, usually preHandle\r\nregister interceptor\r\n\r\n// in com.example.demo.interceptor.LonginInterceptor\npublic class LoginInterceptor implements HandlerInterceptor &#123;\n    @Override\n    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;\n        System.out.println(\"LoginInterceptor\");\n        return true;\n    &#125;\n&#125;\n\n// in com.example.demo.config.WebConfig\npublic class WebConfig implements WebMvcConfigurer &#123;\n    @Override\n    public void addInterceptors(InterceptorRegistry registry) &#123;\n        // interceptor for the path /user/**\n        registry.addInterceptor(new LoginInterceptor()).addPathPatterns(\"/user/**\");\n    &#125;\n&#125;\r\nbuild RESTful serve\r\n\r\nframework rule\r\npopular website software service framework designing\r\nstyle\r\nbasic rule, style\r\nevery URI maps a kind of resource\r\nGET for acquiring resources, POST for\r\ncreating/updating resources, PUT for updating resources,\r\nDELETE for deleting resources\r\n\r\nHTTP method\r\n\r\n\r\n1697178943507\r\n\r\n\r\njust a style, not standard requirements\r\n\r\nHTTP state code\r\n\r\n1xx: info\r\n2xx: success\r\n3xx: redirection\r\n4xx: client error\r\n5xx: server error\r\n\r\nhow to write RESTful API\r\n\r\nuse tags\r\nfor RESTful framework: every address corresponds a resource, so URI\r\nis suggested to be no-verb, pure nouns, and these nouns should\r\nconrresponds to database's sheet\r\n\r\npass parameters using address: @PathVariable in id\r\n\r\n\r\nSwagger\r\n\r\ngenerate API document\r\n\r\ndescription\r\ndebugging\r\ndynamically\r\n\r\nconfiguration:\r\n\r\n\n&lt;!--in pow.xml-->\n&lt;dependency>\n  &lt;groupId>io.springfox&lt;/groupId>\n  &lt;artifactId>springfox-swagger2&lt;/artifactId>\n  &lt;version>2.9.2&lt;/version>\n&lt;/dependency>\n\n&lt;dependency>\n  &lt;groupId>io.springfox&lt;/groupId>\n  &lt;artifactId>springfox-swagger-ui&lt;/artifactId>\n  &lt;version>2.9.2&lt;/version>\n&lt;/dependency>\r\nspring.mvc.pathmatch.matching-strategy=ant_path_matcher\r\n\r\nvisit: http://localhost:8080/swagger-ui.html\r\n\r\nuse swagger notation(tag) add methods docs\r\nswagger\r\ntag usage\r\n\r\nswagger convinent front end developer to look up the apis\r\nanother important thing: swagger page can send requests to debug the\r\napis\r\n\r\n\r\nrecommend springboot 3.0+ is recommended to use\r\nspringdoc\r\n\r\nMyBatis Plus (DB\r\nconfiguration)\r\nORM\r\n\r\nObject relation mapping\r\n\r\nhandle OOP objects -&gt; DB storage or DB storage -&gt; OOP\r\nobjects\r\n\r\n\r\n\r\n\r\n1697182386692\r\n\r\n\r\nMyBatis: ORM frame\r\nMyBatis-Plus: simpified MyBatis\r\n\r\nuse component: mapper\r\ndb-related operations: in mapper package\r\n\r\n\r\nconfiguration\r\n\r\nadd dependencies(pow.xml)\r\n\r\nmybatis-plus\r\nmysql\r\nconnection pool\r\n\r\n\r\n&lt;!--\t\tmybatis-plus dependency-->\n&lt;dependency>\n    &lt;groupId>org.mybatis.spring.boot&lt;/groupId>\n    &lt;artifactId>mybatis-spring-boot-starter&lt;/artifactId>\n    &lt;version>3.0.1&lt;/version>\n&lt;/dependency>\n\n&lt;dependency>\n    &lt;groupId>com.baomidou&lt;/groupId>\n    &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId>\n    &lt;version>3.5.2&lt;/version>\n&lt;/dependency>\n\n&lt;!--\t\tmysql driver-->\n&lt;dependency>\n    &lt;groupId>mysql&lt;/groupId>\n    &lt;artifactId>mysql-connector-java&lt;/artifactId>\n    &lt;version>5.1.47&lt;/version>\n&lt;/dependency>\n\n&lt;!--\t\tsql connection pool-->\n&lt;dependency>\n    &lt;groupId>com.alibaba&lt;/groupId>\n    &lt;artifactId>druid-spring-boot-starter&lt;/artifactId>\n    &lt;version>1.1.20&lt;/version>\n&lt;/dependency>\r\n\r\nglobal configuration(application.properties)\r\n\r\nspring.datasource.type=com.alibaba.druid.pool.DruidDataSource\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\nspring.datasource.url=jdbc:mysql://localhost:3306/mydb?userSSL=false\nspring.datasource.username=root\nspring.datasource.password=root\nmybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl\r\n\r\ncreate MySQL data DB, schema and Tables\r\n\r\n\r\n\r\n1697187931126\r\n\r\n\r\nadd tags on Activation Class (main class):\r\n@MapperScan(\"com.example.demo.mapper\"), scan mapper\r\npackages before running\r\ncreate mapper interface, and implemented by MyBatis (use\r\n@Mapper tag on the interface, meaning this is a component,\r\nit will use Dynamic Proxy to instanciate an object with\r\n@Autowired. And all sql sentences are finished by\r\ntags)\r\n\r\n@Mapper\npublic interface UserMapper &#123;\n    // add declarations, impl by Mybatis\n    // query all users\n    // find in user db (named mydb), configured in application.properties\n    @Select(\"select * from user\")\n    public List&lt;User> query();\n\n    // return type stand for the number of records be inserted\n    @Insert(\"insert into user values (#&#123;id&#125;, #&#123;username&#125;, #&#123;password&#125;, #&#123;gender&#125;)\")\n    public int insert(User user);\n&#125;\r\n\r\nfor a more simply form, namely use MyBatis-Plus, that's all:\r\n\r\nattention that the class name should be the same with your database\r\ntable name, unless you use tags\r\n\r\n\r\n@Mapper\npublic interface UserMapper extends BaseMapper&lt;User> &#123;&#125;\r\n\r\nuse the mapper to edit database data\r\n\r\ncreate controller UserController (we have created\r\nUser entity before)\r\n\r\n\r\n@RestController\npublic class UserController &#123;\n\n    @Autowired\n    private UserMapper userMapper;\n    @GetMapping(\"/user\")\n    public List&lt;User> query() &#123;\n        return userMapper.query();\n    &#125;\n\n    @PostMapping(\"/user\")\n    public String insert(User user) &#123;\n        var state = userMapper.insert(user);\n        if (state &lt;= 0) &#123;\n            return \"insert fail\";\n        &#125;\n        return \"insert success\";\n    &#125;\n&#125;\r\n\r\nwhere @Autowired is an injection method, which helps to\r\ninstanciate UserMapper\r\n\r\nAxios\r\n\r\na frame of front-end requesting - front-end needs data, browser sends\r\nHTTP requests to server to fetch data, Vue bonds data - based on\r\nAjax, promise - use\r\nXMLHttpRequests to send web requests, convert\r\njson data automatically npm install axios\r\n\r\n帮助文档\r\nsend web requests\r\nsend GET requests\r\naxios.get(&#39;&#x2F;user?ID&#x3D;12345&#39;)\n  .then(function(response) &#123;\n    &#x2F;&#x2F; handle successful situations\n    console.log(response);\n  &#125;)\n  .catch(function(error) &#123;\n    &#x2F;&#x2F; handle failures\n    console.log(error);\n  &#125;)\n  .then(function() &#123;\n    &#x2F;&#x2F; always execute\n  &#125;);\n\n&#x2F;&#x2F; the same with:\n\naxios.get(&#39;&#x2F;user&#39;, &#123;\n    params: &#123;\n      ID: 12345\n    &#125;\n  &#125;)\n  .then(function(response) &#123;\n    &#x2F;&#x2F; handle successful situations\n    console.log(response);\n  &#125;)\n  .catch(function(error) &#123;\n    &#x2F;&#x2F; handle failures\n    console.log(error);\n  &#125;)\n  .then(function() &#123;\n    &#x2F;&#x2F; always execute\n  &#125;);\r\nsend POST requests\r\n&#x2F;&#x2F; request body is the second parameter\naxios.post(&#39;&#x2F;user&#39;, &#123;\n    firstName: &#39;xxx&#39;,\n    lastName: &#39;xxx&#39;\n  &#125;)\n  .then(function(response) &#123;\n    &#x2F;&#x2F; handle successful situations\n    console.log(response);\n  &#125;)\n  .catch(function(error) &#123;\n    &#x2F;&#x2F; handle failures\n    console.log(error);\n  &#125;)\n  .then(function() &#123;\n    &#x2F;&#x2F; always execute\n  &#125;);\r\nwhen to send requests\r\ncreated:function()&#123;&#125;: invoked when creating the\r\ncomponents\r\nmounted:function()&#123;&#125;: invoked when mounting the\r\ncomponents\r\ncross-domain\r\n\r\nusually appears in front-back end separation projects\r\nsame-origin-policy: the basic and core security\r\nfunction of browsers\r\n\r\nsame-origin, i.e. same-domain, where two page have the same\r\nprotocol, host and port\r\n\r\ncross-domain: request url appears when any of\r\nprotocol, host and port is different from current page url\r\n\r\ncan't read cross-domain cookies\r\ncan't send Ajax requests to cross-domain addresses\r\n\r\nauthorization to solve cross-domain questions\r\n\r\nCORS(cross-origin resource sharing): designed by W3C\r\nsimple requests/complex requests\r\nback-end server realize CORS interfaces\r\n\r\n\r\nfor single controller: @CrossOrigin\r\nfor global controler: add configuration class\r\n\r\nsend requests when the component is created:\r\n\r\n&#x2F;&#x2F; use arrow function: &quot;this&quot; points to parents&#39; &quot;this&quot;, able to visit parameters of parents &quot;tableData&quot;\ncreated:function()&#123;\n  axios.get(&quot;http:&#x2F;&#x2F;localhost:8088&#x2F;user&#x2F;findAll&quot;).then((response)&#x3D;&gt;&#123;\n    this.tableData &#x3D; response.data\n  &#125;)\n&#125;,\ndata() &#123;\n  return &#123;\n    tableData: []\n  &#125;\n&#125;\r\nset base url\r\n\r\naxios.defualts.baseURL = 'http://api.com'\r\napp.config.globalProperties.$http = axios\r\n\r\nJWT\r\n\r\nauthentication: get resources from server after authentication\r\n\r\n\r\nsession authentication\r\n\r\nuser sends username and password to\r\nserver\r\nserver check the user information\r\nif the login info is right\r\ngenerate cookies, return a session id to user\r\nthe session id will be sent to server again through cookie when user\r\nsends other requests\r\nserver receives the session id and find out the saved data to know\r\nthe user's role (or related data)\r\ndrawbacks: difficult to expand especially for\r\nserver-clusters. (other server has no session id of user as the user\r\nsend session id to another server)\r\n\r\ntoken authentication\r\n\r\na way to solve the drawbacks of session authentiocation\r\nstring-like\r\nstore token in client (such as cookie or localStorage)\r\neverytime user request for server should send token to server at the\r\nsame time\r\ndefend the user to modify the token contents\r\n\r\n\r\nJson web token\r\n\r\na method to realize the token\r\n\r\n\r\ngenerate a JsonObj after the authentication and send it to user\r\nuser communicates with server should send the JsonObj to server, and\r\nthe server only realies on the JsonObj to distinguish the authority of\r\nuser\r\nin order to avoid user modifying the token, server add signature to\r\ntoken when generating it\r\n\r\n3 parts: divided by .\r\n\r\nheader: meta info, will be encode(not encrypt)\r\npayload: data that need to be pass by, will be encode\r\nsignature: appoint a secret-key which only know by\r\nserver, use the signature algrithm defined in header and\r\ngenerate signature by the formula:\r\n\r\nHMACSHA256(\n  base64UrlEncode(header) + &quot;.&quot; + \n  base64UrlEncode(payload),\n  secret\n)\r\nthe token is just a string divided by .:\r\nheader.payload.signature\r\n\r\n\r\n1700222447717\r\n\r\n","categories":["SE"],"tags":["SE","Full Stack","Springboot","Vue"]},{"title":"Inverse Analysis | Requirements Analysis","url":"/2023/10/05/SE/reverse-analysis-0/","content":"\"项目负责人对用户需求的理解程度，在很大程度上决定了项目的成败\"\r\n\r\n需求分析的目的\r\n\r\n更好地了解、分析、明确用户需求，并能够准确清晰地以文档形式表达给参与项目开发的每个成员，保证项目开发按照用户需求的方向进行。\r\n\r\n\r\n需求分析的物质性结果是\r\n软件功能描述书（此外，一般还需要编写用户调查报告和市场调研报告）\r\n\r\n需求分析的内容\r\n\r\n功能性分析\r\n\r\n必须实现哪些功能\r\n向用户提供功能时需要执行的动作\r\n形成软件需求规格说明书\r\n\r\n非功能性分析\r\n\r\n用户界面具体细节\r\n软件性能、可靠性、响应时间需求\r\n运行环境需求\r\n相关标准、规范\r\n安全需求\r\n架构需求\r\n未来可能的扩充需求\r\n\r\n设计约束\r\n\r\n进度约束\r\n预算约束\r\n资源约束\r\n其它\r\n\r\n\r\n需求分析的人员分工\r\n\r\n项目管理者：组织\r\n人员与用户进行交流，组织 人员编写项目功能描述书\r\n\r\n\r\n开发人员：与用户一起进行需求分析\r\n\r\n\r\n美术和技术骨干代表或者全体成员（与用户讨论）编写项目的功能描述书初稿\r\n\r\n\r\n相关人员对功能书的初稿进行修改和完善，形成正式文档\r\n\r\n\r\n用户若有能力，可以参与至功能描述书的编写和修改中\r\n\r\n用户调查报告\r\n\r\n用户的充分配合，必要时需要对用户进行培训。\r\n\r\n\r\n调查的形式：发需求调查表、开需求调查座谈会或者现场调研\r\n\r\n调查内容\r\n\r\n网站当前以及日后可能出现的功能需求\r\n客户对网站的性能(如访问速度)的要求和可靠性的要求\r\n确定网站维护的要求\r\n网站的实际运行环境\r\n网站页面总体风格以及美工效果(必要的时候用户可以提供参考站点或者由公司向用户提供)\r\n主页面和次级页面数量，是否需要多种语言版本等\r\n内容管理及录入任务的分配\r\n各种页面特殊效果及其数量(js，flash等)\r\n项目完成时间及进度(可以根据合同)\r\n明确项目完成后的维护责任\r\n\r\n调查报告的重点内容\r\n\r\n调查概要说明：网站项目的名称;用户单位;参与调查人员;调查开始终止的时间;调查的工作安排。\r\n调查内容说明：用户的基本情况;用户的主要业务;信息化建设现状;网站当前和将来潜在的功能需求、性能需求、可靠性需求、实际运行环境;用户对新网站的期望等。\r\n调查资料汇编：将调查得到的资料分类汇总(如调查问卷，会议记录等等)\r\n\r\n市场调研报告\r\n\r\n市场调研的目的：清晰地分析相似网站的性能和运行情况，帮助项目负责人清楚地构想出自己开发的网站的大体架构和模样，总结同类网站优势和缺点；明确并引导用户需求\r\n\r\n\r\n应尽可能调研到所有比较出名或优秀的同类网站，了解同类网站的使用环境和用户的诧异点。\r\n\r\n调研内容\r\n\r\n市场中同类网站作品的确定。\r\n调研作品的使用范围和访问人群。\r\n调研产品的功能设计(主要模块构成，特色功能，性能情况等等)\r\n简单评价所调研的网站情况。\r\n\r\n调研报告的重点内容\r\n\r\n调研概要说明：调研计划;网站项目名称、调研单位、参与调研、调研开始终止时间。\r\n调研内容说明：调研的同类网站作品名称、网址、设计公司、网站相关说明、开发背景、主要适用访问对象、功能描述、评价等项目管理者联盟\r\n可采用借鉴的调研网站的功能设计：功能描述、用户界面、性能需求、可采用的原因。\r\n不可采用借鉴的调研网站的功能设计：功能描述、用户界面、性能需求、不可采用的原因。\r\n分析同类网站作品和主要竞争对手产品的弱点和缺陷以及本公司产品在这些方面的优势。\r\n调研资料汇编：将调研得到的资料进行分类汇总。\r\n\r\n软件功能描述书\r\n描述书的重点内容\r\n\r\n网站功能\r\n网站用户界面(初步)\r\n网站运行的软硬件环境\r\n网站系统性能定义\r\n网站系统的软件和硬件接口\r\n确定网站维护的要求\r\n确定网站系统空间租赁要求\r\n网站页面总体风格及美工效果。\r\n主页面及次页面大概数量。\r\n管理及内容录入任务分配。\r\n各种页面特殊效果及其数量。\r\n项目完成时间及进度(根据合同)\r\n明确项目完成后的维护责任。\r\n\r\n","categories":["SE"],"tags":["CS","SE","reverse analysis"]},{"title":"Full Stack | RuoYi-Vue","url":"/2023/11/04/SE/ruoyi-vue/","content":"\"a glance of RuoYi-vue framework\"\r\nref\r\n\r\nconfiguration\r\n\r\nmysql configuration\r\n\r\nuse datagrip, run scrips in sql/*.sql\r\nconstruct tables\r\n!!IMPORTANT!! use utf8 when creating your database\r\nedit configuration file\r\nruoyi-admin/src/main/resources/application-druid.yml to\r\nyour database\r\n\r\nstart redis server\r\n\r\nin cmd: redis-server\r\nedit configuration file\r\nruoyi-admin/src/main/resources/application.yml to your\r\nserver port\r\n\r\nstart back-end\r\n\r\nin ruoyi-admin/ find RuoYiApplication.java\r\nand run it\r\nsuccess when visiting http://localhost:8080\r\n\r\n\r\n\r\n\r\n1699079331780\r\n\r\n\r\nstart front-end\r\n\r\ndownload the dependencies\r\nin ruoyi-ui/\r\nnpm run dev\r\n\r\n\r\n\r\n\r\n1699079479931\r\n\r\n\r\nsuccess when the validation code image appears -&gt;\r\nfront-back-end\r\n\r\nvalidation code\r\n\r\nthe first interactions between front and back\r\n\r\n\r\nback-end generate a expr: such as 1+1=2 generate\r\n1+1=?@2, where @ divides question and anwser -\r\ngive 1+1=? to front - store 2 in redis\r\n(会给前端传一份以确定是哪个key，放在隐藏域，用户提交表单) - confirm user\r\ninput and redis anwser\r\n\r\nfront-end\r\n\r\ncross-field\r\n\r\n反向代理，映射到后端\r\n\r\n\r\n\r\n\r\n1699081297521\r\n\r\n\r\n将/dev-api替换为空，再映射到8080\r\n\r\n例如：http://localhost:80/dev-api/captchaImage -&gt;\r\nhttp://localhost:8080/captchaImage\r\n\r\n\r\nback-end\r\n","categories":["SE"],"tags":["Full stack","Framework","RuoYi"]},{"title":"Math | Dual Number","url":"/2023/11/16/Math/dual-number/","content":"Given two real numbers  and\r\ndefines a symbol \r\nsatisfying  while\r\n, the number  is called Dual\r\nnumber.\r\n\r\nDifferentiation\r\nThe most interesting part (in my opinion) is the differentiation of\r\ndual number functions. And it's is the base of auto-differentiation used\r\nin tensorflow or pytorch.\r\nConsidering a function \r\nwhere  is a real number. In the\r\nspace of dual number, our  can be\r\nrepresented as \r\nwhere . In real\r\nspace, we can use  expansion\r\non :\r\n\r\nUse  to\r\nreplace the  in , and use  expansion again:\r\n\r\nAnd the amazing place is the definition of  which satisfies  while , so the expansion can be\r\nwritten as:\r\n\r\n\r\nNote: replace the independent variable in\r\nreal-function with there dual number representation, the function can be\r\nrepresented by the form of dual number where the first item is the\r\nreal-form of itself and the second item is its\r\nderivative.\r\n\r\n","categories":["Math"],"tags":["Math","Number Theory"]},{"title":"Math in a Mess | Automatic differentiation","url":"/2023/11/18/Math/AutoDiff/","content":"The automatic differentiation (AutoDiff) has been widely used in\r\nnerual network, which is a set techniques to evaluate the partial\r\nderivative of a function specified by a computer program. AutoDiff is\r\ndistinct from symbolic differentiation and numerical differentiation.\r\nSymbolic differentiation suffers from being converted into computer\r\nprograms while the numerical differentiation leads to round-off errors\r\nin the discretization process and cancellation.\r\n\r\nChain Rule\r\nThere are two ways to realize AutoDiff, but the main idea of\r\nautomatic differentiation is chain rule. Take\r\nfunction  as an example, the\r\npartial derivative w.r.t the independent variable  is:\r\n\r\nFor a more general form, considering a composite function of , its partial drivative is:\r\n\r\nThe two type of AutoDiff:\r\n\r\nforward accumulation: inside to outside, . For\r\neach independent variable, a separate pass is\r\nnecessary. So this method is more efficient for function .\r\nreverse accumulation: outside to inside, . This method\r\nevaluates the function first and calculates the derivatives w.r.t\r\nall independent variables in an additional\r\npass. So this method is more efficient for function .\r\n\r\nComputational Graph\r\nThe graph that shows the composition relations and orders of one\r\nfunction is computational graph. The detail structure of computational\r\ngraph is shown as follow:\r\n\r\n\r\n1700323960325\r\n\r\nThis graph represents the function:\r\nf = w5\nw5 = w3 + w4\nw4 = a0 * w2\nw3 = a1 * w1\nw2 = w0\nw1 = w0\nw0 = x\r\nForward Accumulation\r\n\r\nfix one variable that you want to perfrom differentiation on\r\ncompute the derivative of each sub-expression recursively\r\n\r\n\r\nFinally, calculate  where . This is a\r\nBottom-up method. Infact,  for a node satisfies . For multiple variable\r\n(say, the vector input), this can be matrix product (Jacobians).\r\nTake the function  as an example, we want to know the derivative on the\r\npoint . Notation: . We choose the independent variable  as our target to perform\r\ndifferentation., which also called a seed. The seed values are\r\ninitialized to:\r\n\r\nAs the seed values are set, the values propagate using the\r\nchain rule:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOperations to compute value\r\nOperations to compute derivative\r\n\r\n\r\n\r\n\r\n\r\n (seed)\r\n\r\n\r\n\r\n (seed)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSo, once given the value of point , we can know the derivative of . If you want to\r\nknow , you\r\nshould conduct the steps above again with the seed .\r\nThe computational graph is\r\n\r\n\r\nforward\r\n\r\nReverse Accumulation\r\n","categories":["Math"],"tags":["Math","differentiation"]},{"title":"Paper | quick palm","url":"/2023/11/18/Papers/quick-palm/","content":"Quick Start\r\n\r\nfew shot\r\nFrom\r\nInstance to Metric Calibration: A Unified Framework for Open-World\r\nFew-Shot Learning. IEEE Transactions on Pattern Analysis and Machine\r\nIntelligence (TPAMI'23)\r\nLearning\r\nto Learn from Corrupted Data for Few-Shot Learning. In: Proceedings of\r\nthe 32nd International Joint Conference on Artificial Intelligence\r\n(IJCAI'23)\r\nBoosting\r\nFew-Shot Open-Set Recognition with Multi-Relation Margin Loss. In:\r\nProceedings of the 32nd International Joint Conference on Artificial\r\nIntelligence (IJCAI'23)\r\nConditional\r\nSelf-Supervised Learning for Few-Shot Classification. In: Proceedings of\r\nthe 30th International Joint Conference on Artificial Intelligence\r\n(IJCAI'21)\r\nkernel/hyperbolic/traditionalML\r\nTowards\r\nKernelizing the Classifier for Hyperbolic Data. Frontiers of Computer\r\nScience (FCS'24)\r\nIndefinite\r\nTwin Support Vector Machine with DC Functions Programming. Pattern\r\nRecognition (PR'22)\r\nCosNet:\r\nA Generalized Spectral Kernel Network. In: Advances in Neural\r\nInformation Processing Systems (NeurIPS'23)\r\nExpanding\r\nthe Hyperbolic Kernels: A Curvature-aware Isometric Embedding View. In:\r\nProceedings of the 32nd International Joint Conference on Artificial\r\nIntelligence (IJCAI'23)\r\nAutomatically\r\nGating Multi-Frequency Patterns through Rectified Continuous Bernoulli\r\nUnits with Theoretical Principles. In: Proceedings of the 31st\r\nInternational Joint Conference on Artificial Intelligence\r\n(IJCAI'22)\r\ncv/super-resolution\r\nFATE:\r\nA Three-stage Method for Arithmetical Exercise Correction. Neural\r\nComputing and Applications (NCA'23)\r\nImproving\r\nScene Text Image Super-resolution via Dual Prior Modulation Network. In:\r\nProceedings of the 37th AAAI Conference on Artificial Intelligence\r\n(AAAI'23)\r\nothers\r\nLearning\r\nGraph-level Representation from Local-structural Distribution with Graph\r\nNeural Networks\r\n","categories":["Papers"],"tags":["paper","palm"]},{"title":"DeepLearning | Schedual","url":"/2023/11/23/DL/schedual/","content":"任务\r\n\r\n场景文本图像增强与识别（Scene Text Image Super-Resolution &amp;\r\nRecognition\r\n），这个任务是需要设计一个co-training的模型，能够既提升图像质量又提升下游识别器的识别性能，所以需要对场景文本图像超分辨率（这个任务的目标是放大场景图像图像的尺寸后（如32\r\nx 128变为64 x\r\n256），仍能保持良好的图像质量和文本识别模型的识别效果）以及场景文本图像识别具有一定的基础知识铺垫\r\n模型基本结构是统一的encoder，然后有两个decoder分别负责\r\n超分和识别\r\n\r\n相关论文\r\n\r\nSTISR：需要掌握的模型有20-ECCV-TSRN（创立这个任务的）、22-CVPR-TATT（一个比较好的学习代码的模型）、23-MM-STIRER\r\nSTR：需要掌握的模型有22-IJCAI-SVTR、22-ECCV-PARSeq、23-ICCV-CCD\r\nTransformer与对比学习：李宏毅2021春机器学习课程视频：https://www.bilibili.com/video/BV1Wv411h7kN?p=49&amp;vd_source=7e3e00e12a59319e12d36c03a368ada5\r\n&amp;\r\n跟李沐学AI：https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.788&amp;vd_source=7e3e00e12a59319e12d36c03a368ada5\r\n&amp;\r\nhttps://www.bilibili.com/video/BV19S4y1M7hm/?spm_id_from=333.788&amp;vd_source=7e3e00e12a59319e12d36c03a368ada5\r\n\r\n计划与安排\r\n\r\n目前的计划安排：\r\n\r\n接下来一周阅读这两个方向的论文（每个方向1篇）并学习代码，并于一周后参与组内讨论，对论文进行分享\r\n随后根据分享与讨论情况布置后续工作与任务安排\r\n后续深造期间的研究方向以这个工作为契机了解场景文本图像相关的内容后再进行确定\r\n毕设方面严卡节点：包括论文翻译、开题报告与答辩、中期、毕业论文（4月底-5月下旬需要撰写论文（3-4周）（即在此节点之前核心实验与算法需要跑完，写论文过程中可以继续增加消融实验），审核并修改（约需2轮，1周）），尽量提前\r\n\r\n\r\n其他\r\n\r\n科研常用的工具：\r\n\r\nTypora：非常好用的Markdown文本编辑器\r\n\r\nhttps://github.com/Keldos-Li/typora-latex-theme\r\nLatex风格的基于Typora的md排版\r\n\r\nMathpix：对公式进行截图并转换成Latex模板\r\nAxmath与Axglyph：国产的可视化公式编辑与画图软件，可插入office等\r\nTablesGenerator：在线的表格转latex工具\r\n网易有道词典：浮窗状态下可右键查询单词\r\nPaperwithCode：论文开源项目推荐与索引网站，包含各种任务的排行榜\r\nReadpaper：论文阅读、要点介绍与细节讨论\r\n坚果云：多设备文件与数据的云备份与更新\r\nNotion：关系型文档数据库（重要，我们Group一般用这个进行交互）\r\n\r\n一个硬核程序员向的notion任务规划导向的搭建\r\n\r\neverything：快速的电脑全局搜索工具\r\n\r\n有任何问题可以及时跟我沟通，我们的小组会一般是每周日下午2点30（需要尽量参加），薛老师大组会2周一次（一般是周二下午）\r\n\r\n","categories":["Dl"],"tags":["deep learning","schedual"]},{"title":"DeepLearning | Quick Start II","url":"/2023/11/24/DL/learn-2/","content":"Is it must be overfitting when the loss is not small? Infact there\r\nare two possible reasons for this situation. The one is model bias and\r\nthe other is the poor optimization.\r\n\r\nQuestions when training\r\nBig loss when training\r\n\r\nNotice that this situation appears in\r\ntraining step\r\n\r\nIf you get the big loss when training the network, one prossible\r\nreason is the model bias, which means you may try other bigger model\r\nwith more elasticity; the other reason is the optimizer works poor.\r\nHow two judge the two situation? If you get bigger loss\r\nnot only in test set, but\r\nalso in train set compared to a smaller model, the most\r\npossible reason is the weak optimizer.\r\n\r\n\r\nbig loss\r\n\r\nOverfitting\r\nSmall loss in training set but big in testing set.\r\nSolutions:\r\n\r\nExpend the training data\r\n\r\ncollect more training data\r\ndata augmentation (must be suitable for the training task)\r\n\r\nConstrain the model, decrease the flexibility\r\n\r\nless parameters (less neuron)\r\nshareing parameters\r\nless features\r\nearly stop\r\nregularization\r\ndropout\r\n\r\n\r\nModel selection\r\nCross validation: split your training set into validation set and\r\ntraining set, use validation set to select model\r\n\r\nk-folder method\r\n\r\nOptimization\r\nGradient nears to zero but loss still big, maybe the model is still\r\non the critical point (gradient equals to\r\nzero, not only the local minima but also the saddle point).\r\nSuppose the loss function is zero at point \\(\\theta^{\\prime}\\), use Taylor formula to\r\nestimate the value of loss around point \\(\\theta\\)\r\n\\[\r\nL(\\theta) \\approx L(\\theta^{\\prime}) + (\\theta - \\theta^{\\prime})^T\r\nL^{\\prime}(\\theta^{\\prime}) +  (\\theta - \\theta^{\\prime})^T\r\n\\mathbf{H}  (\\theta - \\theta^{\\prime})\r\n\\]\r\nwhere \\(\\mathbf{H}\\) is the\r\nHessian matrix \\(H_{ij}=\\frac{\\partial}{\\partial \\theta_i \\partial\r\n\\theta_j}L(\\theta)\\).\r\nConsiderring that the gradient is zero, namely \\(L^{\\prime}(\\theta^{\\prime}) = 0\\)\r\n\\[\r\nL(\\theta) \\approx L(\\theta^{\\prime}) + v^T \\mathbf{H} v\r\n\\]\r\nwhere \\(v = \\theta -\r\n\\theta^{\\prime}\\). For all \\(v\\):\r\n\r\nif \\(v^T \\mathbf{H} v &gt; 0\\),\r\nnamely \\(\\mathbf{H}\\) is positive\r\ndefinite (all eigen values are above zero): \\(L(\\theta) &gt; L(\\theta^{\\prime})\\), which\r\nmeans the point \\(\\theta^{\\prime}\\) is\r\na local minima\r\nif \\(v^T \\mathbf{H} v &lt; 0\\),\r\nnamely \\(\\mathbf{H}\\) is negtive\r\ndefinite (all eigen values are below zero): \\(L(\\theta) &lt; L(\\theta^{\\prime})\\), which\r\nmeans the point \\(\\theta^{\\prime}\\) is\r\na local maxima\r\nif sometimes \\(v^T \\mathbf{H} v &gt;\r\n0\\) while sometimes \\(v^T \\mathbf{H} v\r\n&lt; 0\\), the point is a saddle\r\npoint\r\n\r\nSo one way to judge the type of criticle\r\npoint is to calculate all the eigen value of Hessian and see the sign of\r\nthem\r\n\r\nescape saddle point: the loss can be updated (continue to decrease)\r\ntowards the eigen vector's direction: \\(L(\\theta) \\approx L(\\theta^{\\prime}) + v^T\r\n\\mathbf{H} v = L(\\theta^{\\prime}) + \\lambda ||\\mathbf{u}||\\)\r\nwhere \\(\\mathbf{u}\\) is the eigen\r\nvector for the positive eigen value\r\n\r\n\r\nlocal minima and saddle point:\r\n\r\nWhich one appears more frequently in the practice? There is a\r\nhypothesis. Suppose a local minima for a 1-dimmension independent\r\nvariable, such as the quadratic function, it has a local minima. But put\r\nit to the 2-dimmension space, the local minima may convert into a saddle\r\npoint. So the hypothesis is that if there is a local minima point, it\r\ncan be converted into a saddle point in an enough high space. The big\r\nmodel nowadays has millons of parameters in loss function, so there must\r\nbe lots of saddle point that gradient equaling to zero but can being\r\ndecreased. The saddle points are more common.\r\n\r\n\r\n\r\nbatch: divide all training set into small\r\nbatches, for every batch, calculate\r\nthe loss gradient, update parameters. The process that all batches had\r\nbeen seen is called an epoch.\r\nShuffle all batches after one epoch. Why use\r\nbatch? Decreasing the noise steps when updating the parameters and the\r\nparallel calculation.\r\n\r\n","categories":["DL"],"tags":["Deep learning","tutorial","lhy-ML"]},{"title":"DeepLearning | Quick Start I","url":"/2023/11/24/DL/learn-1/","content":"The first lecture that all ai-beginner should to take.\r\nvideo\r\n\r\nIntroduction\r\n\r\ndeep learning aims to finding a function.\r\n\r\n\r\nclassification\r\nregression\r\nstructured\r\n\r\nThe general step to do machine learning:\r\n\r\nwrite down the function model with unknown\r\nparameters\r\n\r\nneed domain knowledge\r\n\r\ndefine loss from training data,\r\nloss is a function of parameters in your model\r\n\r\nhow good a set of parameters are\r\ncalculate from training data\r\nvisuaization: error surface\r\n\r\noptimization your loss with gradient\r\ndescent\r\n\r\n\r\ngradient descent: \\(w^{\\star}, b^{\\star}=\\arg\\min_{w,b} Loss\\)\r\ncompute the derivative \\(\\frac{\\partial\r\nL}{\\partial w}|_{w=w^0}\\) where \\(w^0\\) is picked randomly. Use the\r\nlearning rate to stand for the step width: \\(w^1 \\leftarrow w^0 - \\eta \\frac{\\partial\r\nL}{\\partial w}|_{w=w^0}\\). Then, update \\(w\\) until reaches a threshold (such as the\r\nmax iteration count or minimum updation)\r\n\r\nThe above three steps is train.\r\nModel\r\nmodel bias: comes from unreasonable model.\r\nHow to construct a reasonable model or how to fit the model functions\r\nare the main problems in ML/DL. The most common idea is to use the\r\ncombination of function units to fit the model. Such as the\r\nsigmoid function.\r\n\r\npiecewise linear: \\(constant + \\sum c_i sigmoid(b_i + w_i x)\\).\r\nThis can simulate lots of functions (including piecewise linear\r\nconstructed with lots of small line segements and curve\r\nfunctions which can be divided into many piecewise linear)\r\n\r\n\\[\r\nc \\cdot Sigmoid(b+ w x) = c\\frac{1}{1 + e^{-(b + w x)}}\r\n\\]\r\n\r\n\\(w\\): change slopes\r\n\\(b\\): shift\r\n\\(c\\): change height\r\n\r\nFor multi-features (suppose \\(j\\)\r\nfestures): \\(y = const + \\sum_i c_i\r\nsigmoid(b_i + \\sum_j w_{ij} x) = const + c^T \\sigma(\\mathbf{b} + W\r\n\\mathbf{x})\\)\r\n\r\n\r\nsigmoid-matrix\r\n\r\nSigmoid function is also called Activation\r\nfunction.\r\nLoss and Optimization\r\nConcatenate \\(W, b, c, const\\) into\r\na vector (for matrix \\(W\\), concatenate\r\nits rows or colums into a vector) named \\(\\theta\\). Optimiza the loss function and\r\ncalculate the parameter \\(\\theta\\):\r\n\\[\r\n\\theta^{\\star} = \\arg\\min_{\\theta} L\r\n\\]\r\nNote \\(\\mathbf{g}\\) for gradient:\r\n\\(\\mathbf{g_i} = \\triangledown\r\nL(\\theta^i)\\), update the \\(\\theta\\) using \\(\\theta_{i+1} \\leftarrow \\theta_i - \\eta\r\n\\mathbf{g_i}, \\mathbf{g}_{i+1}=\\triangledown L(\\theta_{i+1})\\).\r\nStop updating when converging.\r\nTrain with large dataset: divide dataset into\r\nbatches, update \\(\\theta\\) on\r\neach batch; The process that all batches have been seen is named\r\nepoch.\r\nMore activation functions\r\nReLU: Rectified Linear Unit: \\(ReLU(b + w x) = \\max(0, b + w x)\\)\r\nSame as sigmoid, ReLU is also an\r\nactivation function\r\nMore layers\r\nnote \\(a = ReLU(\\mathbf{b}+W\r\n\\mathbf{x})\\), continue the same work: \\(a^{\\prime} = ReLU(\\mathbf{b^{\\prime} +\r\nW^{\\prime}\\mathbf{a}})\\) and so on... The whole thing we do just\r\nnow can be show as follow:\r\n\r\n\r\nmulti-layer\r\n\r\nYou can see the loss decreases more than just one layer.\r\nEach activation function dubbed Neuron, one\r\nlayer neurons overlays another one build the Neural\r\nNetwork or you can also say many hidden\r\nlayers build the Deep\r\nLearning.\r\n\r\noverfitting: sometimes, the deeper network brings\r\noverfitting problem.\r\n\r\n","categories":["DL"],"tags":["deep learning","tutorial","lhy-ML"]},{"title":"DeepLearning | Loss in Super-Resolution","url":"/2023/12/07/DL/loss/","content":"The loss functions used in super-resolution tasks have differences\r\nfrom other tasks like speech recognition. The mainly reason is that not\r\nonly should the researchers compare the distance of two or more image\r\nmatrices, but also the image structure and other indices.\r\n\r\nPeak signal-to-noise ratio\r\n(PSNR)\r\nreference\r\nDefinition: the maximum possible power of a signal\r\nand the power of currupting noise that affects the fidelity of its\r\nrepresentation.\r\n\r\ncommomly used to quantify reconstruction quality for images\r\nand videos subjuct to lossy compression\r\n\r\nFor monochrome image \r\nwith  size and its noise\r\napproximation ,  is defined by\r\n\r\nThe  with  form is\r\n\r\nWhere the  is the maximum\r\npossible pixel value of image , such as the image is represented with\r\n8 bits per pixel, .\r\n\r\nfor 8-bit images, the common  ranges from  to , the higher the better\r\napply this index under the same codec and the same\r\ncontent\r\npoorer performance compared with other quality metrics\r\n\r\nStructural similarity\r\nindex measure (SSIM)\r\nreference\r\nDefinition:  is used for measuring the similarity\r\nbetween two images. This metric is a full reference metric,\r\nwhich means this measurement or prediction of image quality is based on\r\nan initial uncompressed or distortion-free image as a\r\nreference.\r\n\r\nThe difference with other techniques such as MSE or PSNR is that\r\nthese approaches estimate absolute errors. Structural information is the\r\nidea that the pixels have strong inter-dependencies especially when they\r\nare spatially close. These dependencies carry important information\r\nabout the structure of the objects in the visual scene.\r\n\r\nFor luminance , contrast  and structure , the individual formulas between\r\nsamples  are \r\nwhere , and the  is\r\n\r\nSetting the  to , the  can be calculated by\r\n\r\n\r\nthe constants  is used to\r\nstablize the division with weak denominator.\r\n\r\n, , where  is the dynamic range of pixel values\r\n(such as  with 8-bit\r\nimage)\r\n\r\nnot satisfies triangle inequality or non-negativity, thus is not a\r\ndistance function\r\n\r\nunder certain conditions, \r\ncan be converted into a normalized root  measure which is a distance\r\nfunction.\r\n\r\nthe square of it is not convex but a\r\nlocally convex and quasiconvex\r\n\r\nConnectionist\r\ntemporal classification (CTC)\r\nreference-1,\r\nreference-2\r\nThe task of CTC is\r\n\r\ngiven two sequences one is the input  and\r\nthe other is the output , our aim is to find a mapping from the set of\r\n to the set of \r\n\r\nthe length of  and\r\n can vary; no\r\ncorrspondence of the elements between two sequences; the input and the\r\noutput may be disalignment\r\n\r\n\r\nCTC handles the task. The output of CTC is the probability\r\ndistribution of all possible  's and we can use the\r\ndistribution to infer the most likely sequence.\r\nLoss function: for a given input, we want to train\r\nour model to maximize the probability it assigns to right answers.\r\n\r\nCTC is an alignment-free method. The total loss is conducted on a\r\nunion of alphbet  and a blank symbol , namely the input and the output\r\nis the subset of . This map function is conducted by the\r\nfollowing 3 steps:\r\n\r\n\r\nalignment\r\n\r\nUsually, the CTC loss divides the input sequence into small strides\r\nbased on time-step (if for an image, the time step is a small\r\nwidth) and outputs a probability distribution for every\r\ntime-step. The whole output of the input sequence/image forms a\r\nprobability matrix with the shape , where  is the size of\r\nthe union set of alphabet and blank  and  is the sequence length.\r\n\r\n\r\nprobability of sequence\r\n\r\nThe math form is \r\n\r\n: the CTC conditional\r\nprobability\r\n:\r\nmarginalizes over the set of valid alignments\r\n: computing the probability for a single alignment\r\nstep-by-step\r\n\r\nDynamic programming solution\r\n\r\nIf we aren’t careful, the CTC loss can be very expensive to compute.\r\nWe could try the straightforward approach and compute the score for each\r\nalignment summing them all up as we go. The problem is there can be a\r\nmassive number of alignments. For most problems this would be too\r\nslow.\r\n\r\nBut we can conduct DP algorithm to solve this problem. * The key\r\ninsight is that if two alignments have reached the same output at the\r\nsame step, then we can merge them.*\r\n\r\n\r\ndp algorithm\r\n\r\nThere are no need to calculate all paths' probabilities, which is\r\ncomputation consuming. Considerring the order of input and output, we\r\nintroduce a new output sequence noted , namely\r\ninserting the blank \r\nbetween every two neighboring characters including the head and the\r\ntail.\r\nThere are two cases in the whole paths map between neighboring time\r\nstep.\r\n\r\ncase 1 is that for  step character, the preceding nodes\r\ninclude three possible statuses: the , the same character and\r\nthe character different from it.\r\ncase 2 is that for  step character, the current node is\r\nin recurrent, so the preceding nodes include two possible\r\nstatuses: the  and the\r\nsame character.\r\n\r\n\r\n\r\npath map\r\n\r\nFor the formula form, we define the symbol  as the CTC loss of the\r\nsubsequence  after\r\n input time steps.\r\n\r\ncase 1 formula: , where the factor means\r\nthe CTC probability of the two valid subsequences after  input steps and the later\r\nprobability means the probability of the current character at input\r\nstep \r\ncase 2 formula: \r\n\r\nNote that there are two valid start states and two valid\r\nend states, so the compelete probability is\r\nthe sum of the final two nodes.\r\n","categories":["DL"],"tags":["Deep learning","loss function","super-resolution"]},{"title":"DeepLearning | Diffusion Model","url":"/2023/12/08/DL/diffusion/","content":"\"The structure is already complete with the marble block, before I\r\nstart my work. It is already there, I just have to chisel away the\r\nsuperfuous material.\" - Michelangelo\r\n\r\n\r\n\"雕塑已经在大理石里了, 我的工作只是把不要的部分拿掉.\" --\r\n米开朗琪罗\r\n\r\nreference,\r\nDDPM paper, DDPM code, DDPM inference\r\nDiffusion Model\r\n\r\nMost of the diffusion models nowdays are similar to\r\nDenoise Diffusion Probability Model (DDPM), so\r\nthe follow contents will take this base model as an example.\r\n\r\ndiffusion Model Workflow\r\n\r\nreverse process:\r\n\r\ngive an image sampled from Gaussian noise that have the same shape\r\nwith the final expected output\r\ninput this image into a Denoise module,\r\nand the output is a little denoised image\r\nloop the step 2 until reaching the pointed times\r\nyou get the output image\r\n\r\n\r\nthe Denoise modules are marked from  (pointed) to , and the noise is inputed from  to \r\nnot only is the noise image inputed into denoise module, but also\r\nthe texts and the noise degree are\r\ninputed\r\ndenoise module includes a Noise Predictor and a\r\nMinus operator\r\n\r\n\r\n\r\n\r\nreverse process in one\r\nDenoise\r\n\r\n\r\nafter add texts supervise signal, the module is\r\n\r\n\r\n\r\ntexts supervised\r\n\r\n\r\nforward process (also known as difussion\r\nprocess):\r\n\r\ngive an real image\r\nadd noises to the image\r\nloop the step 2\r\nget the niosed image\r\n\r\n\r\nthe noise added in step number is the groud truth\r\nof the input image, the noise degree and the texts\r\nthe training set needs to be a image-text pair\r\n\r\n\r\n\r\n\r\nforward process\r\n\r\n\r\nthe training step is\r\n\r\n\r\n\r\ntraining step\r\n\r\nStabel Diffusion\r\nThe framework includes 3 parts\r\n\r\ntext encoder: inputs the texts and outputs the\r\nvectors that descrip the texts\r\ngeneration model: mostly frequent use is the\r\ndiffusion model, the input is noises and encoded texts, the outputs is\r\nan intermediate products image (compressed image that human can\r\nunderstand or not)\r\ndecoder: the input is the intermediate products and\r\nthe output is the image expected\r\n\r\nThe three modules are trained respectively\r\n\r\n\r\nstable diffusion\r\n\r\nText Encoder\r\n\r\nFréchet Inception Distance (FID): the metric to\r\njudge the performance of one image.\r\n\r\nuse a pre-trained CNN model\r\ninput the real image or generated image respectively\r\ntake out the representation (outputs of the last\r\nlayer)\r\nsuppose the representation vectors submits to Gaussion distribution\r\nrespectively, use Fréchet distance between the two distributions,\r\nsmaller is better\r\na lots of samples are needed\r\n\r\n\r\n\r\nClip: Contrastive Language-Image Pre-Training. Give\r\nthe text-image paris, texts are inputed to a text encoder while the\r\nimages are inputed to a image encoder\r\n\r\nuse this large model to judge if the generated image matches the\r\ntexts\r\ncalculate the distance between encoded text vector and encoded image\r\nvector\r\n\r\n\r\nDecoder\r\nDecoder can be trained without text labels.\r\n\r\nfirst method: use only images, small image and large image pairs,\r\nthe smaller image is the input and larger image is the label\r\nsecond method: use anto-encoder, usually the output\r\nof the generation model is a latent representation. Input\r\nground truth image to auto-encoder to get the latent representation, and\r\ninput latent representation into the decoder\r\n\r\nGeneration Model\r\nUsually use the diffusion model as the generation model.\r\n\r\nforward process: input the ground truth image to\r\nthe encoder and get the latent representation. Add noises to the latent\r\nrepresentation. Loop for a pointed times. Use (noised latent\r\nrepresentation, noise degree, texts) as the noise predictor's input and\r\nthe added noises as the ground truth.\r\nreverse process: input the noises sampled from\r\nGaussian distribution and the encoded texts to denoise module, output\r\nthe latent representation.\r\n\r\nPrinciple of diffusion Model\r\n\r\nVAE model: input the ground truth image and\r\nuse a encoder to get the latent representation and input the\r\nlatent representation to decoder to get the image.\r\n\r\n\r\n\r\ndifferences between VAE and diffusion\r\nmodel\r\n\r\nThe training process is given by the follow algorithm\r\n\r\n\r\ntraining algorithm\r\n\r\nThe  is the clean image\r\nsampled from dataset and  is\r\nsampled from a uniform distribution where  is large.  is sampled from the normal\r\ndsitribution.\r\nGradient calculation:  is\r\ndefined later, the value of \r\ndecreases from  to .  mixes the clean image and the noises.\r\n is a function\r\nwith two parameters one is the mixed image and the other is , which is called noise\r\npredictor. The ground truth is .\r\nThe inference process is given by the follow algorithm\r\n\r\n\r\ninference algorithm\r\n\r\nThe image illustrate the steps of inference algprithm\r\n\r\n\r\ninference algorithm image\r\nversion\r\n\r\n\r\n所有图片生成模型的 共同目标 是希望找到一个\r\nNetwork, 从 易于采样 的分布中抽取样本,\r\n并输出一个给定的分布形式. 通常这个 Network 会输入一段文字\r\n(Condition).\r\n训练的目标是使输出的分布和真实分布越接近越好.\r\n\r\nDefinition and mathematic symbols:\r\n\r\nNetwork: the parameters in network noted as \r\nGenerated distribution: noted as \r\nReal distribution: noted as \r\n\r\n\r\nMaximum Likelihood Estimation\r\n\r\nSample \r\nAssume we can calculate : given , we can calculate the probability\r\ngenerated by \r\nThe target is \r\n\r\n\r\n\r\nWhere the item  is not related to . We can infer that\r\nmaximize likelihood is equivalent to minimize KL\r\ndivergence. (KL-divergence always greater or equal to\r\n0)\r\n\r\nGAN model minimize some divergence\r\nVAE, diffusion model maximize likelihood\r\n\r\n\r\nVAE example:\r\nThe generated distribution can be written as , where  is the\r\ninput known distribution such as Gaussian distrbution. VAE assmu that\r\n where  is the\r\nmean of  and\r\nis also the output of network\r\nVAE maximize the lower bound of  to get as greater\r\n as possible. Take\r\n as  for short\r\n\r\nwhere the item , and the distribution  can be any distribution. In VAE, the distribution\r\n is the\r\nencoder.\r\nThe goal of VAE is to maximize the lower bound: \r\n\r\nFor DDPM, there are many denoise steps. One denoise is\r\none step, after  steps,\r\nDDPM produces the expected image. Every step of denoise produces one\r\nintermidiate product. So DDPM maximize the lower bound: , where  is the diffusion\r\nprocess not the encoder in VAE.\r\n\r\nHow to calculate the distribution . Define the\r\nhyper-parameters .\r\n\r\nThe noise sampling processes are independent to each other. If we\r\nwant to calculate ,\r\nthere is a simpler way using iterative inference.\r\n\r\nWe combine the two steps:\r\n\r\nThe item  is also a\r\nGaussian distribution as \r\nare independent to each other. We denote them as , which means we can only sample one time, from\r\n to . More generally, if we want to\r\ngenerate , we need only sample\r\none time from .x\r\n\r\n\r\ninfer-1\r\n\r\n\r\n\r\ninfer-2\r\n\r\nThe middle item  has no relation to the network parameter . We mainly focus on the third\r\nitem.\r\n\r\ncalculate :\r\ngiven  and , calculate the 's distribution. Using Bayes\r\nformula: \r\n\r\n\r\n\r\ninfer-3\r\n\r\nWhatever, the result is still a Gaussian distribution.\r\nHow to minimize the expectation of KL-Divergence (the third\r\npart): We want to minimize the expectation of\r\nKL-Divergence, so as long as most of the\r\nKL-Divergences are small, the expectation will be probably\r\nsmall. The small divergence means two distributions' distance\r\nis small, the two distributions are close to each other. The\r\nfirst distribution,  is a fixed Gaussian distribution\r\nwith known mean and variance which the second distribution  is our\r\ndenoise module that will be trained.\r\nSo we want to let the second distribution's mean close the fixed\r\nmean (if ignore the variance).\r\nThe deonise model inputs  and\r\nthe value , outputs the\r\nmean of distribution, expecting that the mean is close to the\r\nmean of .\r\nInfact, the parameters \r\nare constants, the only part denoise should predict is the noise  or .\r\n","categories":["DL"],"tags":["Deep learing","Diffusion model"]},{"title":"Papers | PARSeq","url":"/2023/12/11/Papers/parseq/","content":"STR\r\n的方法里面经常会使用自回归的语言模型（ARLM），当然考虑到自回归的一些缺点和局限性，也有人采用外置的语言模型。但是外置的语言模型（external\r\nLM）是条件独立的（conditional\r\nindependence），有时会将正确的预测结果修正为错误的结果。本文提出的PARSeq（Permuted\r\nAutoregressive Sequence）模型使用了Permutation Language Modeling和Weight\r\nSharing来集成自回归语言模型。\r\n\r\n代码，论文\r\nIntroduction\r\n本文先介绍了什么是STR任务并说明该任务成果的应用场景和当前问题，并提出\r\nSTR任务在某些情况下不能仅仅依靠image\r\nfeatures来做推理，而是要依赖一定的language\r\nsemantics。STR网络中内置的语言模型一般是自回归的，不仅能够处理语言特征还能联合地处理图像特征。通常的自回归策略是根据过去的时刻的预测值来预测当前时刻的值：\r\n\\(P(\\mathbf{y} | \\mathbf{x})=\\prod_{t=1}^{T}\r\nP(y_t | \\mathbf{y}_{&lt;t}, \\mathbf{x})\\) （其中 \\(\\mathbf{y}\\) 是一个长度为\\(T\\)的文本序列， \\(\\mathbf{x}\\)\r\n是图像）。这样的自回归策略有两个缺点：\r\n\r\n单向性：文本序列的预测要么从左到右，要么从右到左。预测结果和顺序有一定的关系。\r\n推理的时候，输出的序列顺序和训练是一样的。\r\n\r\n为了解决“单向”的问题，有人提出了 ABINet\r\n的模型，它主要使用掩码的方式，在注意力层多加了一层掩码：\\(P(\\mathbf{y} | \\mathbf{x}) = \\prod_{t=1}^{T}P(y_t\r\n| \\mathbf{x})\\)，其中 \\(P(\\mathbf{y}) =\r\n\\prod_{t=1}^{T}P(y_t | \\mathbf{y}_{t\\neq\r\n1})\\)，即在预测某个字符时，将其做掩码，用其余字符对掩码字符进行预测，这样的好处是能够\r\n利用双向 的字符特征。但是在 ABINet 中，作者采用的是\r\nindependent LM，可以单独训练，不依赖于 image model，用于 纠正\r\nimage model 的错误。但是当语言模型本身的预测结果就是错误的，又何谈\r\n纠错 呢？于是为了利用image和text两种特征，ABINet 的作者又加了\r\nfusion 模块，但本文作者认为这种方式带来了巨大的参数数量。\r\n看了序列模型的综述，本文发现目前比较常用的、研究较多的一类模型是\r\n序列生成模型，作者认为可以将这些模型结合context-free和context-aware后泛化在STR任务中。并且能将\r\n外部 的语言模型转化为 内部 的。本文着重关注的是\r\nPermutation Language Model\r\n(PLM)，后续本文会说明是如何改进PLM，让其更加适用于STR。本文提到：PLM模型可以看作是自回归的一种泛化，能够使用\r\n共享的结构和权值集成\r\n多个自回归模型。如下图所示\r\n\r\n\r\npermiutation\r\n\r\n本文类似这种方式，提出了用于STR任务的PARSeq（Permuted Autoregressive\r\nSequence）模型，并且表现还不错。\r\nRelated Work\r\n本文在这一节主要从三个方面来总结STR的工作，分别是context-free STR,\r\ncontext-aware STR 和 generation from sequence model。\r\n\r\ncontext-free\r\nSTR：直接从图片中预测字符串，每一个字符是概率独立的。缺点是low\r\nrobust，于是大家开始利用语言模型来指导图片文本的识别\r\ncontext-ware\r\nSTR：在图片的文字识别中加入语言模型作为指导，但是会出现两种方式：一种是所谓的external\r\nLM，也就是直接使用现有的语言模型，主要用于通过集成、迭代等方式提升预测的准去率；一种是internal\r\nLM，主要是跟着自己的模型一起训练。不过对于后者，传统的做法是使用monotonic\r\nand standard AR训练方式，PARSeq使用的是PLM方式。\r\ngeneration from sequence model：\r\n\r\nautoregressive (one token at a time)\r\nnon-regressive (all token predicted at once)\r\n\r\n\r\nPermuted Autoregressive\r\nSequence Models\r\n在这一节中，会像大部分论文一样采用三段论：先介绍整体的模型架构，再介绍训练的方式，最后讨论如何运用训练好的模型做推理。\r\n","categories":["Paper"],"tags":["Paper","STR"]},{"title":"Paper | TATT","url":"/2023/12/02/Papers/tatt/","content":"Paper name: A Text Attention Network for Spatial Deformation Robust\r\nScene Text Image Super-resolution\r\n\r\nThe mainly work contributed by this work is providing a noval method\r\nto do super-resolution(SR) on low-resolution(LR) scene-text image.\r\nModel Architecture\r\nThe proposed TATT architecture for STISR is shown as follow.\r\n\r\n\r\nTATT arch\r\n\r\nTPGB is short for text prior guided\r\nblocks, TPG is short for text prior\r\ngenerator and SRB is short for\r\nsequential-recurrent block.\r\n\r\nInput: low-resolution(LR) text images \r\nText prior generotor (TPG): usually CRNN model, input the\r\nLR images and output text prior, the recognition probability\r\nsequence.\r\nConvolution (CONV): inputs low resolution images and\r\noutputs the images features\r\nTP Interpreter (TPI): two inputs, one is the text prior and\r\nthe other is the image features, the outputs is the TP\r\nMap, which can interpret the text prior into image features.\r\nAttention-based block.\r\nText prior genertor block (TPGB): two inputs, one is the image\r\nfeatures deliveryed by CNN, and the other is the text prior\r\nmap deliveryed by TPI. Includes 5 SRB blocks.\r\n\r\nSequential residual block (SRB): proposed in TSRN\r\nmodel.\r\n\r\nPixel shuffle: a method to up-sample the pixel, usually used in\r\nsuper-resolution (SR) tasks. Extend channel dimensions.\r\n\r\nLoss\r\nThe total loss is image loss adding recognizer distill\r\nloss\r\nModel in codes\r\nThe most important function in this codes provided by the authors is\r\ntrain defined in super_resolution.py, which\r\noccupies 822 lines... with a chaotic logic but with no comment. However,\r\nwe should read the codes to study the model deeply.\r\nData type initialization\r\nThere is a class named AsterInfo defined in\r\nbase.py which is used not only for ASTER\r\nmodel, but also for any other CRNN-like models. This class defines\r\nvocabulary and some basic text-sequence parameters.\r\nData stream while training\r\n\r\nThe most important key to understand the whole model in codes is to\r\ngrasp the data stream in calculations.\r\n\r\nWe can look up how the TATT model is invoked firstly:\r\ncascade_images, ret_mid = model_list[pick](\n    images_lr if not self.args.for_cascading else cascade_images, \n    label_vecs_final.detach()  ## text embedding\n)\r\nWhere the images_lr is the LR images\r\nloaded by\r\nimages_lr = self.torch_rotate_img(images_lr, arc, rand_offs),\r\nfor an inversion mode, namely the variable images_lr_ret is\r\ngenerated by\r\nimages_lr_ret = self.torch_rotate_img(images_lr.clone(), -arc, rand_offs).\r\nWe continue to look up the parameter label_vecs_final\r\nand we find that this is calculated by stu_model, namely,\r\nCRNN model:\r\nstu_model = aster_student[tpg_pick]  ## namely, CRNN\naster_dict_lr = self.parse_crnn_data(cascade_images[:, :3, :, :] if not self.args.y_domain else images_lrraw[:, :3, :, :])\nlabel_vecs_logits = stu_model(aster_dict_lr)\nlabel_vecs = torch.nn.functional.softmax(label_vecs_logits, -1)\nlabel_vecs_final = label_vecs.permute(1, 0, 2).unsqueeze(1).permute(0, 3, 1, 2)\r\nWhere the function parse_crnn_data is to interpolate the\r\nLR image (namely the cascade_images) into shape\r\n(32, 100) with bicubic interpolation, but the\r\noutput of this function is a weighted addition image\r\n0.299 * R + 0.587 * G + 0.114 * B (grey-scale image). So\r\nthis part outputs the text priors.\r\nContinue to follow the data cascade_images, which is the\r\noutput of TATT. Surely we find the loss next:\r\nim_quality_loss = image_crit(cascade_images, images_hr),\r\nthis calculates the image quality loss. The image quality loss\r\nincludes two parts: MSE and GradientPriorLoss(GPL).\r\nAfter a series of tricks (calculate the loss_img_each), the\r\nloss become a part of total loss.\r\nSo what's the total loss? There it is:\r\nloss_im = loss_img + loss_recog_distill.\r\n\r\nthe first part loss_img: includes image quality\r\nloss (MSE + GPL) and TSSIM loss, the\r\nsecond item will be introduced later.\r\nthe second part loss_recog_distill: comes from\r\nsemantic loss between student model and teacher model.\r\n\r\nAfter that, our optimizer comes on!\r\noptimizer_G.zero_grad(), which is also the necessary\r\nsentence in training. The next is backward propagation\r\nloss_im.backward(). The model uses\r\nclip_grad_norm_ to prevent gradient explosion. The finally\r\nis taking a step optimizer_G.step().\r\nThe above data-stream is for one batch when training.\r\nTPG block in code\r\nThe text prior generator block uses CRNN (can be\r\ncustomized in command argues, the default is CRNN) to extract the text\r\npriors. And the TPG provides two model options, one is the default CRNN,\r\nand the other is \"TPG\"(infact, the CRNN). And we take CRNN as an\r\nexample,  namely the function CRNN_init defined in\r\nbase.py, which uses the CRNN class defined in\r\ncrnn.py not Model class defined in\r\ncrnn/model.py.\r\nTSRN_TL_TRANS\r\nThe other parts of TATT aggregate to one TSRN-like model defined in\r\ntsrn.py. The initialization in train using\r\ngenertor_init() function and the author put it into\r\nmodel_list.\r\n# model includes 1conv block, 5 srb blocks, 1 conv block, 1 upsample block\n# defines TPS based STN block\nmodel = tsrn.TSRN_TL_TRANS(\n    scale_factor=self.scale_factor, \n    width=cfg.width, \n    height=cfg.height,\n    STN=self.args.STN, \n    mask=self.mask, \n    srb_nums=self.args.srb,\n    hidden_units=self.args.hd_u\n)\r\nFirstly we analysis the forward function.\r\n\r\nTPS-based STN: the input images use TPS-based\r\nSTN to recover the text images to normal shape.\r\nconvolution with PReLU:\r\nblock = {'1': self.block1(x)} and\r\npadding_feature = block['1']\r\nTPInterpreter: in code, it is infoGen:\r\nself.infoGen = TPInterpreter(text_emb, out_text_channels, output_size=(height//scale_factor, width//scale_factor))  # InfoGen(text_emb, out_text_channels)\r\nand invoked by\r\ntp_map, pr_weights = self.infoGen(padding_feature, text_emb).\r\nIts inputs are padding_feature and text_emb\r\nnamely image feature and text prior respectively.\r\n5 SRBs: input image feature coming from convolution\r\nand TPMap coming from TPI, the final output is also image feature.\r\nconvolution with batch norm\r\npixel shuffle: namely, block 8.\r\ntanh: input the output of block-8.\r\n\r\n*TPS-based STN\r\nTPInterpreter\r\nTPI module aims to interpret the text prior to the image\r\nfeature so that the influence of semantics\r\nguidance can be exerted to the correlated spatial position in\r\nthe image feature domain. TPI module is transformer-based, the\r\narchitecture is shown as follow:\r\n\r\n\r\nTPI arch\r\n\r\nThere are two inputs in TPI, one is the text prior (with\r\nshape ) (in\r\nfact the shape is reshaped into  later in forward function) and the other is image feature.\r\nBased on the analysis before, we claim that\r\n\r\ntext prior comes from TPG module, which represents in code\r\nwith the name label_vecs_final\r\nimage feature comes from CONV module, which represent in\r\ncode with the name block1\r\n\r\nThere are only one output in TPI, which is the text prior\r\nmap, TP map for short.\r\nThere are another modules that are similar two a normal transformer.\r\nFirstly, the whole architeture of TPI is divided into\r\nencoder and decoder. MSA is\r\nshort for multi-head self attention, LN is short for\r\nlayer norm, FFN is short for feed-forward\r\nnetwork and MCA is short for multi-head cross\r\nattention. The introduction of attention will be analysised\r\nlater (not in this article).\r\nBut we concern the position encoding\r\nfirstly, FPE (Fixed Positional Encoding) for text prior and\r\nRPE (Recurrent Positional Encoding) for image feature.\r\nThe TPInterpreter initialized with parameters:\r\n## definition of TPI\nclass TPInterpreter(nn.Module):\n    def __init__(\n        self,\n        t_emb,\n        out_text_channels,\n        output_size=(16, 64),\n        feature_in=64,\n        # d_model=512,\n        t_encoder_num=1,\n        t_decoder_num=2,\n    ): pass\r\n\r\ntext_emb = 37\r\nout_text_channels = 64\r\noutput_size = (height//scale_factor, width//scale_factor) = (16, 64)\r\n\r\nFor invoking, the forward function takes in two\r\nparameters image_feature and tp_input (text\r\nprior) and the outputs of TPI are text_prior, pr_weights\r\nwhere the first is the TP map we need:\r\n## definition of forward in TPI\ndef forward(self, image_feature, tp_input):\n  pass\n  return text_prior, pr_weights\n\n## invoke in TATT model's forwrad function\ntp_map, pr_weights = self.infoGen(padding_feature, text_emb)\n\n## invoke of TATT model\ncascade_images, ret_mid = model_list[pick](\n    images_lr if not self.args.for_cascading else cascade_images, \n    label_vecs_final.detach()  ## text embedding\n)\r\nThe image_lr is passed into infoGen after a\r\nconvolution operation, and the label_vecs_final is passed\r\ninto infoGen directly and returns the\r\ntp_map, pr_weights to TATT model. TATT model uses\r\ntp_map later in SRB blocks to guide SR.\r\n\r\nNow analysis the details in forward and\r\n__init__:\r\n\r\nforward: we catch the data stream firstly in this\r\nfunction, and find the definitions in initialization function.\r\n\r\nThe image_feature is reshaped into  firstly\r\n(flatten operation, where 16:width, 64:height, N: batch\r\nsize, 64:channels). And it's passed into InfoTransformer\r\nlater directly.\r\nThe tp_input is reshaped into  firstly, and goes to the\r\nPReLU activation and a fully connected layer\r\nwhich are used for projection.\r\nDefines PositionalEncoding\r\nPasses all those variables into InfoTransformer\r\nreturns text_prior (text prior map namely)\r\n\r\n\r\n\r\n&gt;&gt; InfoTransformer\r\nDefined in transformer_v2.py, which mainly realizes\r\nthe TPI module. Its forward function is\r\ndef forward(self, src, mask, query_embed, pos_embed, tgt=None, text_prior=None, spatial_size=(16, 64)): pass.\r\n\r\nsrc: reshaped text prior, namely the\r\ntp_input with shape \r\nmask: with shape \r\nquery_emb: self.init_factor.weight namely,\r\nwhich is nn.Embedding(1024, 64).weight\r\npos_emb: x_pos passed in TPI, the result\r\nof PositionalEncoding\r\ntgt: x_im namely the\r\nimage_feature with shape \r\n\r\nAfter entering the forward, query_emb is\r\nfirstly passed into GRU to encode.\r\n\r\nwhy call query_emb: because in decoder, this variable\r\nis passed into a multi-head cross attention module and acts as the\r\nquery.\r\n\r\nDifferent from normal transformer, this transformer has only 1\r\nlayer of encoders and 2 layers of decoders.\r\n&gt;&gt;&gt; InfoTransformer.encoder\r\nself.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm),\r\nuse TransformerEncoderLayer and LayerNorm.\r\n\r\nTransformerEncoderLayer: adds positional\r\nencoding and the original tensor, use\r\nforward_post function, same with the architecture image.\r\nThe FPE realized with\r\nPositionalEncoding. Positional Encoding\r\nref.\r\n\r\n&gt;&gt;&gt; InfoTransformer.decoder\r\nself.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,return_intermediate=return_intermediate_dec),\r\nuse TransformerDecoderLayer_TP and LayerNorm.\r\nThe invoking:\r\nhs = self.decoder(tgt, memory, memory_key_padding_mask=mask, pos=pos_embed, query_pos=query_embed)\r\n\r\nTransformerDecoderLayer_TP: the input\r\nquery_embed has been encoded in GRU,\r\nmemory is the output of encoder,\r\npos_embed is the text prior positional encoding,\r\ntgt is the image feature.\r\n\r\nSRB\r\nSRB block is firstly proposed in Scene Text Image\r\nSuper-Resolution in the Wild, where the SR task on scene text\r\nimage is firstly proposed. The architecture of SRB is shown as\r\nfollow:\r\n\r\n\r\nSRB\r\n\r\nHowever, the BiLSTM block is replaced by GRU block\r\nin this work.\r\nPixel Shuffle\r\nUp-sample in channel dimmension.\r\nSSIM Loss\r\nSSIM (Structure Similarity Index) loss is used to compare the\r\nsimilarity between two images, which is sensitive to local structure\r\nvariance. The loss can be divided into 3 parts: illuminance, contrast\r\nratio and structure, the formulas are:\r\n\r\nThe combination of the 3 parts is our SSIM loss.\r\n\r\n","categories":["DL"],"tags":["deep learning","TATT","scene text"]}]