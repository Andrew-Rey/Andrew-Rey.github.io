{"meta":{"title":"Andrew-Rey","subtitle":"醉后不知天在水，满船清梦压星河","description":"","author":null,"url":"https://Andrew-Rey.github.io","root":"/"},"pages":[{"title":"","date":"2023-11-15T03:21:23.859Z","updated":"2023-11-15T03:21:23.859Z","comments":true,"path":"css/prism.css","permalink":"https://andrew-rey.github.io/css/prism.css","excerpt":"","text":"/** * prism.js default theme for JavaScript, CSS and HTML * Based on dabblet (http://dabblet.com) * @author Lea Verou */ code[class*=\"language-\"], pre[class*=\"language-\"] { color: black; background: none; text-shadow: 0 1px white; font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace; font-size: 1em; text-align: left; white-space: pre; word-spacing: normal; word-break: normal; word-wrap: normal; line-height: 1.5; -moz-tab-size: 4; -o-tab-size: 4; tab-size: 4; -webkit-hyphens: none; -moz-hyphens: none; -ms-hyphens: none; hyphens: none; } pre[class*=\"language-\"]::-moz-selection, pre[class*=\"language-\"] ::-moz-selection, code[class*=\"language-\"]::-moz-selection, code[class*=\"language-\"] ::-moz-selection { text-shadow: none; background: #b3d4fc; } pre[class*=\"language-\"]::selection, pre[class*=\"language-\"] ::selection, code[class*=\"language-\"]::selection, code[class*=\"language-\"] ::selection { text-shadow: none; background: #b3d4fc; } @media print { code[class*=\"language-\"], pre[class*=\"language-\"] { text-shadow: none; } } /* Code blocks */ pre[class*=\"language-\"] { padding: 1em; margin: .5em 0; overflow: auto; } :not(pre) > code[class*=\"language-\"], pre[class*=\"language-\"] { background: #f5f2f0; } /* Inline code */ :not(pre) > code[class*=\"language-\"] { padding: .1em; border-radius: .3em; white-space: normal; } .token.comment, .token.prolog, .token.doctype, .token.cdata { color: slategray; } .token.punctuation { color: #999; } .token.namespace { opacity: .7; } .token.property, .token.tag, .token.boolean, .token.number, .token.constant, .token.symbol, .token.deleted { color: #905; } .token.selector, .token.attr-name, .token.string, .token.char, .token.builtin, .token.inserted { color: #690; } .token.operator, .token.entity, .token.url, .language-css .token.string, .style .token.string { color: #9a6e3a; /* This background color was intended by the author of this theme. */ background: hsla(0, 0%, 100%, .5); } .token.atrule, .token.attr-value, .token.keyword { color: #07a; } .token.function, .token.class-name { color: #DD4A68; } .token.regex, .token.important, .token.variable { color: #e90; } .token.important, .token.bold { font-weight: bold; } .token.italic { font-style: italic; } .token.entity { cursor: help; }"},{"title":"","date":"2023-11-15T03:32:59.319Z","updated":"2023-11-15T03:32:59.319Z","comments":true,"path":"css/site.css","permalink":"https://andrew-rey.github.io/css/site.css","excerpt":"","text":"p code, li code, h1 code, h2 code, h3 code { display: inline-block; white-space: no-wrap; background: #f6f6f6; font-size: .9em; line-height: 1.5em; color: #464646; border: 1px solid #eeeeee; -webkit-border-radius: 0.4em; -moz-border-radius: 0.4em; -ms-border-radius: 0.4em; -o-border-radius: 0.4em; border-radius: 0.4em; padding: 0 .3em; margin: -1px 0; } .article-content a { color: #21afd3; } .article-content a:hover { color: #ff6557; text-decoration: underline; } @font-face { font-family: 'CaskaydiaCove Nerd Font'; src: url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.eot'); src: url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.eot?#iefix') format('embedded-opentype'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.woff2') format('woff2'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.woff') format('woff'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.ttf') format('truetype'), url('/fonts/subset-CaskaydiaCoveNerdFontComplete-.svg#CaskaydiaCoveNerdFontComplete-') format('svg'); font-weight: normal; font-style: normal; font-display: swap; } code[class*=\"language-\"], pre[class*=\"language-\"] { border-radius: .6em; font-family: 'CaskaydiaCove Nerd Font' !important; } code { font-family: 'CaskaydiaCove Nerd Font' !important; } .article-content { font-family: 'CaskaydiaCove Nerd Font'; } .original { margin: 2em 0 0; padding: .5em 1em; border-left: 3px solid #fbbc97; background-color: #f9f9f9; font-size: 14px; list-style: none; } .footnote-item p { line-height: 0; } .article-content li { line-height: 2em; }"},{"title":"categories","date":"2023-10-08T07:45:01.957Z","updated":"2023-10-08T07:45:01.957Z","comments":true,"path":"categories/index.html","permalink":"https://andrew-rey.github.io/categories/index.html","excerpt":"","text":""},{"title":"二十岁的自传","date":"2023-10-31T17:44:21.025Z","updated":"2023-10-31T17:44:21.025Z","comments":true,"path":"about/index.html","permalink":"https://andrew-rey.github.io/about/index.html","excerpt":"","text":"我在十九岁最后两天的时候给自己写了点东西，当时在教室，周围是考研的学长学姐。 现在是二十岁的我，自传嘛，随便写写，但是并不代表对自己的亵渎。 写点什么呢。 弱冠年，本科在读。 二零年毕业于郴州市一中。 高考延期，现在仍然记得高考时的座位靠窗，那天很热。 疫情在高三开始， 高二的回忆是关于粉橙色的夕阳和理综数学， 高一开始当了纪律委员， 夏令营的天很蓝。 初三的风，初二的她，初一的混乱和美术。 县城小学四年，与父骑车游玩， 浑身是泥，不汗不归。 一二年级在积木中度过。 宅居校内，父母为师， 门前青草针叶衫，夜晚是母亲扇风的手。 再往前，记忆只在照片中凝固。 就这样，平常地活着，安然无恙地活着。 当时怎知宇宙之大，也从不担忧人生几何； 现在知道了宇宙的度量，明白了人生几何， 目睹了活着，和死亡，目睹了一个时代的结束。 人体维护的一切，只是将熵增的速率变缓。 但那又怎样。 我欣然接受。 我已经学会了走路，奔跑；说话，呐喊；回忆，思考；见面，告别。 并且我仍然会 奔跑着，呐喊着，思考着，以及告别着。 但无论怎样，请别忘了： 走路，说话，回忆和见面， 也是你的本能。"},{"title":"tags","date":"2023-10-08T07:45:01.958Z","updated":"2023-10-08T07:45:01.958Z","comments":true,"path":"tags/index.html","permalink":"https://andrew-rey.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Papers | PARSeq","slug":"Papers/parseq","date":"2023-12-11T05:58:48.000Z","updated":"2023-12-22T15:28:41.176Z","comments":true,"path":"2023/12/11/Papers/parseq/","link":"","permalink":"https://andrew-rey.github.io/2023/12/11/Papers/parseq/","excerpt":"STR 的方法里面经常会使用自回归的语言模型（ARLM），当然考虑到自回归的一些缺点和局限性，也有人采用外置的语言模型。但是外置的语言模型（external LM）是条件独立的（conditional independence），有时会将正确的预测结果修正为错误的结果。本文提出的PARSeq（Permuted Autoregressive Sequence）模型使用了Permutation Language Modeling和Weight Sharing来集成自回归语言模型。","text":"STR 的方法里面经常会使用自回归的语言模型（ARLM），当然考虑到自回归的一些缺点和局限性，也有人采用外置的语言模型。但是外置的语言模型（external LM）是条件独立的（conditional independence），有时会将正确的预测结果修正为错误的结果。本文提出的PARSeq（Permuted Autoregressive Sequence）模型使用了Permutation Language Modeling和Weight Sharing来集成自回归语言模型。 代码，论文 Introduction 本文先介绍了什么是STR任务并说明该任务成果的应用场景和当前问题，并提出 STR任务在某些情况下不能仅仅依靠image features来做推理，而是要依赖一定的language semantics。STR网络中内置的语言模型一般是自回归的，不仅能够处理语言特征还能联合地处理图像特征。通常的自回归策略是根据过去的时刻的预测值来预测当前时刻的值： （其中 是一个长度为的文本序列， 是图像）。这样的自回归策略有两个缺点： 单向性：文本序列的预测要么从左到右，要么从右到左。预测结果和顺序有一定的关系。 推理的时候，输出的序列顺序和训练是一样的。 为了解决“单向”的问题，有人提出了 ABINet 的模型，它主要使用掩码的方式，在注意力层多加了一层掩码：，其中 ，即在预测某个字符时，将其做掩码，用其余字符对掩码字符进行预测，这样的好处是能够 利用双向 的字符特征。但是在 ABINet 中，作者采用的是 independent LM，可以单独训练，不依赖于 image model，用于 纠正 image model 的错误。但是当语言模型本身的预测结果就是错误的，又何谈 纠错 呢？于是为了利用image和text两种特征，ABINet 的作者又加了 fusion 模块，但本文作者认为这种方式带来了巨大的参数数量。 看了序列模型的综述，本文发现目前比较常用的、研究较多的一类模型是 序列生成模型，作者认为可以将这些模型结合context-free和context-aware后泛化在STR任务中。并且能将 外部 的语言模型转化为 内部 的。本文着重关注的是 Permutation Language Model (PLM)，后续本文会说明是如何改进PLM，让其更加适用于STR。本文提到：PLM模型可以看作是自回归的一种泛化，能够使用 共享的结构和权值集成 多个自回归模型。如下图所示 permiutation 本文类似这种方式，提出了用于STR任务的PARSeq（Permuted Autoregressive Sequence）模型，并且表现还不错。 Related Work 本文在这一节主要从三个方面来总结STR的工作，分别是context-free STR, context-aware STR 和 generation from sequence model。 context-free STR：直接从图片中预测字符串，每一个字符是概率独立的。缺点是low robust，于是大家开始利用语言模型来指导图片文本的识别 context-ware STR：在图片的文字识别中加入语言模型作为指导，但是会出现两种方式：一种是所谓的external LM，也就是直接使用现有的语言模型，主要用于通过集成、迭代等方式提升预测的准去率；一种是internal LM，主要是跟着自己的模型一起训练。不过对于后者，传统的做法是使用monotonic and standard AR训练方式，PARSeq使用的是PLM方式。 generation from sequence model： autoregressive (one token at a time) non-regressive (all token predicted at once) Permuted Autoregressive Sequence Models 在这一节中，会像大部分论文一样采用三段论：先介绍整体的模型架构，再介绍训练的方式，最后讨论如何运用训练好的模型做推理。 模型架构 模型架构 整体的PARSeq模型依然是熟悉的encoder-decoder架构，这种架构在处理序列型任务中表现很好。在本文设计的PARSeq模型中，encoder有12层，但decoder只有一层。encoder主要是面向image设计的，采用的是ViT-encoder（如图中Vit Encoder）所示；而decoder部分有三个输入（position queries, context, image tokens）和一个可选的mask（attention mask）。decoder内部有两个多头注意力（MHA）和一个MLP，每一个部分都采用残差连接。PARSeq模型的最大特点在于其 mask的设计，即采用了 permutation的方式对输入context的序列进行 理解层面 的排列。从这里就可以看出一条context序列在训练时对应的训练数据不止一个。 ViT Encoder ViT Encoder 首先了解图中的embedded patches是怎么来的。与原始的ViT不同，该模型舍弃了[cls] token。对于输入的image ，对其进行所谓的 tokenize 操作：即在 的维度上进行切分，每一份称之为一个 patch，切分的尺度为 ，也即切分成了 份 patches，将每一个 patch进行 flatten 后，得到一维的且长度为 的向量，最后对每一个这样的向量做线性投影 ，其中 是一个超参，最后得到的长度为 的向量称之为一个 token，一共有 个 token。 得到embedded patches后，将其输入自注意力模块，最终通过12层的处理得到输出的 图片特征 。 Visio-lingual Decoder Visio-lingual Decoder 对前文整体架构的decoder部分做了一个补充，该部分只有1层。 Permutation Language Modeling 该部分是本文的核心部分，主要讨论的是多头注意力的mask设计。对于标准的自回归模型，使用的都是根据时间 按顺序 进行预测，从而最大化似然函数 。但是这种方式对于注意力机制可能不太合理，毕竟注意力是在全局上提取特征的，如果真的想用自回归，也是没问题的，只要在注意力计算中添加如下的mask即可（假设输入是长度为3的context，包括[B], [E]后长度为4）： 其中每一行表示要预测的字符，每一列表示输入的字符。即第一个要预测的字符（第一行）由开始符号[B]（第一列）来预测（其实就是用整张图片image来预测），而第二个要预测的字符（第二行）由第一个输入字符（第二列）来预测，以此类推。这种方式是标准的自回归mask，根据时间顺序来进行mask，防止在预测当前字符时使用“未来”的信息。 但是对于 PLM（permuted language model） 而言，本质思想是在每一个字符串的 所有排列 上做训练，也就是一个字符串对应的 个排列上训练，这种模型对应的对数似然为： 一个比较好的理解方式：自回归其实是这种方式的一种特例，是“严格”的。将所有排列作为横轴，则自回归mask进行训练采用的是“冲激函数”的形式，而permutation的mask是将其视作分布，将冲激进行了一次平滑处理，最后的输出则是这个分布的期望。 值得一提：本文 并没有 在数据层面上进行全排列，而是设计了多种mask来体现全排列。 排列的选择方式 本文在这里的设计十分巧妙。嘴上说着随机从所有排列中选取 个排列生成mask，但实际上是这样做的： 随机选取 个排列，其中一个为正排列，即 另一半 个排列是 上述排列的倒序 在附录中，作者说明这样选择的优势 permutation selection 如上表所示 在推理阶段，输入的必然是正排列，因此训练的mask中必须含有正排列对应的mask才比较合理 而对于 一对 正反排列，每一个字符都由其它另外的字符作为条件 进行估计，二者互补 Permutation Mask的计算顺序 虽然名为“排列”，但是其计算顺序并未改变，因为该模型采用的是掩码方式，而不是扩增数据集的方式。 permutation mask的方式如下所示 permutation mask example 结合前文的条件概率，以 为例， ，即计算顺序依然是从左到右，在预测 时由于没有其它任何字符被预测出来，因此只能根据图片 进行预测；而对于 此时 已经被预测出来，可以作为条件去预测 ，因此mask在 处是 ；而最后预测的 使用了 作为条件，对应的掩码位置也是 。具体过程如下图所示。 calculation sequence 通过permutation mask的方式，对不同的permutation都对 时的字符进行了mask，防止“看到”后续的信息，但不同的排列结合后又 有可能 得到完整信息。 Overall Formula 模型输出 图片特征为 ，作为第二个 MHA 的 query 是位置编码，作为第一个 MHA 的 query， 是 embedded 后并进行位置编码的文本向量， 是掩码。 所有的 MHA 采用的都是 缩放点积。 Results and Analysis 数据预处理 Label preprocessing：最大的标签长度是 25（不包括开始和结束token），字符集包括大小写和不同发音标记，一共 94 个 Image preprocessing： Augmentation：consist primarily of RandAugment operations excluding Sharpness and Invert. Use GaussianBlur and PoissonNoise. Resizing： Normalization： 在推理时的数据集设置 推理charset 值（排列数）的选择 K值选择 在 Synthetic 数据集和 Real 数据上与 SOTA 对比结果 不同数据集上的对比结果 在更有难度的数据集上的对比结果 更多对比 在模型大小上的对比 模型大小的对比 Appendix Permuted Language Model 乱序语言模型 PERT 这种训练方式的提出是基于一个有趣的现象：一句话中打乱某几个词语的顺序，并不会影响我们对这句话的理解，因此 PERT 模型的作者在 BERT 的基础上修改了输入方式提出了 乱序 的预训练任务。本文也借鉴于此。 ViT 编码输入编码方式 参考","categories":[{"name":"Paper","slug":"Paper","permalink":"https://andrew-rey.github.io/categories/Paper/"}],"tags":[{"name":"Paper","slug":"Paper","permalink":"https://andrew-rey.github.io/tags/Paper/"},{"name":"STR","slug":"STR","permalink":"https://andrew-rey.github.io/tags/STR/"}],"author":"Andrew-Rey"},{"title":"DeepLearning | Diffusion Model","slug":"DL/diffusion","date":"2023-12-08T02:54:32.000Z","updated":"2023-12-21T01:50:03.333Z","comments":true,"path":"2023/12/08/DL/diffusion/","link":"","permalink":"https://andrew-rey.github.io/2023/12/08/DL/diffusion/","excerpt":"\"The structure is already complete with the marble block, before I start my work. It is already there, I just have to chisel away the superfuous material.\" - Michelangelo","text":"\"The structure is already complete with the marble block, before I start my work. It is already there, I just have to chisel away the superfuous material.\" - Michelangelo \"雕塑已经在大理石里了, 我的工作只是把不要的部分拿掉.\" -- 米开朗琪罗 reference, DDPM paper, DDPM code, DDPM inference Diffusion Model Most of the diffusion models nowdays are similar to Denoise Diffusion Probability Model (DDPM), so the follow contents will take this base model as an example. diffusion Model Workflow reverse process: give an image sampled from Gaussian noise that have the same shape with the final expected output input this image into a Denoise module, and the output is a little denoised image loop the step 2 until reaching the pointed times you get the output image the Denoise modules are marked from (pointed) to , and the noise is inputed from to not only is the noise image inputed into denoise module, but also the texts and the noise degree are inputed denoise module includes a Noise Predictor and a Minus operator reverse process in one Denoise after add texts supervise signal, the module is texts supervised forward process (also known as difussion process): give an real image add noises to the image loop the step 2 get the niosed image the noise added in step number is the groud truth of the input image, the noise degree and the texts the training set needs to be a image-text pair forward process the training step is training step Stabel Diffusion The framework includes 3 parts text encoder: inputs the texts and outputs the vectors that descrip the texts generation model: mostly frequent use is the diffusion model, the input is noises and encoded texts, the outputs is an intermediate products image (compressed image that human can understand or not) decoder: the input is the intermediate products and the output is the image expected The three modules are trained respectively stable diffusion Text Encoder Fréchet Inception Distance (FID): the metric to judge the performance of one image. use a pre-trained CNN model input the real image or generated image respectively take out the representation (outputs of the last layer) suppose the representation vectors submits to Gaussion distribution respectively, use Fréchet distance between the two distributions, smaller is better a lots of samples are needed Clip: Contrastive Language-Image Pre-Training. Give the text-image paris, texts are inputed to a text encoder while the images are inputed to a image encoder use this large model to judge if the generated image matches the texts calculate the distance between encoded text vector and encoded image vector Decoder Decoder can be trained without text labels. first method: use only images, small image and large image pairs, the smaller image is the input and larger image is the label second method: use anto-encoder, usually the output of the generation model is a latent representation. Input ground truth image to auto-encoder to get the latent representation, and input latent representation into the decoder Generation Model Usually use the diffusion model as the generation model. forward process: input the ground truth image to the encoder and get the latent representation. Add noises to the latent representation. Loop for a pointed times. Use (noised latent representation, noise degree, texts) as the noise predictor's input and the added noises as the ground truth. reverse process: input the noises sampled from Gaussian distribution and the encoded texts to denoise module, output the latent representation. Principle of diffusion Model VAE model: input the ground truth image and use a encoder to get the latent representation and input the latent representation to decoder to get the image. differences between VAE and diffusion model The training process is given by the follow algorithm training algorithm The is the clean image sampled from dataset and is sampled from a uniform distribution where is large. is sampled from the normal dsitribution. Gradient calculation: is defined later, the value of decreases from to . mixes the clean image and the noises. is a function with two parameters one is the mixed image and the other is , which is called noise predictor. The ground truth is . The inference process is given by the follow algorithm inference algorithm The image illustrate the steps of inference algprithm inference algorithm image version 所有图片生成模型的 共同目标 是希望找到一个 Network, 从 易于采样 的分布中抽取样本, 并输出一个给定的分布形式. 通常这个 Network 会输入一段文字 (Condition). 训练的目标是使输出的分布和真实分布越接近越好. Definition and mathematic symbols: Network: the parameters in network noted as Generated distribution: noted as Real distribution: noted as Maximum Likelihood Estimation Sample Assume we can calculate : given , we can calculate the probability generated by The target is Where the item is not related to . We can infer that maximize likelihood is equivalent to minimize KL divergence. (KL-divergence always greater or equal to 0) GAN model minimize some divergence VAE, diffusion model maximize likelihood VAE example: The generated distribution can be written as , where is the input known distribution such as Gaussian distrbution. VAE assmu that where is the mean of and is also the output of network VAE maximize the lower bound of to get as greater as possible. Take as for short where the item , and the distribution can be any distribution. In VAE, the distribution is the encoder. The goal of VAE is to maximize the lower bound: For DDPM, there are many denoise steps. One denoise is one step, after steps, DDPM produces the expected image. Every step of denoise produces one intermidiate product. So DDPM maximize the lower bound: , where is the diffusion process not the encoder in VAE. How to calculate the distribution . Define the hyper-parameters . The noise sampling processes are independent to each other. If we want to calculate , there is a simpler way using iterative inference. We combine the two steps: The item is also a Gaussian distribution as are independent to each other. We denote them as , which means we can only sample one time, from to . More generally, if we want to generate , we need only sample one time from .x infer-1 infer-2 The middle item has no relation to the network parameter . We mainly focus on the third item. calculate : given and , calculate the 's distribution. Using Bayes formula: infer-3 Whatever, the result is still a Gaussian distribution. How to minimize the expectation of KL-Divergence (the third part): We want to minimize the expectation of KL-Divergence, so as long as most of the KL-Divergences are small, the expectation will be probably small. The small divergence means two distributions' distance is small, the two distributions are close to each other. The first distribution, is a fixed Gaussian distribution with known mean and variance which the second distribution is our denoise module that will be trained. So we want to let the second distribution's mean close the fixed mean (if ignore the variance). The deonise model inputs and the value , outputs the mean of distribution, expecting that the mean is close to the mean of . Infact, the parameters are constants, the only part denoise should predict is the noise or .","categories":[{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/categories/DL/"}],"tags":[{"name":"Deep learing","slug":"Deep-learing","permalink":"https://andrew-rey.github.io/tags/Deep-learing/"},{"name":"Diffusion model","slug":"Diffusion-model","permalink":"https://andrew-rey.github.io/tags/Diffusion-model/"}],"author":"Andrew-Rey"},{"title":"DeepLearning | Loss in Super-Resolution","slug":"DL/loss","date":"2023-12-07T01:38:57.000Z","updated":"2023-12-21T01:50:03.356Z","comments":true,"path":"2023/12/07/DL/loss/","link":"","permalink":"https://andrew-rey.github.io/2023/12/07/DL/loss/","excerpt":"The loss functions used in super-resolution tasks have differences from other tasks like speech recognition. The mainly reason is that not only should the researchers compare the distance of two or more image matrices, but also the image structure and other indices.","text":"The loss functions used in super-resolution tasks have differences from other tasks like speech recognition. The mainly reason is that not only should the researchers compare the distance of two or more image matrices, but also the image structure and other indices. Peak signal-to-noise ratio (PSNR) reference Definition: the maximum possible power of a signal and the power of currupting noise that affects the fidelity of its representation. commomly used to quantify reconstruction quality for images and videos subjuct to lossy compression For monochrome image with size and its noise approximation , is defined by The with form is Where the is the maximum possible pixel value of image , such as the image is represented with 8 bits per pixel, . for 8-bit images, the common ranges from to , the higher the better apply this index under the same codec and the same content poorer performance compared with other quality metrics Structural similarity index measure (SSIM) reference Definition: is used for measuring the similarity between two images. This metric is a full reference metric, which means this measurement or prediction of image quality is based on an initial uncompressed or distortion-free image as a reference. The difference with other techniques such as MSE or PSNR is that these approaches estimate absolute errors. Structural information is the idea that the pixels have strong inter-dependencies especially when they are spatially close. These dependencies carry important information about the structure of the objects in the visual scene. For luminance , contrast and structure , the individual formulas between samples are where , and the is Setting the to , the can be calculated by the constants is used to stablize the division with weak denominator. , , where is the dynamic range of pixel values (such as with 8-bit image) not satisfies triangle inequality or non-negativity, thus is not a distance function under certain conditions, can be converted into a normalized root measure which is a distance function. the square of it is not convex but a locally convex and quasiconvex Connectionist temporal classification (CTC) reference-1, reference-2 The task of CTC is given two sequences one is the input and the other is the output , our aim is to find a mapping from the set of to the set of the length of and can vary; no corrspondence of the elements between two sequences; the input and the output may be disalignment CTC handles the task. The output of CTC is the probability distribution of all possible 's and we can use the distribution to infer the most likely sequence. Loss function: for a given input, we want to train our model to maximize the probability it assigns to right answers. CTC is an alignment-free method. The total loss is conducted on a union of alphbet and a blank symbol , namely the input and the output is the subset of . This map function is conducted by the following 3 steps: alignment Usually, the CTC loss divides the input sequence into small strides based on time-step (if for an image, the time step is a small width) and outputs a probability distribution for every time-step. The whole output of the input sequence/image forms a probability matrix with the shape , where is the size of the union set of alphabet and blank and is the sequence length. probability of sequence The math form is : the CTC conditional probability : marginalizes over the set of valid alignments : computing the probability for a single alignment step-by-step Dynamic programming solution If we aren’t careful, the CTC loss can be very expensive to compute. We could try the straightforward approach and compute the score for each alignment summing them all up as we go. The problem is there can be a massive number of alignments. For most problems this would be too slow. But we can conduct DP algorithm to solve this problem. * The key insight is that if two alignments have reached the same output at the same step, then we can merge them.* dp algorithm There are no need to calculate all paths' probabilities, which is computation consuming. Considerring the order of input and output, we introduce a new output sequence noted , namely inserting the blank between every two neighboring characters including the head and the tail. There are two cases in the whole paths map between neighboring time step. case 1 is that for step character, the preceding nodes include three possible statuses: the , the same character and the character different from it. case 2 is that for step character, the current node is in recurrent, so the preceding nodes include two possible statuses: the and the same character. path map For the formula form, we define the symbol as the CTC loss of the subsequence after input time steps. case 1 formula: , where the factor means the CTC probability of the two valid subsequences after input steps and the later probability means the probability of the current character at input step case 2 formula: Note that there are two valid start states and two valid end states, so the compelete probability is the sum of the final two nodes.","categories":[{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/categories/DL/"}],"tags":[{"name":"Deep learning","slug":"Deep-learning","permalink":"https://andrew-rey.github.io/tags/Deep-learning/"},{"name":"loss function","slug":"loss-function","permalink":"https://andrew-rey.github.io/tags/loss-function/"},{"name":"super-resolution","slug":"super-resolution","permalink":"https://andrew-rey.github.io/tags/super-resolution/"}],"author":"Andrew-Rey"},{"title":"Paper | TATT","slug":"Papers/tatt","date":"2023-12-02T06:03:59.000Z","updated":"2023-12-21T01:50:03.360Z","comments":true,"path":"2023/12/02/Papers/tatt/","link":"","permalink":"https://andrew-rey.github.io/2023/12/02/Papers/tatt/","excerpt":"Paper name: A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution","text":"Paper name: A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution The mainly work contributed by this work is providing a noval method to do super-resolution(SR) on low-resolution(LR) scene-text image. Model Architecture The proposed TATT architecture for STISR is shown as follow. TATT arch TPGB is short for text prior guided blocks, TPG is short for text prior generator and SRB is short for sequential-recurrent block. Input: low-resolution(LR) text images Text prior generotor (TPG): usually CRNN model, input the LR images and output text prior, the recognition probability sequence. Convolution (CONV): inputs low resolution images and outputs the images features TP Interpreter (TPI): two inputs, one is the text prior and the other is the image features, the outputs is the TP Map, which can interpret the text prior into image features. Attention-based block. Text prior genertor block (TPGB): two inputs, one is the image features deliveryed by CNN, and the other is the text prior map deliveryed by TPI. Includes 5 SRB blocks. Sequential residual block (SRB): proposed in TSRN model. Pixel shuffle: a method to up-sample the pixel, usually used in super-resolution (SR) tasks. Extend channel dimensions. Loss The total loss is image loss adding recognizer distill loss Model in codes The most important function in this codes provided by the authors is train defined in super_resolution.py, which occupies 822 lines... with a chaotic logic but with no comment. However, we should read the codes to study the model deeply. Data type initialization There is a class named AsterInfo defined in base.py which is used not only for ASTER model, but also for any other CRNN-like models. This class defines vocabulary and some basic text-sequence parameters. Data stream while training The most important key to understand the whole model in codes is to grasp the data stream in calculations. We can look up how the TATT model is invoked firstly: cascade_images, ret_mid = model_list[pick]( images_lr if not self.args.for_cascading else cascade_images, label_vecs_final.detach() ## text embedding ) Where the images_lr is the LR images loaded by images_lr = self.torch_rotate_img(images_lr, arc, rand_offs), for an inversion mode, namely the variable images_lr_ret is generated by images_lr_ret = self.torch_rotate_img(images_lr.clone(), -arc, rand_offs). We continue to look up the parameter label_vecs_final and we find that this is calculated by stu_model, namely, CRNN model: stu_model = aster_student[tpg_pick] ## namely, CRNN aster_dict_lr = self.parse_crnn_data(cascade_images[:, :3, :, :] if not self.args.y_domain else images_lrraw[:, :3, :, :]) label_vecs_logits = stu_model(aster_dict_lr) label_vecs = torch.nn.functional.softmax(label_vecs_logits, -1) label_vecs_final = label_vecs.permute(1, 0, 2).unsqueeze(1).permute(0, 3, 1, 2) Where the function parse_crnn_data is to interpolate the LR image (namely the cascade_images) into shape (32, 100) with bicubic interpolation, but the output of this function is a weighted addition image 0.299 * R + 0.587 * G + 0.114 * B (grey-scale image). So this part outputs the text priors. Continue to follow the data cascade_images, which is the output of TATT. Surely we find the loss next: im_quality_loss = image_crit(cascade_images, images_hr), this calculates the image quality loss. The image quality loss includes two parts: MSE and GradientPriorLoss(GPL). After a series of tricks (calculate the loss_img_each), the loss become a part of total loss. So what's the total loss? There it is: loss_im = loss_img + loss_recog_distill. the first part loss_img: includes image quality loss (MSE + GPL) and TSSIM loss, the second item will be introduced later. the second part loss_recog_distill: comes from semantic loss between student model and teacher model. After that, our optimizer comes on! optimizer_G.zero_grad(), which is also the necessary sentence in training. The next is backward propagation loss_im.backward(). The model uses clip_grad_norm_ to prevent gradient explosion. The finally is taking a step optimizer_G.step(). The above data-stream is for one batch when training. TPG block in code The text prior generator block uses CRNN (can be customized in command argues, the default is CRNN) to extract the text priors. And the TPG provides two model options, one is the default CRNN, and the other is \"TPG\"(infact, the CRNN). And we take CRNN as an example, namely the function CRNN_init defined in base.py, which uses the CRNN class defined in crnn.py not Model class defined in crnn/model.py. TSRN_TL_TRANS The other parts of TATT aggregate to one TSRN-like model defined in tsrn.py. The initialization in train using genertor_init() function and the author put it into model_list. # model includes 1conv block, 5 srb blocks, 1 conv block, 1 upsample block # defines TPS based STN block model = tsrn.TSRN_TL_TRANS( scale_factor=self.scale_factor, width=cfg.width, height=cfg.height, STN=self.args.STN, mask=self.mask, srb_nums=self.args.srb, hidden_units=self.args.hd_u ) Firstly we analysis the forward function. TPS-based STN: the input images use TPS-based STN to recover the text images to normal shape. convolution with PReLU: block = {'1': self.block1(x)} and padding_feature = block['1'] TPInterpreter: in code, it is infoGen: self.infoGen = TPInterpreter(text_emb, out_text_channels, output_size=(height//scale_factor, width//scale_factor)) # InfoGen(text_emb, out_text_channels) and invoked by tp_map, pr_weights = self.infoGen(padding_feature, text_emb). Its inputs are padding_feature and text_emb namely image feature and text prior respectively. 5 SRBs: input image feature coming from convolution and TPMap coming from TPI, the final output is also image feature. convolution with batch norm pixel shuffle: namely, block 8. tanh: input the output of block-8. *TPS-based STN TPInterpreter TPI module aims to interpret the text prior to the image feature so that the influence of semantics guidance can be exerted to the correlated spatial position in the image feature domain. TPI module is transformer-based, the architecture is shown as follow: TPI arch There are two inputs in TPI, one is the text prior (with shape ) (in fact the shape is reshaped into later in forward function) and the other is image feature. Based on the analysis before, we claim that text prior comes from TPG module, which represents in code with the name label_vecs_final image feature comes from CONV module, which represent in code with the name block1 There are only one output in TPI, which is the text prior map, TP map for short. There are another modules that are similar two a normal transformer. Firstly, the whole architeture of TPI is divided into encoder and decoder. MSA is short for multi-head self attention, LN is short for layer norm, FFN is short for feed-forward network and MCA is short for multi-head cross attention. The introduction of attention will be analysised later (not in this article). But we concern the position encoding firstly, FPE (Fixed Positional Encoding) for text prior and RPE (Recurrent Positional Encoding) for image feature. The TPInterpreter initialized with parameters: ## definition of TPI class TPInterpreter(nn.Module): def __init__( self, t_emb, out_text_channels, output_size=(16, 64), feature_in=64, # d_model=512, t_encoder_num=1, t_decoder_num=2, ): pass text_emb = 37 out_text_channels = 64 output_size = (height//scale_factor, width//scale_factor) = (16, 64) For invoking, the forward function takes in two parameters image_feature and tp_input (text prior) and the outputs of TPI are text_prior, pr_weights where the first is the TP map we need: ## definition of forward in TPI def forward(self, image_feature, tp_input): pass return text_prior, pr_weights ## invoke in TATT model's forwrad function tp_map, pr_weights = self.infoGen(padding_feature, text_emb) ## invoke of TATT model cascade_images, ret_mid = model_list[pick]( images_lr if not self.args.for_cascading else cascade_images, label_vecs_final.detach() ## text embedding ) The image_lr is passed into infoGen after a convolution operation, and the label_vecs_final is passed into infoGen directly and returns the tp_map, pr_weights to TATT model. TATT model uses tp_map later in SRB blocks to guide SR. Now analysis the details in forward and __init__: forward: we catch the data stream firstly in this function, and find the definitions in initialization function. The image_feature is reshaped into firstly (flatten operation, where 16:width, 64:height, N: batch size, 64:channels). And it's passed into InfoTransformer later directly. The tp_input is reshaped into firstly, and goes to the PReLU activation and a fully connected layer which are used for projection. Defines PositionalEncoding Passes all those variables into InfoTransformer returns text_prior (text prior map namely) &gt;&gt; InfoTransformer Defined in transformer_v2.py, which mainly realizes the TPI module. Its forward function is def forward(self, src, mask, query_embed, pos_embed, tgt=None, text_prior=None, spatial_size=(16, 64)): pass. src: reshaped text prior, namely the tp_input with shape mask: with shape query_emb: self.init_factor.weight namely, which is nn.Embedding(1024, 64).weight pos_emb: x_pos passed in TPI, the result of PositionalEncoding tgt: x_im namely the image_feature with shape After entering the forward, query_emb is firstly passed into GRU to encode. why call query_emb: because in decoder, this variable is passed into a multi-head cross attention module and acts as the query. Different from normal transformer, this transformer has only 1 layer of encoders and 2 layers of decoders. &gt;&gt;&gt; InfoTransformer.encoder self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm), use TransformerEncoderLayer and LayerNorm. TransformerEncoderLayer: adds positional encoding and the original tensor, use forward_post function, same with the architecture image. The FPE realized with PositionalEncoding. Positional Encoding ref. &gt;&gt;&gt; InfoTransformer.decoder self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,return_intermediate=return_intermediate_dec), use TransformerDecoderLayer_TP and LayerNorm. The invoking: hs = self.decoder(tgt, memory, memory_key_padding_mask=mask, pos=pos_embed, query_pos=query_embed) TransformerDecoderLayer_TP: the input query_embed has been encoded in GRU, memory is the output of encoder, pos_embed is the text prior positional encoding, tgt is the image feature. SRB SRB block is firstly proposed in Scene Text Image Super-Resolution in the Wild, where the SR task on scene text image is firstly proposed. The architecture of SRB is shown as follow: SRB However, the BiLSTM block is replaced by GRU block in this work. Pixel Shuffle Up-sample in channel dimmension. SSIM Loss SSIM (Structure Similarity Index) loss is used to compare the similarity between two images, which is sensitive to local structure variance. The loss can be divided into 3 parts: illuminance, contrast ratio and structure, the formulas are: The combination of the 3 parts is our SSIM loss.","categories":[{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/categories/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://andrew-rey.github.io/tags/deep-learning/"},{"name":"TATT","slug":"TATT","permalink":"https://andrew-rey.github.io/tags/TATT/"},{"name":"scene text","slug":"scene-text","permalink":"https://andrew-rey.github.io/tags/scene-text/"}],"author":"Andrew-Rey"},{"title":"DeepLearning | Quick Start II","slug":"DL/learn-2","date":"2023-11-24T06:16:26.000Z","updated":"2023-12-22T15:26:36.115Z","comments":true,"path":"2023/11/24/DL/learn-2/","link":"","permalink":"https://andrew-rey.github.io/2023/11/24/DL/learn-2/","excerpt":"Is it must be overfitting when the loss is not small? Infact there are two possible reasons for this situation. The one is model bias and the other is the poor optimization.","text":"Is it must be overfitting when the loss is not small? Infact there are two possible reasons for this situation. The one is model bias and the other is the poor optimization. Questions when training Big loss when training Notice that this situation appears in training step If you get the big loss when training the network, one prossible reason is the model bias, which means you may try other bigger model with more elasticity; the other reason is the optimizer works poor. How two judge the two situation? If you get bigger loss not only in test set, but also in train set compared to a smaller model, the most possible reason is the weak optimizer. big loss Overfitting Small loss in training set but big in testing set. Solutions: Expend the training data collect more training data data augmentation (must be suitable for the training task) Constrain the model, decrease the flexibility less parameters (less neuron) shareing parameters less features early stop regularization dropout Model selection Cross validation: split your training set into validation set and training set, use validation set to select model k-folder method Optimization Gradient nears to zero but loss still big, maybe the model is still on the critical point (gradient equals to zero, not only the local minima but also the saddle point). Suppose the loss function is zero at point , use Taylor formula to estimate the value of loss around point where is the Hessian matrix . Considerring that the gradient is zero, namely where . &lt;u&gt;For all&lt;/u&gt; : if , namely is positive definite (all eigen values are above zero): , which means the point is a local minima if , namely is negtive definite (all eigen values are below zero): , which means the point is a local maxima if sometimes while sometimes , the point is a saddle point So one way to judge the type of criticle point is to calculate all the eigen value of Hessian and see the sign of them escape saddle point: the loss can be updated (continue to decrease) towards the eigen vector's direction: where is the eigen vector for the positive eigen value local minima and saddle point: Which one appears more frequently in the practice? There is a hypothesis. Suppose a local minima for a 1-dimmension independent variable, such as the quadratic function, it has a local minima. But put it to the 2-dimmension space, the local minima may convert into a saddle point. So the hypothesis is that if there is a local minima point, it can be converted into a saddle point in an enough high space. The big model nowadays has millons of parameters in loss function, so there must be lots of saddle point that gradient equaling to zero but can being decreased. The saddle points are more common. batch: divide all training set into small batches, for every batch, calculate the loss gradient, update parameters. The process that all batches had been seen is called an epoch. Shuffle all batches after one epoch. Why use batch? Decreasing the noise steps when updating the parameters and the parallel calculation.","categories":[{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/categories/DL/"}],"tags":[{"name":"Deep learning","slug":"Deep-learning","permalink":"https://andrew-rey.github.io/tags/Deep-learning/"},{"name":"tutorial","slug":"tutorial","permalink":"https://andrew-rey.github.io/tags/tutorial/"},{"name":"lhy-ML","slug":"lhy-ML","permalink":"https://andrew-rey.github.io/tags/lhy-ML/"}],"author":"Andrew-Rey"},{"title":"DeepLearning | Quick Start I","slug":"DL/learn-1","date":"2023-11-24T02:54:38.000Z","updated":"2023-12-22T15:26:31.334Z","comments":true,"path":"2023/11/24/DL/learn-1/","link":"","permalink":"https://andrew-rey.github.io/2023/11/24/DL/learn-1/","excerpt":"The first lecture that all ai-beginner should to take. video","text":"The first lecture that all ai-beginner should to take. video Introduction deep learning aims to finding a function. classification regression structured The general step to do machine learning: write down the function model with unknown parameters need domain knowledge define loss from training data, loss is a function of &lt;u&gt;parameters&lt;/u&gt; in your model how good a set of parameters are calculate from training data visuaization: error surface optimization your loss with gradient descent gradient descent: \\(w^{\\star}, b^{\\star}=\\arg\\min_{w,b} Loss\\) compute the derivative \\(\\frac{\\partial L}{\\partial w}|_{w=w^0}\\) where \\(w^0\\) is picked randomly. Use the learning rate to stand for the step width: \\(w^1 \\leftarrow w^0 - \\eta \\frac{\\partial L}{\\partial w}|_{w=w^0}\\). Then, update \\(w\\) until reaches a threshold (such as the max iteration count or minimum updation) The above three steps is train. Model model bias: comes from unreasonable model. How to construct a reasonable model or how to fit the model functions are the main problems in ML/DL. The most common idea is to use the combination of function units to fit the model. Such as the sigmoid function. piecewise linear: \\(constant + \\sum c_i sigmoid(b_i + w_i x)\\). This can simulate lots of functions (including &lt;u&gt;piecewise linear constructed with lots of small line segements&lt;/u&gt; and &lt;u&gt;curve functions&lt;/u&gt; which can be divided into many piecewise linear) \\[ c \\cdot Sigmoid(b+ w x) = c\\frac{1}{1 + e^{-(b + w x)}} \\] \\(w\\): change slopes \\(b\\): shift \\(c\\): change height For multi-features (suppose \\(j\\) festures): \\(y = const + \\sum_i c_i sigmoid(b_i + \\sum_j w_{ij} x) = const + c^T \\sigma(\\mathbf{b} + W \\mathbf{x})\\) sigmoid-matrix Sigmoid function is also called Activation function. Loss and Optimization Concatenate \\(W, b, c, const\\) into a vector (for matrix \\(W\\), concatenate its rows or colums into a vector) named \\(\\theta\\). Optimiza the loss function and calculate the parameter \\(\\theta\\): \\[ \\theta^{\\star} = \\arg\\min_{\\theta} L \\] Note \\(\\mathbf{g}\\) for gradient: \\(\\mathbf{g_i} = \\triangledown L(\\theta^i)\\), update the \\(\\theta\\) using \\(\\theta_{i+1} \\leftarrow \\theta_i - \\eta \\mathbf{g_i}, \\mathbf{g}_{i+1}=\\triangledown L(\\theta_{i+1})\\). Stop updating when converging. Train with large dataset: divide dataset into batches, update \\(\\theta\\) on each batch; The process that all batches have been seen is named epoch. More activation functions ReLU: Rectified Linear Unit: \\(ReLU(b + w x) = \\max(0, b + w x)\\) Same as sigmoid, ReLU is also an activation function More layers note \\(a = ReLU(\\mathbf{b}+W \\mathbf{x})\\), continue the same work: \\(a^{\\prime} = ReLU(\\mathbf{b^{\\prime} + W^{\\prime}\\mathbf{a}})\\) and so on... The whole thing we do just now can be show as follow: multi-layer You can see the loss decreases more than just one layer. Each activation function dubbed Neuron, one layer neurons overlays another one build the Neural Network or you can also say many hidden layers build the Deep Learning. overfitting: sometimes, the deeper network brings overfitting problem.","categories":[{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/categories/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://andrew-rey.github.io/tags/deep-learning/"},{"name":"tutorial","slug":"tutorial","permalink":"https://andrew-rey.github.io/tags/tutorial/"},{"name":"lhy-ML","slug":"lhy-ML","permalink":"https://andrew-rey.github.io/tags/lhy-ML/"}],"author":"Andrew-Rey"},{"title":"DeepLearning | Schedual","slug":"DL/schedual","date":"2023-11-23T04:04:03.000Z","updated":"2023-12-21T01:50:03.359Z","comments":true,"path":"2023/11/23/DL/schedual/","link":"","permalink":"https://andrew-rey.github.io/2023/11/23/DL/schedual/","excerpt":"","text":"任务 场景文本图像增强与识别（Scene Text Image Super-Resolution &amp; Recognition ），这个任务是需要设计一个co-training的模型，能够既提升图像质量又提升下游识别器的识别性能，所以需要对场景文本图像超分辨率（这个任务的目标是放大场景图像图像的尺寸后（如32 x 128变为64 x 256），仍能保持良好的图像质量和文本识别模型的识别效果）以及场景文本图像识别具有一定的基础知识铺垫 模型基本结构是统一的encoder，然后有两个decoder分别负责 超分和识别 相关论文 STISR：需要掌握的模型有20-ECCV-TSRN（创立这个任务的）、22-CVPR-TATT（一个比较好的学习代码的模型）、23-MM-STIRER STR：需要掌握的模型有22-IJCAI-SVTR、22-ECCV-PARSeq、23-ICCV-CCD Transformer与对比学习：李宏毅2021春机器学习课程视频：https://www.bilibili.com/video/BV1Wv411h7kN?p=49&amp;vd_source=7e3e00e12a59319e12d36c03a368ada5 &amp; 跟李沐学AI：https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.788&amp;vd_source=7e3e00e12a59319e12d36c03a368ada5 &amp; https://www.bilibili.com/video/BV19S4y1M7hm/?spm_id_from=333.788&amp;vd_source=7e3e00e12a59319e12d36c03a368ada5 计划与安排 目前的计划安排： 接下来一周阅读这两个方向的论文（每个方向1篇）并学习代码，并于一周后参与组内讨论，对论文进行分享 随后根据分享与讨论情况布置后续工作与任务安排 后续深造期间的研究方向以这个工作为契机了解场景文本图像相关的内容后再进行确定 毕设方面严卡节点：包括论文翻译、开题报告与答辩、中期、毕业论文（4月底-5月下旬需要撰写论文（3-4周）（即在此节点之前核心实验与算法需要跑完，写论文过程中可以继续增加消融实验），审核并修改（约需2轮，1周）），尽量提前 其他 科研常用的工具： Typora：非常好用的Markdown文本编辑器 https://github.com/Keldos-Li/typora-latex-theme Latex风格的基于Typora的md排版 Mathpix：对公式进行截图并转换成Latex模板 Axmath与Axglyph：国产的可视化公式编辑与画图软件，可插入office等 TablesGenerator：在线的表格转latex工具 网易有道词典：浮窗状态下可右键查询单词 PaperwithCode：论文开源项目推荐与索引网站，包含各种任务的排行榜 Readpaper：论文阅读、要点介绍与细节讨论 坚果云：多设备文件与数据的云备份与更新 Notion：关系型文档数据库（重要，我们Group一般用这个进行交互） 一个硬核程序员向的notion任务规划导向的搭建 everything：快速的电脑全局搜索工具 有任何问题可以及时跟我沟通，我们的小组会一般是每周日下午2点30（需要尽量参加），薛老师大组会2周一次（一般是周二下午）","categories":[{"name":"Dl","slug":"Dl","permalink":"https://andrew-rey.github.io/categories/Dl/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://andrew-rey.github.io/tags/deep-learning/"},{"name":"schedual","slug":"schedual","permalink":"https://andrew-rey.github.io/tags/schedual/"}],"author":"Andrew-Rey"},{"title":"Math in a Mess | Automatic differentiation","slug":"Math/AutoDiff","date":"2023-11-18T15:13:18.000Z","updated":"2023-11-20T07:29:36.395Z","comments":true,"path":"2023/11/18/Math/AutoDiff/","link":"","permalink":"https://andrew-rey.github.io/2023/11/18/Math/AutoDiff/","excerpt":"The automatic differentiation (AutoDiff) has been widely used in nerual network, which is a set techniques to evaluate the partial derivative of a function specified by a computer program. AutoDiff is distinct from symbolic differentiation and numerical differentiation. Symbolic differentiation suffers from being converted into computer programs while the numerical differentiation leads to round-off errors in the discretization process and cancellation.","text":"The automatic differentiation (AutoDiff) has been widely used in nerual network, which is a set techniques to evaluate the partial derivative of a function specified by a computer program. AutoDiff is distinct from symbolic differentiation and numerical differentiation. Symbolic differentiation suffers from being converted into computer programs while the numerical differentiation leads to round-off errors in the discretization process and cancellation. Chain Rule There are two ways to realize AutoDiff, but the main idea of automatic differentiation is chain rule. Take function as an example, the partial derivative w.r.t the independent variable is: For a more general form, considering a composite function of , its partial drivative is: The two type of AutoDiff: forward accumulation: inside to outside, . For each independent variable, a separate pass is necessary. So this method is more efficient for function . reverse accumulation: outside to inside, . This method evaluates the function first and calculates the derivatives w.r.t all independent variables in an additional pass. So this method is more efficient for function . Computational Graph The graph that shows the composition relations and orders of one function is computational graph. The detail structure of computational graph is shown as follow: 1700323960325 This graph represents the function: f = w5 w5 = w3 + w4 w4 = a0 * w2 w3 = a1 * w1 w2 = w0 w1 = w0 w0 = x Forward Accumulation fix one variable that you want to perfrom differentiation on compute the derivative of each sub-expression recursively Finally, calculate where . This is a Bottom-up method. Infact, for a node satisfies . For multiple variable (say, the vector input), this can be matrix product (Jacobians). Take the function as an example, we want to know the derivative on the point . Notation: . We choose the independent variable as our target to perform differentation., which also called a seed. The seed values are initialized to: As the seed values are set, the values propagate using the chain rule: Operations to compute value Operations to compute derivative (seed) (seed) So, once given the value of point , we can know the derivative of . If you want to know , you should conduct the steps above again with the seed . The computational graph is forward Reverse Accumulation","categories":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/tags/Math/"},{"name":"differentiation","slug":"differentiation","permalink":"https://andrew-rey.github.io/tags/differentiation/"}],"author":"Andrew-Rey"},{"title":"Paper | quick palm","slug":"Papers/quick-palm","date":"2023-11-18T02:33:01.000Z","updated":"2023-11-18T04:43:35.856Z","comments":true,"path":"2023/11/18/Papers/quick-palm/","link":"","permalink":"https://andrew-rey.github.io/2023/11/18/Papers/quick-palm/","excerpt":"Quick Start","text":"Quick Start few shot From Instance to Metric Calibration: A Unified Framework for Open-World Few-Shot Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI'23) Learning to Learn from Corrupted Data for Few-Shot Learning. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI'23) Boosting Few-Shot Open-Set Recognition with Multi-Relation Margin Loss. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI'23) Conditional Self-Supervised Learning for Few-Shot Classification. In: Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI'21) kernel/hyperbolic/traditionalML Towards Kernelizing the Classifier for Hyperbolic Data. Frontiers of Computer Science (FCS'24) Indefinite Twin Support Vector Machine with DC Functions Programming. Pattern Recognition (PR'22) CosNet: A Generalized Spectral Kernel Network. In: Advances in Neural Information Processing Systems (NeurIPS'23) Expanding the Hyperbolic Kernels: A Curvature-aware Isometric Embedding View. In: Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI'23) Automatically Gating Multi-Frequency Patterns through Rectified Continuous Bernoulli Units with Theoretical Principles. In: Proceedings of the 31st International Joint Conference on Artificial Intelligence (IJCAI'22) cv/super-resolution FATE: A Three-stage Method for Arithmetical Exercise Correction. Neural Computing and Applications (NCA'23) Improving Scene Text Image Super-resolution via Dual Prior Modulation Network. In: Proceedings of the 37th AAAI Conference on Artificial Intelligence (AAAI'23) others Learning Graph-level Representation from Local-structural Distribution with Graph Neural Networks","categories":[{"name":"Papers","slug":"Papers","permalink":"https://andrew-rey.github.io/categories/Papers/"}],"tags":[{"name":"paper","slug":"paper","permalink":"https://andrew-rey.github.io/tags/paper/"},{"name":"palm","slug":"palm","permalink":"https://andrew-rey.github.io/tags/palm/"}],"author":"Andrew-Rey"},{"title":"Math | Dual Number","slug":"Math/dual-number","date":"2023-11-16T08:18:11.000Z","updated":"2023-11-16T09:06:35.071Z","comments":true,"path":"2023/11/16/Math/dual-number/","link":"","permalink":"https://andrew-rey.github.io/2023/11/16/Math/dual-number/","excerpt":"Given two real numbers and defines a symbol satisfying while , the number is called Dual number.","text":"Given two real numbers and defines a symbol satisfying while , the number is called Dual number. Differentiation The most interesting part (in my opinion) is the differentiation of dual number functions. And it's is the base of auto-differentiation used in tensorflow or pytorch. Considering a function where is a real number. In the space of dual number, our can be represented as where . In real space, we can use expansion on : Use to replace the in , and use expansion again: And the amazing place is the definition of which satisfies while , so the expansion can be written as: Note: replace the independent variable in real-function with there dual number representation, the function can be represented by the form of dual number where the first item is the real-form of itself and the second item is its derivative.","categories":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/tags/Math/"},{"name":"Number Theory","slug":"Number-Theory","permalink":"https://andrew-rey.github.io/tags/Number-Theory/"}],"author":"Andrew-Rey"},{"title":"Racket | How to launch a rocket","slug":"PL/Racket-How-to-Launch-a-Rocket","date":"2023-11-11T12:55:57.000Z","updated":"2023-11-12T08:07:33.176Z","comments":true,"path":"2023/11/11/PL/Racket-How-to-Launch-a-Rocket/","link":"","permalink":"https://andrew-rey.github.io/2023/11/11/PL/Racket-How-to-Launch-a-Rocket/","excerpt":"\"...Introductory books on programming tend to contain lots of materials about the authors' favorate application discipline: puzzles, mathematics, physics, music and so on. Such material is natural because programming is obviously useful in all these areas, but it also dstracts from the essential elements of programming. Hence, we have made every attempt to minimize the use of knowledge from other areas so that we can focus on what computer science can teach you about computational problems solving.\"","text":"\"...Introductory books on programming tend to contain lots of materials about the authors' favorate application discipline: puzzles, mathematics, physics, music and so on. Such material is natural because programming is obviously useful in all these areas, but it also dstracts from the essential elements of programming. Hence, we have made every attempt to minimize the use of knowledge from other areas so that we can focus on what computer science can teach you about computational problems solving.\" show an image Zerothly, you want to show your favorate image on the screen, such as the rocket. Firstly, maybe you in your DrRacket but I used to use my VSCode anyway, if you want to show an image, one convinient module is slideshow, just add #lang slideshow in the first line. Secondly, just look up the help manual and find a function called show-pict, which can display and image. Meanwhile, bitmap/file can load an image based on the given string-like path. So just use as what I did: #lang slideshow (show-pict (bitmap/file \"path/to/image\")) Run and you will see: 1699773585285 Yes, and we will launch it into the sky. define a scene Racket provides an empty-scene where we can put our objects on it (of course, out of it is just ok), use empty-scene to define. Now we put our samll rocket overlay it but with a smaller size using scale. Before that, you should (require 2htdp/image) to add package. (require 2htdp/image) (empty-scene 800 600) (overlay (scale 0.2 (bitmap/file \"path/to/image\")) scene) But this expression maybe a little annoyed as the nested and confused expressions. So it's necessary to define some constants and variables. ; constants (define image-scaler .2) (define scene-length 800) (define scene-width 600) (define x-offset 0) (define max-height (/ scene-width 2)) (define min-height (- 0 (/ scene-width 2))) ; variables (define scene (empty-scene scene-length scene-width)) (define rocket (scale image-scaler (bitmap/file \"path/to/image\"))) ; interactions (show-pict (overlay rocket scene)) 1699774549772 Note: the CENTER coordinate is (0, 0) change positions We use overlay/offset to define the relative position on our scene. Insert a xy-pair between rocket and scene to define the rocket's position on scene. If we want show another little different positions, just modify the xy-pair to another values. There we use a function provided by 2htdp/universe named animate, which consumes a function as an variable and apply it to time tick every frame. Inaddition, we prefer to use function to do such work. ; functions (animate (λ (height) (overlay/offset rocket x-offset height scene))) 1699775538374 refine the program Anyway, the rocket can fly safely? But there is a little more problems in our program. the rocket flies out of our scene, should stop when flies too high the rocket flies with a constant speed, should be accelerated the rocket flies start on the scene center, not the land ; add acceleration in our rocket (define acceleration .5) (define (get-height t) (/ (* acceleration (* t t)) 2)) ; stop when reach to the highest use max or min height defined above (define (get-location t) (min (+ min-height (get-height t)) max-height)) ; the finally animation (define (fly-up t) (overlay/offset rocket x-offset (get-location t) scene)) 1699776100626 although you can launch a rocket in some degrees, but however, you learnt nothing. I mean you are just coding but with an image or with a new programming language. The same problems appear when you construct a larger project in your usual life. We not learn functional programming but learn the essential elements of programming, learn the construction of projects and learn how to use these tools to solve computational problems.","categories":[{"name":"Programming Language","slug":"Programming-Language","permalink":"https://andrew-rey.github.io/categories/Programming-Language/"}],"tags":[{"name":"Racket","slug":"Racket","permalink":"https://andrew-rey.github.io/tags/Racket/"},{"name":"Functional Programming","slug":"Functional-Programming","permalink":"https://andrew-rey.github.io/tags/Functional-Programming/"}],"author":"Andrew-Rey"},{"title":"Full Stack | RuoYi-Vue","slug":"SE/ruoyi-vue","date":"2023-11-04T06:20:10.000Z","updated":"2023-11-04T07:04:03.642Z","comments":true,"path":"2023/11/04/SE/ruoyi-vue/","link":"","permalink":"https://andrew-rey.github.io/2023/11/04/SE/ruoyi-vue/","excerpt":"\"a glance of RuoYi-vue framework\" ref","text":"\"a glance of RuoYi-vue framework\" ref configuration mysql configuration use datagrip, run scrips in sql/*.sql construct tables !!IMPORTANT!! use utf8 when creating your database edit configuration file ruoyi-admin/src/main/resources/application-druid.yml to your database start redis server in cmd: redis-server edit configuration file ruoyi-admin/src/main/resources/application.yml to your server port start back-end in ruoyi-admin/ find RuoYiApplication.java and run it success when visiting http://localhost:8080 1699079331780 start front-end download the dependencies in ruoyi-ui/ npm run dev 1699079479931 success when the validation code image appears -&gt; front-back-end validation code the first interactions between front and back back-end generate a expr: such as 1+1=2 generate 1+1=?@2, where @ divides question and anwser - give 1+1=? to front - store 2 in redis (会给前端传一份以确定是哪个key，放在隐藏域，用户提交表单) - confirm user input and redis anwser front-end cross-field 反向代理，映射到后端 1699081297521 将/dev-api替换为空，再映射到8080 例如：http://localhost:80/dev-api/captchaImage -&gt; http://localhost:8080/captchaImage back-end","categories":[{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/categories/SE/"}],"tags":[{"name":"Full stack","slug":"Full-stack","permalink":"https://andrew-rey.github.io/tags/Full-stack/"},{"name":"Framework","slug":"Framework","permalink":"https://andrew-rey.github.io/tags/Framework/"},{"name":"RuoYi","slug":"RuoYi","permalink":"https://andrew-rey.github.io/tags/RuoYi/"}],"author":"Andrew-Rey"},{"title":"Profile | Welcome","slug":"Life/profile","date":"2023-10-31T16:33:00.000Z","updated":"2023-10-31T17:39:49.894Z","comments":true,"path":"2023/11/01/Life/profile/","link":"","permalink":"https://andrew-rey.github.io/2023/11/01/Life/profile/","excerpt":"Welcome to my Site import seu.chien_shiung_wu as csw profile = csw.register(\"Andrew-Rey\").time(\"2020-9\") profile.study = \"cs-vr\" profile.current_work = [\"poem2scene\"] profile.published = [] # but will be more and more csw.graduate(profile).time(\"2024-6\")","text":"Welcome to my Site import seu.chien_shiung_wu as csw profile = csw.register(\"Andrew-Rey\").time(\"2020-9\") profile.study = \"cs-vr\" profile.current_work = [\"poem2scene\"] profile.published = [] # but will be more and more csw.graduate(profile).time(\"2024-6\") find me on github find me on bilibili","categories":[{"name":"Profile","slug":"Profile","permalink":"https://andrew-rey.github.io/categories/Profile/"}],"tags":[{"name":"welcome","slug":"welcome","permalink":"https://andrew-rey.github.io/tags/welcome/"}],"author":"Andrew-Rey"},{"title":"Paper Reading | Learning Transferable Visual Models From Natural Language Supervision","slug":"Papers/clip-1","date":"2023-10-30T07:58:33.000Z","updated":"2023-10-31T17:43:40.020Z","comments":true,"path":"2023/10/30/Papers/clip-1/","link":"","permalink":"https://andrew-rey.github.io/2023/10/30/Papers/clip-1/","excerpt":"Natural Language Supervision Learning Transferable Visual Models 利用自然语言的监督信号来训练一个泛化性能好的视觉模型","text":"Natural Language Supervision Learning Transferable Visual Models 利用自然语言的监督信号来训练一个泛化性能好的视觉模型 abstract 之前的视觉模型训练：先有一个固定的、提前定义好的物体类别集合，模型去预测这些定义好的类别来完成训练。 收集数据集简单，模型训练简单 有限制的监督信号，限制了模型本身的泛化性 CLIP 直接从自然语言得到监督信号 范围广，语言描述过的物体就可以被识别到 给定一张图片，给定一个句子，判断配对 训练样本是图片和句子的配对 数据集规模：4亿 自监督 多模态的对比学习 自然语言用于引导模型做物体分类(prompt) 能扩展到新的任务 大多数迁移效果好 开源了预训练模型，只公开了推理的代码，未公开预训练代码 my focus text2image Downstream task forzen clip model and train generator encode text text: poems images: height maps, grey-scale","categories":[{"name":"Paper","slug":"Paper","permalink":"https://andrew-rey.github.io/categories/Paper/"}],"tags":[{"name":"clip","slug":"clip","permalink":"https://andrew-rey.github.io/tags/clip/"},{"name":"paper","slug":"paper","permalink":"https://andrew-rey.github.io/tags/paper/"},{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/tags/DL/"},{"name":"Multimodule","slug":"Multimodule","permalink":"https://andrew-rey.github.io/tags/Multimodule/"}],"author":"Andrew-Rey"},{"title":"Design Pattern | Creational","slug":"SE/design-pattern-2","date":"2023-10-15T02:14:10.000Z","updated":"2023-11-12T13:00:35.455Z","comments":true,"path":"2023/10/15/SE/design-pattern-2/","link":"","permalink":"https://andrew-rey.github.io/2023/10/15/SE/design-pattern-2/","excerpt":"使用设计模式是为了可重用代码，让代码更容易被他人理解，保证程序的可靠性和重用性","text":"使用设计模式是为了可重用代码，让代码更容易被他人理解，保证程序的可靠性和重用性 肯特·贝克和沃德·坎宁安：建筑设计思想 -&gt; Smalltalk的图形用户接口生成 Erich Gamma改为适用于软件开发 James Coplien致力于C++开发，Advanced C++ Idioms Gang of Four (Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides) 出版 Design Patterns - Elements of Reusable Object-Oriented Software ref 工厂方法模式 创建对象时，不直接使用new来创建对象，而是使用工厂方法模式 - 程序中使用大量的new时，当某个对象/构造方法发生变化时，难以维护 - 将频繁出现的对象创建，封装到一个工厂类中 简单工厂模式 public class FruitFactory() &#123; public Fruit getFruit(ID id) &#123; if (id == 0) &#123;return new Apple();&#125; if (id == 1) &#123;return new Peach();&#125; ... &#125; &#125; 当调用方添加新对象时，需要 修改 工厂代码 上述代码不符合 开闭原则：对扩展开放，对修改关闭 FruitFactory提供给调用方使用，应该对修改关闭，即不要修改工厂代码 但是提供方可以对扩展开放 工厂方法模式 public abstract class FruitFactory&lt;T extends Fruit> &#123; public abstract T getFruit(); &#125; public class AppleFactory extends FruitFactory&lt;Apple> &#123; @Override public Apple getFruit() &#123; return new Apple(); &#125; &#125; ... 如果新增了新对象，调用方只需要自己添加对应的工厂并继承最高的工厂即可 使用者只需要关心如何使用对象，工厂屏蔽了对象的创建细节 抽象工厂模式 工厂方法模式只适用于简单对象，如果需要许多的产品族时会显得乏力 建造者模式 builder 如果一个对象构造时用的参数过多，可以使用builder优雅地完成构造 public class Student &#123; int id; int age; int grade; String name; String college; String profession; List&lt;String> awards; public Student(int id, int age, int grade, String name, String college, String profession, List&lt;String> awards) &#123; this.id = id; this.age = age; this.grade = grade; this.name = name; this.college = college; this.profession = profession; this.awards = awards; &#125; &#125; 参数错位，构造方法太长 public class Student &#123; ... //一律使用建造者来创建，不对外直接开放 private Student(int id, int age, int grade, String name, String college, String profession, List&lt;String> awards) &#123; ... &#125; public static StudentBuilder builder()&#123; //通过builder方法直接获取建造者 return new StudentBuilder(); &#125; public static class StudentBuilder&#123; //这里就直接创建一个内部类 //Builder也需要将所有的参数都进行暂时保存，所以Student怎么定义的这里就怎么定义 int id; int age; int grade; String name; String college; String profession; List&lt;String> awards; public StudentBuilder id(int id)&#123; //直接调用建造者对应的方法，为对应的属性赋值 this.id = id; return this; //为了支持链式调用，这里直接返回建造者本身，下同 &#125; public StudentBuilder age(int age)&#123; this.age = age; return this; &#125; ... public StudentBuilder awards(String... awards)&#123; this.awards = Arrays.asList(awards); return this; &#125; public Student build()&#123; //最后我们只需要调用建造者提供的build方法即可根据我们的配置返回一个对象 return new Student(id, age, grade, name, college, profession, awards); &#125; &#125; &#125; 最后可以使用链式调用完成对象的创建 public static void main(String[] args) &#123; Student student = Student.builder() //获取建造者 .id(1) //逐步配置各个参数 .age(18) .grade(3) .name(\"小明\") .awards(\"ICPC-ACM 区域赛 金牌\", \"LPL 2022春季赛 冠军\") .build(); //最后直接建造我们想要的对象 &#125; 还包括 协调者模式 单例模式 一个类始终只有一个实例对象/直接使用类的静态方法 饿汉式单例 类加载时将对象创建好 懒汉式单例 延迟加载，需要时才创建 线程不安全 多线程不能保证只创建一次 在创建实例的方法上加锁synchronized（但高并发效率较低） 可以在判断实例是否创建的if语句内加锁 volatile保证线程可见 懒汉式+饿汉式 在懒汉式的内部创建内部类 由静态内部类持有单例对象，根据类加载特性，仅使用外层类时，不会对静态内部类进行初始化 原型模式 原型对象作为模板，通过复制该对象来创建新对象 浅拷贝 基本数据类型将值赋值，引用类型只复制地址，指向的还是原来的对象 深拷贝 拷贝为一个全新的对象 Java拷贝机制 Cloneable接口 重写clone方法 内层对象依然是引用的复制，用==进行地址比较 应该在clone中处理成员变量","categories":[{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/categories/SE/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/tags/SE/"},{"name":"Java","slug":"Java","permalink":"https://andrew-rey.github.io/tags/Java/"},{"name":"Disgn Pattern","slug":"Disgn-Pattern","permalink":"https://andrew-rey.github.io/tags/Disgn-Pattern/"}],"author":"Andrew-Rey"},{"title":"Design Pattern | OOD","slug":"SE/design-pattern-1","date":"2023-10-14T14:51:29.000Z","updated":"2023-11-12T13:00:19.014Z","comments":true,"path":"2023/10/14/SE/design-pattern-1/","link":"","permalink":"https://andrew-rey.github.io/2023/10/14/SE/design-pattern-1/","excerpt":"项目的可维护性和可复用性，项目规范，团队开发","text":"项目的可维护性和可复用性，项目规范，团队开发 单一职责原则 一个对象应该只包含单一职责，并且该职责被完整地封装在一个类中 - 用于控制类的粒度大小 class People(object): def coding(): pass def riding(): pass def cooking(): pass ... 臃肿：修改任意行为都会修改People类 People拥有 不止一个引起它变化的原因 class Programmer(object): pass class Rider(object): pass class Chef(object): pass 类的 粒度更细，根据不同业务划分，采用单一职责原则，高内聚低耦合 开闭原则 软件实体应当对扩展开放，对修改关闭 - 扩展开放：对提供方而言 - 修改关闭：对调用方而言 from abc import ABCMeta, abstractmethod # define abstract class or interface class IProgrammer(metaclass=ABCMeta): @abstractmethod def coding(): pass class JavaProgrammer(IProgrammer): # override def coding(): java() class PythonProgrammer(IProgrammer): # override def coding(): python() 将不同程序员写代码抽象为一个统一的接口/抽象类，不进行实现 扩展开放：开放了统一的接口coding，不同程序员自由决定如何编程 修改关闭：具体程序员如何重写coding是自己负责，不受别人干扰 里氏替换原则 所有引用基类的地方必须能透明使用其子类的对象（子类能扩展父类的功能） - 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法（既然是继承于父类，子类已经不具有父类的原本行为了，如果有这样的需求，一是可以取消继承，二是可以重新写一个方法） - 子类可以增加自己特有的方法 - 当子类方法重载父类方法时，方法的前置条件（入参）要比父类更加宽松 - 当子类方法重载父类方法时，方法的后置条件（返回值）要比父类更加严格或一样 依赖倒转原则 高层模块不依赖于底层模块，它们都应该依赖抽线。抽象不应依赖于细节，细节应该依赖于抽象 - Spring框架 class UserMapper(object): # use CRUD pass class UserService(object): # use methods in UserMapper user_mapper = UserMapper() class UserController(object): # use methods in UserService user_service = UserService() 上述代码结构好，但是如果底层逻辑（例如UserService）发生改变，则上层逻辑也要跟着改变，层次分明但 耦合度高 from abc import ABCMeta, abstractmethod class UserMapper(metaclass=ABCMeta): # provides abstract interfaces only pass class UserMapperImpl(UserMapper): # implements interfaces of UserMapper pass class UserService(metaclass=ABCMeta): # provides abstract interfaces only pass class UserServiceImpl(UserService): # use dependency injection to instantiate UserMapper such as: self.user_mapper = None def set_user_mapper(user_mapper: UserMapper): pass class UserController(): # use dependency injection to instantiate UserService such as: self.user_service = None def set_user_service(user_service: UserService): pass 如果修改了底层逻辑（例如UserService），只需要修改其实现类（UserServiceImpl）即可，上层逻辑由依赖注入完成 上层只需要知道接口中定义了什么方法然后使用即可，具体操作内容由接口的实现类完成 使用了 依赖注入 降低了类之间的耦合度 接口隔离原则 客户端不应依赖那些它不需要的接口 - 对接口的细化 - 定义接口时要注意接口的粒度 合成复用原则 优先使用对象组合，而不是通过继承来达到复用的目的 - 核心是委派 - 在一个新的对象里面使用一些已有的对象，使之成为新对象的一部分，新的对象通过向这些对象的委派达到已有功能复用的目的 继承可以实现复用，但 耦合度高 子类得到父类细节（某些字段或方法），不安全 组合实现复用 考虑函数传参，用的时候才传参 考虑成为成员变量，构造时指定 迪米特法则/最少知识原则 每一个软件单位对其它单位都只有最少的知识，而且局限于那些与本单位密切相关的软件单位 - 一个模块与其它模块交互越少越好 - 降低耦合度 def main_task(): my_secret = Secret() test = Test() # test_start may need one of the property of Secret test.test_start(my_secret) # in Test def test_start(my_secret: Secret): print(my_secret.sth) test_start参数改为sth: Something减少与其它功能模块的交互","categories":[{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/categories/SE/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/tags/SE/"},{"name":"Java","slug":"Java","permalink":"https://andrew-rey.github.io/tags/Java/"},{"name":"Design Pattern","slug":"Design-Pattern","permalink":"https://andrew-rey.github.io/tags/Design-Pattern/"}],"author":"Andrew-Rey"},{"title":"Full Stack | Quick Start","slug":"SE/full-stack","date":"2023-10-12T10:51:22.000Z","updated":"2023-11-18T02:30:50.300Z","comments":true,"path":"2023/10/12/SE/full-stack/","link":"","permalink":"https://andrew-rey.github.io/2023/10/12/SE/full-stack/","excerpt":"SpringBoot + Vue development JavaEE: SpringBoot + MyBatis Plus Web front end: Vue + ElementUI","text":"SpringBoot + Vue development JavaEE: SpringBoot + MyBatis Plus Web front end: Vue + ElementUI configure environment jdk IDE: JetBrain IDEA auto build tool: maven projects auto build dependencies manegement standard development structure maven usage edit local repo (default is under user dir) (optional) configure mirrors configure self downloaded maven with IDEA 1697109448690 springboot intro framework provided by Pivotal Team convention Over Configuration(约定优于配置) embedded server(Tomcat, Jetty; no war file, just jar) simplify maven configuration pure Java springboot SSM Spring, Spring mvc, MyBatis difficult to configure adv quickly, simplely initialize Springboot IDEA Spring Initializer(maven based infact) fill project information(group id &amp; artifact id) generate project on the web, download it and load in select jdk initialize Springboot choose Maven project choose springboot version 1697114528195 initialize Springboot configure project information 1697114538516 initialize Springboot add dependencies (web app, choose spring web) 1697114557959 coding springboot backend project: receive requests from brower use Components @RestController public class DemoController&#123;&#125; coding springboot add controller member function // https://localhost:8080/demo @GetMapping(\"/demo\") public String demo()&#123; return \"hello world\"; &#125; coding springboot start the project(yes, no additional code in main) 1697116224333 hot deployment normal case: every time you modify the code, restart the project need hot-deployment spring-boot-devtools component listen the variations of classpath, trigger Restart class loader to reload the class not every change needs to restart app (static res, view templates), use spring.devtools.restart.exclude to exclude the dirs/files add dev-tools dependency in pow.xml &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-devtools&lt;/artifactId> &lt;optional>true&lt;/optional> &lt;/dependency> add properties in application.properties spring.devtools.restart.enabled = true spring.devtools.restart.additional-paths = src/main/java spring-devtools.restart.additional.exclude = static/** web development basic request methods: get request: usualy send request using address bar, only for aquiring resources; post request: commit entity to pointed resources, which changes the state or has sideffects on server, more secure (however both unsecure on the view of data transmition, using https to encrypt) entity: usually the object spring-boot-starter-web: springboot provides an integrated frame of mvc, json, tomcat webmvc: basic frame json: data parser tomcat: container dependency auto-configure when ticking spring web &lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-web&lt;/artifactId> &lt;/dependency> controller Springboot provides @Controller and @RestController tags receive &amp; handle HTTP requests @Controller tag: when requesting page and data, usually with Thymeleaf template engine (no seperation between front and back ends) @RestController tag: when requesting only data, usually converting object into json formmat in default when return MVC frame 1697121109192 controller receives users' requests, fetches data from model and reture data to view router mapping receive user requests @RequestMapping tag: URL router mapping, defines http request-mapping rule, can be added on controller class or method for class: whole router mapping will add this mapping rule for method: only the method itself properties: value: url path, supports url templates, reg-expr, @RequestMapping(\"/user\") method: http request methods (get, post), also use: @GetMapping, @PostMapping consumes: content-type for requests, e.g. application/json producer: content-type for resposes (html, json) params, headers: requests parameters and requests header parameters passing get parameters, such as http://localhost:8080/demo?nickname=zhangsan passing zhangsan to param: nickname @RequestParam tag: receiving parameters from http requests body or QueryString of url, omitted when request-parameter's name is the same with controller method @PathVariable tag: handle dynamic url, the value of url can be the parameters of controller handler @RequestBody tag: receives parameters from requestBody, handling data such as application/json, application/xml @RestController public class DemoController &#123; @RequestMapping(value=\"/demo\", method=RequestMethod.GET) public String myDemo(String nickname, String phone) &#123; return \"nickname: \" + nickname + \" phone: \" + phone; &#125; &#125; my request: http://localhost:8080/demo?nickname=xr&amp;phone=123 output: 1697123584376 now parameter mapping handles the situation where request-parameters conflict with method parameters @RestController public class DemoController &#123; @RequestMapping(value=\"/demo\", method=RequestMethod.GET) public String myDemo(@RequestParam(\"nickname\", required = false) String nickname, String phone) &#123; return \"nickname: \" + nickname + \" phone: \" + phone; &#125; &#125; static resources visit put self static resources here visit by http://localhost:8080/test.png 1697173796277 modify static path pattern: add spring.mvc.static-path-pattern=/images/** to application.properties upload files front end form modify enctype=\"multipart/form-data\", otherwise can't upload files tomcat limits the size of files by default: single file &lt; 1MB single request &lt; 10MB modify the size spring.servlet.multipart.max-file-size=10MB spring.servlet.multipart.max-request-size=10MB receiver data-type: MultipartFile receiver writing method: transferTo attention: the parameter name should be same with front end form name interceptor situation: use every function after login judge login state in ANY controller -&gt; code repeating extract to intercepter authority checking performance monitoring general behaviour: reading cookie to get user information and put it to request Springboot interface: HandlerInterceptor preHandle postHandle afterCompletion 1697177213073 how to define extend from system HandlerInterceptor override parents' method used, usually preHandle register interceptor // in com.example.demo.interceptor.LonginInterceptor public class LoginInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; System.out.println(\"LoginInterceptor\"); return true; &#125; &#125; // in com.example.demo.config.WebConfig public class WebConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; // interceptor for the path /user/** registry.addInterceptor(new LoginInterceptor()).addPathPatterns(\"/user/**\"); &#125; &#125; build RESTful serve framework rule popular website software service framework designing style basic rule, style every URI maps a kind of resource GET for acquiring resources, POST for creating/updating resources, PUT for updating resources, DELETE for deleting resources HTTP method 1697178943507 just a style, not standard requirements HTTP state code 1xx: info 2xx: success 3xx: redirection 4xx: client error 5xx: server error how to write RESTful API use tags for RESTful framework: every address corresponds a resource, so URI is suggested to be no-verb, pure nouns, and these nouns should conrresponds to database's sheet pass parameters using address: @PathVariable in id Swagger generate API document description debugging dynamically configuration: &lt;!--in pow.xml--> &lt;dependency> &lt;groupId>io.springfox&lt;/groupId> &lt;artifactId>springfox-swagger2&lt;/artifactId> &lt;version>2.9.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>io.springfox&lt;/groupId> &lt;artifactId>springfox-swagger-ui&lt;/artifactId> &lt;version>2.9.2&lt;/version> &lt;/dependency> spring.mvc.pathmatch.matching-strategy=ant_path_matcher visit: http://localhost:8080/swagger-ui.html use swagger notation(tag) add methods docs swagger tag usage swagger convinent front end developer to look up the apis another important thing: swagger page can send requests to debug the apis recommend springboot 3.0+ is recommended to use springdoc MyBatis Plus (DB configuration) ORM Object relation mapping handle OOP objects -&gt; DB storage or DB storage -&gt; OOP objects 1697182386692 MyBatis: ORM frame MyBatis-Plus: simpified MyBatis use component: mapper db-related operations: in mapper package configuration add dependencies(pow.xml) mybatis-plus mysql connection pool &lt;!-- mybatis-plus dependency--> &lt;dependency> &lt;groupId>org.mybatis.spring.boot&lt;/groupId> &lt;artifactId>mybatis-spring-boot-starter&lt;/artifactId> &lt;version>3.0.1&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>com.baomidou&lt;/groupId> &lt;artifactId>mybatis-plus-boot-starter&lt;/artifactId> &lt;version>3.5.2&lt;/version> &lt;/dependency> &lt;!-- mysql driver--> &lt;dependency> &lt;groupId>mysql&lt;/groupId> &lt;artifactId>mysql-connector-java&lt;/artifactId> &lt;version>5.1.47&lt;/version> &lt;/dependency> &lt;!-- sql connection pool--> &lt;dependency> &lt;groupId>com.alibaba&lt;/groupId> &lt;artifactId>druid-spring-boot-starter&lt;/artifactId> &lt;version>1.1.20&lt;/version> &lt;/dependency> global configuration(application.properties) spring.datasource.type=com.alibaba.druid.pool.DruidDataSource spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/mydb?userSSL=false spring.datasource.username=root spring.datasource.password=root mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl create MySQL data DB, schema and Tables 1697187931126 add tags on Activation Class (main class): @MapperScan(\"com.example.demo.mapper\"), scan mapper packages before running create mapper interface, and implemented by MyBatis (use @Mapper tag on the interface, meaning this is a component, it will use Dynamic Proxy to instanciate an object with @Autowired. And all sql sentences are finished by tags) @Mapper public interface UserMapper &#123; // add declarations, impl by Mybatis // query all users // find in user db (named mydb), configured in application.properties @Select(\"select * from user\") public List&lt;User> query(); // return type stand for the number of records be inserted @Insert(\"insert into user values (#&#123;id&#125;, #&#123;username&#125;, #&#123;password&#125;, #&#123;gender&#125;)\") public int insert(User user); &#125; for a more simply form, namely use MyBatis-Plus, that's all: attention that the class name should be the same with your database table name, unless you use tags @Mapper public interface UserMapper extends BaseMapper&lt;User> &#123;&#125; use the mapper to edit database data create controller UserController (we have created User entity before) @RestController public class UserController &#123; @Autowired private UserMapper userMapper; @GetMapping(\"/user\") public List&lt;User> query() &#123; return userMapper.query(); &#125; @PostMapping(\"/user\") public String insert(User user) &#123; var state = userMapper.insert(user); if (state &lt;= 0) &#123; return \"insert fail\"; &#125; return \"insert success\"; &#125; &#125; where @Autowired is an injection method, which helps to instanciate UserMapper Axios a frame of front-end requesting - front-end needs data, browser sends HTTP requests to server to fetch data, Vue bonds data - based on Ajax, promise - use XMLHttpRequests to send web requests, convert json data automatically npm install axios 帮助文档 send web requests send GET requests axios.get(&#39;&#x2F;user?ID&#x3D;12345&#39;) .then(function(response) &#123; &#x2F;&#x2F; handle successful situations console.log(response); &#125;) .catch(function(error) &#123; &#x2F;&#x2F; handle failures console.log(error); &#125;) .then(function() &#123; &#x2F;&#x2F; always execute &#125;); &#x2F;&#x2F; the same with: axios.get(&#39;&#x2F;user&#39;, &#123; params: &#123; ID: 12345 &#125; &#125;) .then(function(response) &#123; &#x2F;&#x2F; handle successful situations console.log(response); &#125;) .catch(function(error) &#123; &#x2F;&#x2F; handle failures console.log(error); &#125;) .then(function() &#123; &#x2F;&#x2F; always execute &#125;); send POST requests &#x2F;&#x2F; request body is the second parameter axios.post(&#39;&#x2F;user&#39;, &#123; firstName: &#39;xxx&#39;, lastName: &#39;xxx&#39; &#125;) .then(function(response) &#123; &#x2F;&#x2F; handle successful situations console.log(response); &#125;) .catch(function(error) &#123; &#x2F;&#x2F; handle failures console.log(error); &#125;) .then(function() &#123; &#x2F;&#x2F; always execute &#125;); when to send requests created:function()&#123;&#125;: invoked when creating the components mounted:function()&#123;&#125;: invoked when mounting the components cross-domain usually appears in front-back end separation projects same-origin-policy: the basic and core security function of browsers same-origin, i.e. same-domain, where two page have the same protocol, host and port cross-domain: request url appears when any of protocol, host and port is different from current page url can't read cross-domain cookies can't send Ajax requests to cross-domain addresses authorization to solve cross-domain questions CORS(cross-origin resource sharing): designed by W3C simple requests/complex requests back-end server realize CORS interfaces for single controller: @CrossOrigin for global controler: add configuration class send requests when the component is created: &#x2F;&#x2F; use arrow function: &quot;this&quot; points to parents&#39; &quot;this&quot;, able to visit parameters of parents &quot;tableData&quot; created:function()&#123; axios.get(&quot;http:&#x2F;&#x2F;localhost:8088&#x2F;user&#x2F;findAll&quot;).then((response)&#x3D;&gt;&#123; this.tableData &#x3D; response.data &#125;) &#125;, data() &#123; return &#123; tableData: [] &#125; &#125; set base url axios.defualts.baseURL = 'http://api.com' app.config.globalProperties.$http = axios JWT authentication: get resources from server after authentication session authentication user sends username and password to server server check the user information if the login info is right generate cookies, return a session id to user the session id will be sent to server again through cookie when user sends other requests server receives the session id and find out the saved data to know the user's role (or related data) drawbacks: difficult to expand especially for server-clusters. (other server has no session id of user as the user send session id to another server) token authentication a way to solve the drawbacks of session authentiocation string-like store token in client (such as cookie or localStorage) everytime user request for server should send token to server at the same time defend the user to modify the token contents Json web token a method to realize the token generate a JsonObj after the authentication and send it to user user communicates with server should send the JsonObj to server, and the server only realies on the JsonObj to distinguish the authority of user in order to avoid user modifying the token, server add signature to token when generating it 3 parts: divided by . header: meta info, will be encode(not encrypt) payload: data that need to be pass by, will be encode signature: appoint a secret-key which only know by server, use the signature algrithm defined in header and generate signature by the formula: HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret ) the token is just a string divided by .: header.payload.signature 1700222447717","categories":[{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/categories/SE/"}],"tags":[{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/tags/SE/"},{"name":"Full Stack","slug":"Full-Stack","permalink":"https://andrew-rey.github.io/tags/Full-Stack/"},{"name":"Springboot","slug":"Springboot","permalink":"https://andrew-rey.github.io/tags/Springboot/"},{"name":"Vue","slug":"Vue","permalink":"https://andrew-rey.github.io/tags/Vue/"}],"author":"Andrew-Rey"},{"title":"Project | CSW-Microservice","slug":"Project/csw-microservice","date":"2023-10-05T11:56:53.000Z","updated":"2023-11-11T12:54:24.926Z","comments":true,"path":"2023/10/05/Project/csw-microservice/","link":"","permalink":"https://andrew-rey.github.io/2023/10/05/Project/csw-microservice/","excerpt":"\"Microservice platform of Chien-Shiung Wu Colledge\"","text":"\"Microservice platform of Chien-Shiung Wu Colledge\" Requirements 功能性分析 用户权限管理 管理人员权限 普通用户权限 运维人员权限 书院通知管理 发布功能 置顶功能 招聘管理 轮播功能 静态内容展示 书院文化展示（lt最爱） 会议室预约管理 会议室预约时间展示：展示近几天所有会议室的空闲时间、占用时间 会议审核功能：查看审核协议；审核通过/拒绝用户在会议室预约的会议 会议预约功能：填写会议室、使用时间、会议主题、预约人；提供增添预约、删除预约、修改预约、查看预约，撤回预约等功能 大厅座位预约管理 大厅座位空间时间展示：展示座位空间分布 -&gt; 展示某个选中座位的预约时间 座位预约功能：预约时间、预约人 咖啡预约管理 咖啡厅餐品展示 咖啡预约功能：口味选择、预期提取时间、预约人 器材出借管理 器材管理：增删查改，器材描述 器材出借功能：出借人、出借/归还时间、审批人 问题上报管理 问题收集管理：增删查改 问题审批功能：审批人、问题描述、上报人（可选） 操作日志管理 日志查看功能 个人账号管理 个人信息维护 密码管理（修改密码，找回密码） 个人权限维护 个人诚信点管理 非功能性分析 用户界面的具体细节 参照东大信息化（？） 有吴健雄学院的特色 对稳定性要求要高 旧的书院系统容易崩溃 响应时间要尽量快 实际中会经常面对临时预约会议的情况，此时需要快速响应 不存在网上交易平台，但仍然需要保证用户权限的分离和安全性 设计约束 进度约束：本学期结束前有测试版，争取寒假能上线正式版 初步讨论使用maven进行项目管理，Springboot+Vue技术栈（相关版本未定） 相关use case图 预约模块 1696654551214 个人信息管理 1696654600025 问题反馈 1696654633120 浏览模块 1696654672936 1696654662518","categories":[{"name":"Project","slug":"Project","permalink":"https://andrew-rey.github.io/categories/Project/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"Project","slug":"Project","permalink":"https://andrew-rey.github.io/tags/Project/"},{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/tags/SE/"},{"name":"Microservice","slug":"Microservice","permalink":"https://andrew-rey.github.io/tags/Microservice/"}],"author":"Andrew-Rey"},{"title":"Inverse Analysis | Requirements Analysis","slug":"SE/reverse-analysis-0","date":"2023-10-05T07:03:25.000Z","updated":"2023-11-11T12:53:04.717Z","comments":true,"path":"2023/10/05/SE/reverse-analysis-0/","link":"","permalink":"https://andrew-rey.github.io/2023/10/05/SE/reverse-analysis-0/","excerpt":"\"项目负责人对用户需求的理解程度，在很大程度上决定了项目的成败\"","text":"\"项目负责人对用户需求的理解程度，在很大程度上决定了项目的成败\" 需求分析的目的 更好地了解、分析、明确用户需求，并能够准确清晰地以文档形式表达给参与项目开发的每个成员，保证项目开发按照用户需求的方向进行。 需求分析的物质性结果是 软件功能描述书（此外，一般还需要编写用户调查报告和市场调研报告） 需求分析的内容 功能性分析 必须实现哪些功能 向用户提供功能时需要执行的动作 形成软件需求规格说明书 非功能性分析 用户界面具体细节 软件性能、可靠性、响应时间需求 运行环境需求 相关标准、规范 安全需求 架构需求 未来可能的扩充需求 设计约束 进度约束 预算约束 资源约束 其它 需求分析的人员分工 项目管理者：组织 人员与用户进行交流，组织 人员编写项目功能描述书 开发人员：与用户一起进行需求分析 美术和技术骨干代表或者全体成员（与用户讨论）编写项目的功能描述书初稿 相关人员对功能书的初稿进行修改和完善，形成正式文档 用户若有能力，可以参与至功能描述书的编写和修改中 用户调查报告 用户的充分配合，必要时需要对用户进行培训。 调查的形式：发需求调查表、开需求调查座谈会或者现场调研 调查内容 网站当前以及日后可能出现的功能需求 客户对网站的性能(如访问速度)的要求和可靠性的要求 确定网站维护的要求 网站的实际运行环境 网站页面总体风格以及美工效果(必要的时候用户可以提供参考站点或者由公司向用户提供) 主页面和次级页面数量，是否需要多种语言版本等 内容管理及录入任务的分配 各种页面特殊效果及其数量(js，flash等) 项目完成时间及进度(可以根据合同) 明确项目完成后的维护责任 调查报告的重点内容 调查概要说明：网站项目的名称;用户单位;参与调查人员;调查开始终止的时间;调查的工作安排。 调查内容说明：用户的基本情况;用户的主要业务;信息化建设现状;网站当前和将来潜在的功能需求、性能需求、可靠性需求、实际运行环境;用户对新网站的期望等。 调查资料汇编：将调查得到的资料分类汇总(如调查问卷，会议记录等等) 市场调研报告 市场调研的目的：清晰地分析相似网站的性能和运行情况，帮助项目负责人清楚地构想出自己开发的网站的大体架构和模样，总结同类网站优势和缺点；明确并引导用户需求 应尽可能调研到所有比较出名或优秀的同类网站，了解同类网站的使用环境和用户的诧异点。 调研内容 市场中同类网站作品的确定。 调研作品的使用范围和访问人群。 调研产品的功能设计(主要模块构成，特色功能，性能情况等等) 简单评价所调研的网站情况。 调研报告的重点内容 调研概要说明：调研计划;网站项目名称、调研单位、参与调研、调研开始终止时间。 调研内容说明：调研的同类网站作品名称、网址、设计公司、网站相关说明、开发背景、主要适用访问对象、功能描述、评价等项目管理者联盟 可采用借鉴的调研网站的功能设计：功能描述、用户界面、性能需求、可采用的原因。 不可采用借鉴的调研网站的功能设计：功能描述、用户界面、性能需求、不可采用的原因。 分析同类网站作品和主要竞争对手产品的弱点和缺陷以及本公司产品在这些方面的优势。 调研资料汇编：将调研得到的资料进行分类汇总。 软件功能描述书 描述书的重点内容 网站功能 网站用户界面(初步) 网站运行的软硬件环境 网站系统性能定义 网站系统的软件和硬件接口 确定网站维护的要求 确定网站系统空间租赁要求 网站页面总体风格及美工效果。 主页面及次页面大概数量。 管理及内容录入任务分配。 各种页面特殊效果及其数量。 项目完成时间及进度(根据合同) 明确项目完成后的维护责任。","categories":[{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/categories/SE/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/tags/SE/"},{"name":"reverse analysis","slug":"reverse-analysis","permalink":"https://andrew-rey.github.io/tags/reverse-analysis/"}],"author":"Andrew-Rey"},{"title":"PCG | Perlin Noise","slug":"Math/PerlinNoise","date":"2023-09-20T16:25:23.000Z","updated":"2023-11-18T06:12:59.495Z","comments":true,"path":"2023/09/21/Math/PerlinNoise/","link":"","permalink":"https://andrew-rey.github.io/2023/09/21/Math/PerlinNoise/","excerpt":"“程序化内容生成——地形/材质等的程序化生成” ——关于一些剑走偏锋的项目历程","text":"“程序化内容生成——地形/材质等的程序化生成” ——关于一些剑走偏锋的项目历程 Perlin Noise about the Perlin noise: ref 4 types of noise that are similar and that are often confused with one another classic Perlin noise improved Perlin noise simplex noise valuse noise about the range of Perlin noise: ref // mapping a 2D position to a random number range from -1 to 1 var perlinValue = PerlinNoise(float x, float y); for texture: x, y stand for pixel position, but multiplied by a small number called the frequency Parameters frequency of 2D waves amplitude of 2D waves octaves: the amount of waves to be generated persistence: amount of change in size between one curve and the next offset: provide variation in the output height scale: scaling factor to accentuate the output generated Property if 2 inputs are near to each other, the results of the noise function will be near to each other too. guarantee continuity Generation given a 2D grid as following, the input of Perlin noise is each pixel. 1695262810353 assign each gird point a random constant vector. (note: gridVector[4]) get the vectors pointing from the grid point to the input point(target pixel). (note: inputVector[4]) for each of the 4 corners of the square where the target pixel lies, calculate the dot products: for i in range(4): calculate dot(gridVector[i], inputVector[i]) the dot product means the effects corners value to target pixels 1695264135606 interpolate between those 4 values and the result is the value of the target pixel. difference between Perlin noise and value noise: Perlin noise use dot product between 2 vectors to get 4 corners' values while value noise use a pseudo-random number. Discussion gradient constant vectors why we need permutation table(noted as P) &amp; gradient table(noted as G): P is used to select a random gradient from G. P provides randomness and repeatability(???)4 how to generate a permutation table: the core is double and shuffle. we have known that permutation table is used to select a gradient from gradient table and one gradient is defined by (x,y) (which is the grid point position). so one tuple (x,y) defines one permutation value. so the size of permutation table is (double). to guarantee the randomness, we can do shuffle for . the code to generate permutation table is (where ): var permutationTable = new int[2 * len(X)]; for (var i = 0; i &lt; len(X); i += 1) permutationTable[i] = i; permutationTable = Shuffle(permutationTable); for (var i = 0; i &lt; len(X); i += 1) permutationTable[len(X) + i] = permutationTable[i]; // visit the table given (x,y) var valueTopRight = P[P[x+1]+y+1]; var valueTopLeft = P[P[x]+y+1]; var valueBottomRight = P[P[x+1]+y]; var valueBottomLeft = P[P[x]+y]; how to generate a gradient table: use 4 constant vectors: . so just do modulo with the permutation value given (x,y) can get one gradient vector. Vector2 GetConstantVector(v) { switch v % 4: case 0: return Vector2(1f,1f); case 1: return Vector2(1f,-1f); case 2: return Vector2(-1f,1f); case 3: return Vector2(-1f,-1f); default: throw undefined error; } interpolation how to interpotate between such 4 values: 4 values (a1,a2,b1,b2), firstly interpolate between a1 and a2 which produces v1, secondly interpolate between b1 and b2 which produces v2, finally interpolate v1 and v2 which produces v, the interpolated value. which interpolation function should be used: if we use linear interplation to get our in , there will be a \"hard transition\" between 3 points (x=0,1,2, while y=2,0,1.5) 1695314437553 but if we use an unlinear method, it will be smoothed 1695314441924 the normally used interpolation function is , the image is: 1695314653794 frequency what dose frequency means in Perlin noise: consider this situation: what is the interpolate value when our target pixel happens to be the bottom left grid point? ZERO. because the inputVector is zero and thus all dot products are zero. to solve ths issue, we generallt multiply the inputs target pixel by a small value called frequency. amplitude what dose amplitude means in Perlin noise: this will be used in following section. amplitude is the multiplier before one item. octave what dose octave means in Perlin noise: this will also be used in following section. when one layer has a frequency that is double the frequency of the previous layer, this layer is called an octave. More? FBM: Fractal brownian motion 1695315613281 obviously, the left is better. the left image uses FBM to simulate the terrains in real world. but...how? 1695317070446 so the high frequencies and low amplitudes generate more details than just one single layer, we can keep changing the frequencies and amplitudes in a for-loop, and add them together.","categories":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/categories/Math/"}],"tags":[{"name":"Unity","slug":"Unity","permalink":"https://andrew-rey.github.io/tags/Unity/"},{"name":"PCG","slug":"PCG","permalink":"https://andrew-rey.github.io/tags/PCG/"},{"name":"Terrain","slug":"Terrain","permalink":"https://andrew-rey.github.io/tags/Terrain/"}]},{"title":"C++ Programming | class template","slug":"CS/class-template","date":"2023-02-28T12:21:43.000Z","updated":"2023-11-11T12:50:23.626Z","comments":true,"path":"2023/02/28/CS/class-template/","link":"","permalink":"https://andrew-rey.github.io/2023/02/28/CS/class-template/","excerpt":"\"C++17类模板\"","text":"\"C++17类模板\" 编译器用于创建类的模板: 自动生成类 标准库 类模板不是类, 是创建类的一种方式 实例 编译器从类模板中生成的类, 在第一次使用模板类型声明变量是, 会创建类模板的一个实例, 以后定义同类型的变量时, 会使用已创建的第一个实例. 在创建类模板时, 也可以不同时声明变量. 数据的组织 独立于 对象类型 类模板的定义 template&lt;typename T1, typename T2, Type Arg1&gt; class ClassName { // template class definition }; 模板参数 类型参数 typename 实参总是类型: int, float... 非类型参数 Type 实参是整数类型的字面量: 200, 10... 整数常量表达式 指向对象的指针或引用, 函数指针或空指针 模板 实参是类模板的一个实例 在模板定义中, 不需要使用完整的ID, 例如构造函数 ClassName&lt;T1&gt;();可以写成ClassName(); 不过在模板体的外部标识模板, 则必须使用模板ID (即在模板类外定义模板中的成员函数时需要显式写出ID) 一个例子 template&lt;typename T1&gt; class PythonList { private: int len_; int size_; T1* elements_; public: explicit PythonList&lt;T1&gt;(size_t list_len); PythonList&lt;T1&gt;(const PythonList&lt;T1&gt;&amp; python_list); ~PythonList(); T1&amp; operator[](size_t index); const T1&amp; operator[](size_t index) const; PythonList&lt;T1&gt;&amp; operator=(const PythonList&lt;T1&gt;&amp; rhs_list); size_t get_len() const { return len_; } void allocate_double(); }; 类模板成员函数的定义 若在模板类的内部定义, 实则为 内联 如何理解该语法 类模板的成员函数的外部定义本身就是函数模板, 即使成员函数不依赖类型参数. 若函数没有在类内定义, 则它需要一个模板定义. 定义函数模板中的参数列表必须与类模板参数列表相同. 例如 // 析构函数 template &lt;typename T1&gt; PythonList&lt;T1&gt;::~PythonList&lt;T1&gt;() { delete [] elements_; } // 构造函数 template &lt;typename T1&gt; PythonList&lt;T1&gt;::PythonList(size_t list_len) : len_(list_len), size_(FOLD * list_len), elements_(new T1(list_len)) {} template &lt;typename T1&gt; PythonList&lt;T1&gt;::PythonList(const PythonList&lt;T1&gt; &amp;python_list) : PythonList{python_list.len_} { for (size_t i{}; i &lt; len_; ++i) { elements_[i] = python_list.elements_[i]; } } // 下标运算符 template &lt;typename T1&gt; T1 &amp;PythonList&lt;T1&gt;::operator[](size_t index) { if (index &gt;= len_) { throw std::out_of_range{\"Index out of range: \" + std::to_string(index)}; } return elements_[index]; } template &lt;typename T1&gt; const T1 &amp;PythonList&lt;T1&gt;::operator[](size_t index) const { if (index &gt;= len_) { throw std::out_of_range{\"Index out of range: \" + std::to_string(index)}; } return elements_[index]; } // 赋值运算符 template &lt;typename T1&gt; PythonList&lt;T1&gt; PythonList&lt;T1&gt;::operator=(const PythonList&lt;T1&gt;&amp; rhs_list) { if (&amp;rhs_list != this) { delete [] elements_; len_ = rhs_list.len_; size_ = rhs_list.size_; elements_ = new T1[len_]; for (size_t i {}; i &lt; size_; ++i) { elements_[i] = rhs_list.elements_[i]; } } return *this; } 第一行说明该函数为模板函数; 在限定成员函数时, 作用域需要带上模板ID 有时候需要提供自己的拷贝构造(或析构), 因为涉及到动态内存分配时, 默认拷贝构造(或析构)有可能会出现负面效应 在赋值重载时, 需要 检查左右操作数是否相等, 否则会释放this指向的对象后再进行复制. 代码重复 在上述的定义中, const的重载和非const的重载模板函数代码重复, 代码重复不利于后续的维护 对抗重复的方法: 函数, 模板, 基类 传统方法: 用const实现非const template &lt;typename T1&gt; T1&amp; PythonList&lt;T1&gt;::operator[](size_t index) { return const_cast&lt;T1&amp;&gt;(static_cast&lt;const PythonList&lt;T1&gt;&amp;&gt;(*this) [index]); } C++17: std::as_const()(utility头文件) template &lt;typename T1&gt; T1&amp; PythonList&lt;T1&gt;::operator[](size_t index) { return const_cast&lt;T1&amp;&gt;(std::as_const(*this)[index]); } 异常安全性 在赋值运算符重载的时候, 由于使用了new, 可能会出现std::bad_alloc异常 在elements_[i] = rhs_list.elements_[i];可能会出现关于类型T1的赋值异常 当声明了noexcept后, 表示代码内部不发生异常, 使得编译器能做更多的优化, 例如大部分析构都隐式声明了noexcept cppreference noexcept 在以上的赋值运算符中使用 复制后交换 定义模板类注意 成员函数模板与类模板的定义放在同一个文件中: 当编译器生成类模板时, 需要去使用函数模板, 所以在使用模板的源文件中, 这些成员函数的定义必须可用. 类模板实例化 PythonList&lt;double&gt; data {10}; 编译器只编译程序使用的成员函数, 不会为某个模板参数的实例而一次性编译整个类: 例如上述代码编译后的类中只有构造函数和析构函数. 声明对象类型的指针 不会 创建模板实例: PythonList&lt;std::string&gt;* data_p; 非类型的类模板参数 主要用于定义指定容器有效的值, 如数组的维数 非类型参数只能是整数类型 (size_t, long), 枚举类型, 对象的指针或引用, 函数的指针或引用, 类成员的指针 当作常量 template&lt;typename T1, size_t size&gt; class ClassName { // definition }; // 还有一些比较无语的 template&lt;typename T1, T1 value&gt; ... // 此时T1只能是模板的非类型参数所允许的类型 注意 只有模板参数完全相同的情况下, 编译器才不会再次编译模板类; 任意一个不同, 编译器都会认为是不同的类, 后果是代码膨胀 解决方法 (待定) 模板参数的默认值 与函数的默认参数类似 如果某个模板参数有默认值, 则后续的参数也必须有默认值 如果某个模板参数的实参被省略, 则后续的所有实参也必须省略 不需要在成员函数的模板中指定默认值 template&lt;typename T1 = int, int value = 10&gt; ... 模板的显式实例化 template class ClassName&lt;T1, 10, ...&gt; 编译器会从模板中实例化所有的成员函数, 无论是否调用 类模板特化 模板的使用中有时候只对 某些类型 有用, 而不支持其他类型; 因此使用 特化 来处理某些特殊情况. 例如整型变量的相等和浮点型的比较并不相同, 这时可以使用模板的特化来处理. 对于类模板中的成员函数: 如果成员函数是在类模板的外部定义的, 而不是在类模板体中定义的, 则可以提供函数模板的特化 全特化 即规定模板实现的所有模板参数 template&lt;&gt; class PythonList&lt;const char*&gt;{}; 特化的定义必须放在原有的定义或声明后面. 因为指定了所有参数, 所以是 全特化 偏特化 即只规定模板参数列表中的一部分模板参数 template&lt;Type value&gt; class PythonList&lt;const char*, value&gt; {}; template后的参数列表包含的是为这个模板特化的实例所指定的参数, 即实例化时需要指定value 模板名后面的尖括号指定原有类模板定义中的参数如何特化. 该参数列表必须与原来未特化的类模板个数相同 指针类型的偏特化 例如下面代码: template的第一个参数仍是T1, 但模板名后面可以跟着T* template&lt;typename T1, Type value&gt; class ClassName&lt;T1*, value&gt; {}; 特化的选择 当匹配给定特化的每个实参匹配多个特化时, 编译器会选择 最特殊 的一个特化. 特殊是指有多个匹配, 如果符合A特化, 也符合B特化, 但反过来不行时, 则A比B更特殊 (A含于B) 在类模板中使用static_assert() static_assert()接受两个参数, 第一个参数为false时, 输出第二个参数指定的消息. 第一个实参使用type_traits.h中的模板 type_traits 类模板的友元 对于友元函数和友元类的情况与一般情况相同 模板友元 类模板的参数列表一般包含定义友元模板的所有参数 如果类模板的一些参数在友元模板中没有, 则友元模板的实例会用于类模板的几个实例 普通类若有友元模板, 则友元的每一个实例都是这个类的友元","categories":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/categories/CS/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"C++","slug":"C","permalink":"https://andrew-rey.github.io/tags/C/"}],"author":"Andrew-Rey"},{"title":"DeepLearning | Image Semantic Segmentation based on UNet","slug":"ML/UNet","date":"2022-08-21T02:50:30.000Z","updated":"2023-11-11T12:52:26.884Z","comments":true,"path":"2022/08/21/ML/UNet/","link":"","permalink":"https://andrew-rey.github.io/2022/08/21/ML/UNet/","excerpt":"\"Semantic segmentation of images, use UNet model.\"","text":"\"Semantic segmentation of images, use UNet model.\" Abstract In this project, we realize an basic UNet model and UNet++ model, then we apply them on image semantic segmentation. We show our basic theory of UNet and an improvement of it, and we provide main code of this program. Finally, we give the result of segmentation images, loss-curve and accuracy-curve on both training and validation set. The copyright of this program is owned by our team mentioned on the end of this blog. UNet Structure The paper published in 2015 propose a noval network structure, whose shape is similar with the captal \"U\". The idea comes from FCNN. U-Net is one of the classes of \"Encoder-Decoder\" structure. U-Net Structure The front half of the network is \"encoder\". The input image passes covolutional kernel, and then passes the pooling layer (or other dimension-decreasing layer). The opposite of that is the back part of UNet, the \"decoder\". The input of decoder is a sequence of feature maps with highly contracted pixels. The output of the decoder (or the whole network) is an image with the same shape of input image, where each pixel has its own class. In this project, we decrease the number of convolutional layers so that there are only two convolutional layers in each convolutional kernel as the dataset includes images with shape . Operator Definitions Convolutional Kernel: We define the basic convolutional kernel as follow: self.layer = nn.Sequential( # in_channel, out_channel, kernel_size, stride, padding # batch size * channel * height * weight nn.Conv2d(C_in, C_out, kernel_size=(3, 3), stride=(1, 1), padding=1), # 64 64 128 256 nn.BatchNorm2d(C_out), nn.Dropout(0.2), nn.LeakyReLU(), nn.Conv2d(C_out, C_out, kernel_size=(3, 3), stride=(1, 1), padding=1), # 64 64 128 256 nn.BatchNorm2d(C_out), nn.Dropout(0.5), nn.LeakyReLU(), It includes two convolution operations. Down Sampling Kernel: As for downsampling kernel, we replace conditional pooling layer to convolutional layer with stride equaling to 2, which means the shape will be shrunk to while remaining the same channels. self.Down = nn.Sequential( nn.Conv2d(C, C, kernel_size=(3, 3), stride=(2, 2), padding=1), # 64 64 64 128 nn.LeakyReLU() ) Up Sampling Kernel: The basic structure of up-sampling contains only one convolutional layer with convolutional kernel size and half out-channel. The feature map should pass an interpolation layer before getting into the convolutional layer. def __init__(self, C): super(UpSampling, self).__init__() # out-channel = 1/2 in-channel self.Up = nn.Conv2d(C, C // 2, kernel_size=(1, 1), stride=(1, 1)) def forward(self, x, r): # neighbor interpolation up = F.interpolate(x, scale_factor=2, mode=\"nearest\") x = self.Up(up) # concatenate the feature map in encoder and # the feature map in corrsponding decoder layer, in channel dimension res = torch.cat((x, r), 1) return res The interpolation mode we choose is \"nearest\". The function torch.cat(dim=1) is used to concatenate two feature maps in channel dimension. Network Definition Based on the operators defined above, we link these blocks together like UNet structure. def __init__(self): super(UNet, self).__init__() # down sampling self.C1 = Conv(3, 64) self.D1 = DownSampling(64) self.C2 = Conv(64, 128) self.D2 = DownSampling(128) self.C3 = Conv(128, 256) self.D3 = DownSampling(256) self.C4 = Conv(256, 512) self.D4 = DownSampling(512) self.C5 = Conv(512, 1024) # up sampling self.U1 = UpSampling(1024) self.C6 = Conv(1024, 512) self.U2 = UpSampling(512) self.C7 = Conv(512, 256) self.U3 = UpSampling(256) self.C8 = Conv(256, 128) self.U4 = UpSampling(128) self.C9 = Conv(128, 64) self.C10 = torch.nn.Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=1) self.pred = torch.nn.Conv2d(3, 34, kernel_size=(1, 1), stride=(1, 1)) self.Th = torch.nn.Sigmoid() Like U-Net mentioned in that paper, we designed 4 layer deep network. def forward(self, x): # part 1: down sampling, decreasing dimension R1 = self.C1(x) R2 = self.C2(self.D1(R1)) R3 = self.C3(self.D2(R2)) R4 = self.C4(self.D3(R3)) Y1 = self.C5(self.D4(R4)) # part 2: up sampling, connect priori knowledge O1 = self.C6(self.U1(Y1, R4)) O2 = self.C7(self.U2(O1, R3)) O3 = self.C8(self.U3(O2, R2)) O4 = self.C9(self.U4(O3, R1)) # part 3: active function return self.Th(self.pred(self.C10(O4))) As you can see, the difference between U-Net and other networks before U-Net is that U-Net conbines the former information from encoder and current information from decoder. Code During the training process, we want to keep some information of loss values and accuracy values on training set and validation set so that we can analyze the variance. In the function named train(), we take optimizer and loss as two parameters used in training process. The outputs of this function are loss and accuracy on both training set and validation set. If we get the data about training set and validation set, we can draw the curves. If both training and validation loss values decrease during training process, we can conclude that our model converges and does not overfit on training set. The training code is shown as follow: self.model.train() for batch in self.train_loader: batch_num += 1 optimizer.zero_grad() rgbs, segs = batch s, _, m, n = segs.shape segs = torch.reshape(segs, (s, m, n)) pred_segs = self.model(rgbs).to(self.device) loss_val = loss(pred_segs, segs) loss_val.backward() optimizer.step() The data collecting code can be written as follow: Statistic data of training set for ... : with torch.no_grad(): if batch_num % 5 == 0: logging.info(f\"batch num {batch_num}, loss {loss_val}\") # delete or add comments when needed train_loss += loss_val # statistic valid classified samples total_pix += s * m * n idx = torch.argmax(pred_segs, dim=1) train_valid_pix += torch.eq(idx, segs).sum().float().item() torch.cuda.empty_cache() epoch_acc = train_valid_pix / total_pix train_epoch_loss.append(train_loss / batch_num) train_epoch_acc.append(epoch_acc) Statistic data of validation set self.model.eval() with torch.no_grad(): for valid_batch in self.valid_loader: valid_batch_num += 1 rgbs, segs = valid_batch s, _, m, n = segs.shape segs = torch.reshape(segs, (s, m, n)) pred_segs = self.model(rgbs).to(self.device) loss_val = loss(pred_segs, segs) valid_loss += loss_val valid_total_pix += s * m * n idx = torch.argmax(pred_segs, dim=1) valid_valid_pix += torch.eq(idx, segs).sum().float().item() epoch_acc = valid_valid_pix / valid_total_pix valid_epoch_loss.append(valid_loss / valid_batch_num) valid_epoch_acc.append(epoch_acc) The point you should pay attention to is that you should use with torch.no_grad() before you do some work that have no relation with training process, otherwise your GPU memory will be full or even overflow. Result After a long time training, we get the satisfying result with U-Net model. Former Model The \"former model\" infers the U-Net model, and you will see we use other upgraded model named \"UNet++\" which will be introduced later. We output the segmentation results and their uncertainties. picture 1 result-UNet Model Upgrade For some reasons, we try another U-Net-like model, Nested UNet, namely UNet++. It has a nested convolutional blocks like a pyramid and there is a chain passing connectivity between each convolutional block every layer. Neseted UNet The black nodes are the same with U-Net model. The green nodes are what Nested UNet newly added. Both green and blue lines are skip pathways that pass connectivities from encoder to decoder. The use of Nested UNet gives us a little improvement on final results. Analysis U-Net We analyze the loss value and accuracy on both training and validation set: unet loss We find that after 100 epochs, the model has not convergenced yet, but the loss on validation decreases to the bottom. unet accuracy From the accuracy curves, we find that both training set and validation set have increasing accuracy, which means our model does not overfit. Nested UNet Meanwhile, we analyze the loss and accuracy of Nested UNet model on both training and validation set. nested loss We find that Nested UNet has a faster convergency speed than UNet. It uses only about 60 epochs. But to our surprise, we find that Neseted UNet overfit after about only 20 epochs because the validation loss does not decrease anymore. nested accuracy The performance on validation accuracy stays the same with UNet model.","categories":[{"name":"ML","slug":"ML","permalink":"https://andrew-rey.github.io/categories/ML/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://andrew-rey.github.io/tags/Deep-Learning/"}]},{"title":"C++ Programming | CMake Tutorial","slug":"CS/cmake","date":"2022-08-10T04:56:16.000Z","updated":"2023-11-11T12:50:38.840Z","comments":true,"path":"2022/08/10/CS/cmake/","link":"","permalink":"https://andrew-rey.github.io/2022/08/10/CS/cmake/","excerpt":"CMake version: 3.x","text":"CMake version: 3.x Command Line # (configure step) create build dir, and generate build/Makefile -> generate Makefile cmake -B build # (build step) invoke building system and build the project in different OS -> generate executable file cmake --build build -j4 # invoke building system to execute target \"install\" cmake --build build --target install # define configure variables, only use in configure step # use -D # set build type in configure step, the value will remain when invoked the second time unless delete build dir cmake -B build -DCMAKE_BUILD_TYPE=Release # Specify generator (generator: generate build system build rule from CMakeLists.txt) # use -G # generator Ninja, faster than Unix Makefile, generate *.ninja cmake -B build -G Ninja CMakeLists.txt add source file (1). single file: main.cpp add_executable(main main.cpp) or add_executable(main) target_sources(main PUBLIC main.cpp) (2). multiple files: main.cpp | other.cpp | other.h add_executable(main) target_sources(main PUBLIC main.cpp other.cpp) or set a new variable add_executable(main) set(sources main.cpp other.cpp other.h) # other.h can delete target_sources(main PUBLIC $&#123;sources&#125;) or use GLOB to search all files in current dir add_executable(main) file(GLOB sources CONFIGURE_DEPENDS *.cpp *.h) # add CONFIGURE_DEPENDS to detect any change when next build target_sources(main PUBLIC $&#123;sources&#125;) when we have a dir structure: mylib +----*.cpp +----*.h *.cpp *.h no need to write all files: # add all file in current dir and mylib dir add_executable(main) aux_source_directory(. sources) aux_source_directory(mylib sources) target_sources(main PUBLIC $&#123;sources&#125;) or use GLOB_RECURSE to find all files recursely: add_executable(main) file(GLOB_RECURSE sources CONFIGURE_DEPENDS *.cpp *.h) target_sources(main PUBLIC $&#123;sources&#125;) ERROR: use GLOB_RECURSE will include *.cpp files in build dir. solution: Add all source files in a dir named src Configure variables CMAKE_BUILD_TYPE: type of build, Release, Debug, MinSizeRel and RelWithDebInfo, defualt: none (debug). set(CMAKE_BUILD_TYPE Release) set default build type as Release to reach high performance: in the first three lines: if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Release) endif() # Specify version of cmake cmake_minimum_required(VERSION 3.22) # set c++ standard # don't modify CMAKE_CXX_FLAGS to add -std=c++17 set(CMAKE_CXX_STANDARD 17) # if use the needed CXX standard defined. set(CMAKE_CXX_STANDARD_REQUIRED ON) # OFF default # prevent features GCC only set(CMAKE_CXX_EXTENSIONS OFF) # set project info project(project_name LANGUAGES language_list(such as C CXX ASM...)) Linkable library add_executable(main mian.cpp mylib.cpp) or generate a static library add_library(mylib STATIC mylib.cpp) # create libmylib.a add_executable(main main.cpp) target_link_libraries(main PUBLIC mylib) or generate dynamic lib add_library(mylib SHARED mylib.cpp) add_executable(main main.cpp) target_link_libraries(main PUBLIC mylib) or use object lib, no *.a file, let CMake remember which objects files are created add_library(mylib OBJECT mylib.cpp) add_executable(main main.cpp) target_link_libraries(main PUBLIC mylib) 静态库问题: GCC会自行剔除没有引用符号的对象, 此时使用对象库避免, 从而不会自动剔除没引用到的对象文件, 绕开编译器不统一问题. 动态库也可以避免剔除没引用的对象文件, 但引入了运行时链接的麻烦. # no specify variable in add_library() set(BUILD_SHARED_LIBS ON) # default OFF add_library(mylib mylib.cpp) HINT 静态库常常被认为直接链接到可执行文件上. 因此在动态库中不要链接静态库. 很呆. 地址会变. 当然解决方法是: 要么转化为对象库, 要么让静态库变成地址无关的代码PIC # set global property set(CMAKE_POSITION_INDEPENDENT_CODE ON) add_library(otherlib STATIC otherlib.cpp) add_library(mylib SHARED mylib.cpp) target_link_libraries(mylib PUBLIC otherlib) add_executable(main main.cpp) target_link_libraries(main PUBLIC mylib) or set local property # set local property add_library(otherlib STATIC otherlib.cpp) set_property(TARGET otherlib PROPERTY POSITION_INDEPENDENT_CODE ON) add_library(mylib SHARED mylib.cpp) target_link_libraries(mylib PUBLIC otherlib) add_execuable(main main.cpp) target_link_libraries(main PUBLIC mylib) Attributes of objects 设置单属性: set_property(TARGET ... PROPERTY ...); 设置多属性: set_target_properties(file_name PROPERTIES properties_list) HINT: 以上命令在add_executable后有效. 设置全局属性 (改变属性的默认值): set(CMAKE_XXX), 在add_executable前设置. 如果需要在Windows下面使用动态库 (Windows对动态链接不友好), 则需要在定义和声明添加: Deffinition: #include &lt;cstdio> #ifdef _MSC_VER __declspec(dllexport) #endif void sayy_hello()&#123;&#125; Declaration: #pragma once #ifdef _MSC_VER __declspec(dllimport) #endif void say_hello(); 然后CMakeLists.txt这样写: # In Main dir cmake_minimum_required(VERSION 3.22) add_subdirectory(mylib) # add sub module add_executable(main main.cpp) target_link_libraries(main PUBLIC mylib) # In sub module dir add_library(mylib SHARED mylib.cpp mylib.h) 然后Windows极有可能会报错: 运行时找不到dll; 原因是dll和exe不在同一目录 (Windows只会查找exe所在目录和PATH). - 把dll添加到PATH环境变量 - 或者dll和dll其他的所有依赖dll, 全部拷贝到exe同一目录 这是因为CMake把main放在build下, 而mylib放在build/mylib/mylib.dll 因此重定向输出路径, 改变mylib属性, 让.dll文件输出到 PROJECT_BINARY_DIR 里面. set_property(TARGET mylib PROPERTY RUNTIME_OUTPUT_DIRECTORY_(DEBUG | RELEASE | NONE) | ARCHIVE_OUTPUT_DIRECTORY | LIBRARY_OUTPUT_DIRECTORY $&#123;PROJECT_BINARY_DIR&#125;) Externel library In Linux: feel free to link externel libraries. (/usr/lib/...) But Windows can't. Linux can also include head file directly (/usr/include/...). HINT: CMake 的分隔符永远是 \"/\", 即使是Windows, CMake会自动转化. More general method: find_package(package_name REQUIRED) 没听懂, 以后补, 以后也不想补. Variables and Outputs output some log infomation when running cmake -B build, used for debugging. message(\"log info\") message(STATUS \"status info\") # -- prefix message(WARNING \"warning info\") # yellow message(SEND_ERROR \"error info\") # send error log but continue to run message(FATAL_ERROR \"error info\") # print error and stop running Variable and Cache 重复执行cmake -B build: 第一次较慢, 将环境的检测存入缓存, 第二次以及以后直接查看缓存内容. 因此某些错误可以通过删除 ./build/CMakeCache.txt解决. 当然也可以删了整个build文件夹重新编译, 慢一点而已.","categories":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/categories/CS/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://andrew-rey.github.io/tags/C/"},{"name":"Programming","slug":"Programming","permalink":"https://andrew-rey.github.io/tags/Programming/"}],"author":"Andrew-Rey"},{"title":"Algorithm | FFT","slug":"Algorithm/fft","date":"2022-04-06T08:08:39.000Z","updated":"2023-11-15T04:33:48.976Z","comments":true,"path":"2022/04/06/Algorithm/fft/","link":"","permalink":"https://andrew-rey.github.io/2022/04/06/Algorithm/fft/","excerpt":"\"对称，万变不离其中\"","text":"\"对称，万变不离其中\" 多项式乘积问题 首先来思考这样的一个问题: Question 1 你有两个多项式函数: 应该如何计算它们的乘积? 当然, 我不是说要用笔算的方式, 而是用计算机. 显然这个问题我们在小学二年级就写过的, 当初正在学习\"数据结构\"这门课, 如果没记错, 应该是用链表实现的. 但是, 就算是用链表实现, 那不也是和手算一样的原理吗? 将二者相乘 分配律 合并同类项 例如上面那个例子: solution 1 (???是什么动力让我深夜在这里口算多项式乘法???) 显然, 如果一个 n 次多项式乘上一个 m 次多项式, 在合并同类项前应该有 次多项式, 这谁顶得住? 对于正常人类而言显然顶不住, 对于计算机而言, 时间复杂度是, 也是算比较大的开销了吧. 咋办? 点表示法 开始 有谁规定, 我多项式一定是用系数表示的? 好家伙, 你这样说我就摸不着头脑了, 难道除了系数表示还有其他表示方法吗? 首先, 多项式集合其实是构成了一个线性空间, 也就是说, 任意两个多项式进行线性运算 (加法和数乘) 后, 结果仍然是多项式. 事实上 构成了该空间的一组基, 将函数展开成 Taylor 级数便用了这组基作为基底, 基前面的系数也就是坐标. 其次, 对于一个 n 次多项式而言, 只要我们确定了它的坐标, 就能唯一确定这个多项式. 现在的问题是不知道坐标, 如何确定多项式. 这里的巧妙之处就在于, 多项式函数是一个映射, 对于一个特定的 x, 总是能给出唯一一个值与之对应, 这不就是一个方程吗? 我给你一个 x, 你输出一个值, 同时由于多项式系数全部未知, 这就是一个关于 个系数的方程 显然, 我需要 个不同的点来唯一确定我的系数. 这就是所谓的点表示法. 这样一来, 我们将这 个方程写成矩阵形式: 看到这里我终于理解了为什么在学高等代数时要突然讲一个范德蒙德(Vandermonde)行列式, 也就是这里的 将上述矩阵定义为我们最喜欢的字母. 好, 既然这东西是范德蒙德行列式, 那我们可以知道它行列式不为 0, 也就是说, 这个矩阵是可逆的, 也就是当我们取 个不同点时, 确实是可以使方程组有唯一解, 也就是 个点可以唯一表示一个 n 次多项式. 乘法 问题来了, 如何做乘法? 我们有 n 次多项式和 m 次多项式做乘法, 得到的是一个 次多项式, 那么我们只要找到 个点即可, 也就是只要在 n 次多项式和 m 次多项式中分别找 个点, 这些点的横坐标 x 相等, 再将对应的函数值相乘即可. 进一步 现在, 我们知道了如何用点表示多项式, 以及如何用点表示进行乘法运算. 但是仔细一想, 这种方法需要求解线性方程组, 这里的计算复杂度并不低. 也就是从系数表示法到点表示法的转化过程带来的计算复杂度还是很高的. 有什么方法可以进行简化吗? 先等一等, 我们先来梳理我们用点表示求多项式乘法的思路: MainIdea 将 n 次多项式和 m 次多项式分别从系数表示转化为点表示 对应点相乘 将得到的 个点表示的多项式转化为系数表示 奇偶 先来考虑简单的情况: Question 2 多项式 和多项式 用点表示法相乘 那我们当然是按部就班地进行乘法啦~ - 由于结果是 5 次多项式, 因此对取 5 个点, 对取 5 个点. 取点, 说得轻巧, 做起来倒是挺犹豫的. 取什么样的点能满足要求呢? 或者得寸进尺地说, 什么样的点能让效率更高呢? 注意到二次函数是对称的, 那我们是不是只要取正的 2 个点, 就能知道负的 2 个点, 另外加一个原点? 确实如此. 那三次多项式呢? 照理来说, 我们同样也是只要取一半的点就能知道另一半点的值(这里的\"一半\"针对正负而言), 只不过要在函数值上添加负号, 何必呢? 还不如干脆 提出一个 x, 然后不也变成了二次函数? 事实上, 一般而言, 我们要用点表示法表示多项式, 可以用如下方法: Method1 其中, 表示只含偶次的多项式函数, 表示只含奇次的多项式. 这样, 我们只要在非负轴上取值就可以确定整个多项式, 取点的个数是 原来的一半. 甚至, 这里形成了一个 递归 算法: 分解后的不也是一个关于 x 的多项式吗?! 那我继续啊, 把继续分解啊, 大事化小, 小事化了. 等等! 我们的其实是, 这里每个都是非负的啊. 未来我们只能在非负轴取值了, 也就是说, 分解为偶次多项式后, 递归停止了. 完蛋. 复数域分解 \"山重水复疑无路, 柳暗花明又一村\" 看到标题就已经知道要怎么做了. 既然在实数域上无法继续分解, 那为何不去复数域呢? 在复数域上我们可以快乐地进行递归. 如何个快乐法呢? 我们来细品: 偶次多项式在复平面上的根 为什么突然变成了 求根? 从第二节中\"奇偶\", 我们可以选取对称的点, 来减少选取点的个数(即原来的一半). 接着我们把任意 n 次多项式分解成两个偶次多项式, 偶次多项式的好处在于容易选取对称的点. 但是由于在实数范围内, 在对偶次多项式进行递归时会发生中断, 于是我们扩展至复数域讨论分解. 方便起见: 对于, 我们取作为特征点, 对于, 我们取作为三个特征点, 那对于, 我们应该怎样取点, 抛开不谈, 令, 由 代数基本定理, 该方程在复数域上有 4 个 根, 对于其它偶次多项式我们以此类推. 就这样, 我们找到了一个简单的方法寻找所有需要的点, 进行递归. 单位根 写到这里, 我也感觉有点吃力, 关键是为什么一定就取了令呢? 虽然但是, 确实是所谓的\"方便起见\", 这是因为, 取了\"1\", 我们可以在复平面上的单位圆上讨论这个问题. 在小学二年级我们就知道, 的根可以用我们熟悉的的幂来表示, 即 这些个点在复平面单位圆上 对称分布. 每递归一次, 单位根的数量减少一半, 但保持对称性不变. 确实方便. 快速傅里叶变换(FFT) 终于能正式地介绍世界上最美丽的算法了: 快速傅里叶变换(FFT). FFT解决的是多项式从系数表示到点表示的过程中, 计算复杂度的问题. 框架 分解: 递归: 加和: 返回 时间复杂度为: 一些数学 我们在复数域上考虑, 令 (这是因为, 我们希望多项式在复数域上考虑时, 我们可以在单位圆周上讨论. 其中表示我们取的第 k 个点, 刚好与 是对应的.) 则线性方程组可以化为: 其中 称为离散傅里叶变换矩阵(DFT)显然该矩阵是 对称的 且 可逆, 其逆矩阵为: 并且, 该逆矩阵看起来和原矩阵 一模一样! . 结束了? 当我们乐呵呵地把FFT转化为代码时, 开心的分解多项式, 然后选点, 相乘, 等等! 你还没告诉我, 怎么从点表示转化回系数表示呢! 这就是FFT对称的魅力了. 由点求系数, 不过是矩阵求逆的过程: 显然, 由于DFT和DFT逆矩阵具有相似的形式, 我们完全可以用同一个函数完成快速傅里叶的正反变换!","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://andrew-rey.github.io/categories/Algorithm/"}],"tags":[{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://andrew-rey.github.io/tags/Algorithm/"}],"author":"Andrew-Rey"},{"title":"Math in a Mess | Space","slug":"Math/space","date":"2022-03-19T17:20:10.000Z","updated":"2023-11-11T12:51:29.657Z","comments":true,"path":"2022/03/20/Math/space/","link":"","permalink":"https://andrew-rey.github.io/2022/03/20/Math/space/","excerpt":"\"内积空间和度量空间有什么区别? Hilbert空间是什么? 它与线性空间的关系是什么?\" \"我已经晕了.\"","text":"\"内积空间和度量空间有什么区别? Hilbert空间是什么? 它与线性空间的关系是什么?\" \"我已经晕了.\" 数域 是包含0, 1的数集, 且对 中任意两个数的加减乘除运算封闭, 则称 是一个数域. 线性空间 在数域的基础上, 我们提出线性空间的概念: 给定数域 , 和集合 . 有如下映射: 且 () 满足八条基本性质, 则称为一个线性空间. 赋范空间 赋范空间是定义在线性空间之上的. 定义在数域 的线性空间 存在如下映射: 且该映射满足: 正定, 齐次, 三角不等式. 则 是一个赋范空间, 其中映射 称为范数. 内积空间 内积空间是定义在线性空间之上的. 定义在数域 的线性空间 存在如下映射: 则 是一个内积空间. 定义了内积后, 我们可以讨论向量 (即线性空间的元素) 间的长度和夹角, 并进一步讨论正交性等. 注意: 内积本身具有自然定义的范数, 即内积可以诱导出范数, , 因此内积空间含于赋范空间. 度量空间 度量空间是某个具有距离函数的集合. 该函数定义的是集合内所有元素的距离, 即集合上的某种度量, 即: 给定集合, 有映射: 满足: [ ] 注意: 此处并未要求线性结构. 注意: 赋范空间一定可以诱导出度量空间, 因此赋范空间含于度量空间 完备空间 完备空间又称 Cauchy 空间. 完备空间是定义在度量空间之上的. 若度量空间 中所有的柯西序列都收敛在 中的一点, 则 是一个完备空间. Hilbert空间 在内积空间的基础上增添完备性条件, 即得到Hilbert空间. 总结 范数运算+向量空间=(线性)赋范空间 (线性)赋范空间 + 内积运算=内积空间 (线性)赋范空间 + 完备性 = Banach 空间 内积空间 + 完备性 = Hilbert 空间 内积空间 + 完备性 + 有限维 = Euclidean 空间 References zhihu: https://www.zhihu.com/question/332144499/answer/731866608 https://www.zhihu.com/question/42312263/answer/699451330 wikipedia: https://en.wikipedia.org/wiki/Complete_metric_space https://en.wikipedia.org/wiki/Metric_space https://en.wikipedia.org/wiki/Cauchy_sequence https://en.wikipedia.org/wiki/Cauchy_sequence https://en.wikipedia.org/wiki/Cauchy_sequence https://en.wikipedia.org/wiki/Normed_vector_space","categories":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/categories/Math/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/tags/Math/"}]},{"title":"first blog | Hello World","slug":"Life/first","date":"2022-01-11T01:09:04.000Z","updated":"2023-11-11T12:51:00.082Z","comments":true,"path":"2022/01/11/Life/first/","link":"","permalink":"https://andrew-rey.github.io/2022/01/11/Life/first/","excerpt":"Finished! My First Blog! After a long time deploying my blog webpage and a lot of other borthering settings, I finally finished it! I mean, FINALLY!!! :laughing: :laughing: :laughing:","text":"Finished! My First Blog! After a long time deploying my blog webpage and a lot of other borthering settings, I finally finished it! I mean, FINALLY!!! :laughing: :laughing: :laughing: Original Intention Can a programmer has no personal blog? I have seen many blogers writing their own blogs no metter answering a question or just taking notes from time to time on websites such as zhihu and csdn, but among which I prefer is to establish a personal website where I can put my blogs on. So, at first I have no intention about what to do with my site, maybe I just feel that it's really cool to have such a lovely home for oneself to \"lie down and rest\". But when it was finally established by myself, experencing a lot of confusing problems and taking amount of time to debug, I must to say that, I love here, and I believe I will take after it like taking after a baby, a baby who are growing up. :blush: Thanks I would not finish my work without the help of JerryYang, whose helpful blog is the guidance of mine (though there are still some mistakes maybe? :dizzy_face:). Based on it, I have known some basic command with Linux, Git and Github, which is also beneficial for my lessons next term. Except him I want to link some videos there to thank for another ups from bilibili: using hexo to start blog how to writing blogs","categories":[{"name":"Life","slug":"Life","permalink":"https://andrew-rey.github.io/categories/Life/"}],"tags":[{"name":"Blog","slug":"Blog","permalink":"https://andrew-rey.github.io/tags/Blog/"},{"name":"Life","slug":"Life","permalink":"https://andrew-rey.github.io/tags/Life/"}],"author":"Andrew-Rey"}],"categories":[{"name":"Paper","slug":"Paper","permalink":"https://andrew-rey.github.io/categories/Paper/"},{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/categories/DL/"},{"name":"Dl","slug":"Dl","permalink":"https://andrew-rey.github.io/categories/Dl/"},{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/categories/Math/"},{"name":"Papers","slug":"Papers","permalink":"https://andrew-rey.github.io/categories/Papers/"},{"name":"Programming Language","slug":"Programming-Language","permalink":"https://andrew-rey.github.io/categories/Programming-Language/"},{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/categories/SE/"},{"name":"Profile","slug":"Profile","permalink":"https://andrew-rey.github.io/categories/Profile/"},{"name":"Project","slug":"Project","permalink":"https://andrew-rey.github.io/categories/Project/"},{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/categories/CS/"},{"name":"ML","slug":"ML","permalink":"https://andrew-rey.github.io/categories/ML/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://andrew-rey.github.io/categories/Algorithm/"},{"name":"Life","slug":"Life","permalink":"https://andrew-rey.github.io/categories/Life/"}],"tags":[{"name":"Paper","slug":"Paper","permalink":"https://andrew-rey.github.io/tags/Paper/"},{"name":"STR","slug":"STR","permalink":"https://andrew-rey.github.io/tags/STR/"},{"name":"Deep learing","slug":"Deep-learing","permalink":"https://andrew-rey.github.io/tags/Deep-learing/"},{"name":"Diffusion model","slug":"Diffusion-model","permalink":"https://andrew-rey.github.io/tags/Diffusion-model/"},{"name":"Deep learning","slug":"Deep-learning","permalink":"https://andrew-rey.github.io/tags/Deep-learning/"},{"name":"loss function","slug":"loss-function","permalink":"https://andrew-rey.github.io/tags/loss-function/"},{"name":"super-resolution","slug":"super-resolution","permalink":"https://andrew-rey.github.io/tags/super-resolution/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://andrew-rey.github.io/tags/deep-learning/"},{"name":"TATT","slug":"TATT","permalink":"https://andrew-rey.github.io/tags/TATT/"},{"name":"scene text","slug":"scene-text","permalink":"https://andrew-rey.github.io/tags/scene-text/"},{"name":"tutorial","slug":"tutorial","permalink":"https://andrew-rey.github.io/tags/tutorial/"},{"name":"lhy-ML","slug":"lhy-ML","permalink":"https://andrew-rey.github.io/tags/lhy-ML/"},{"name":"schedual","slug":"schedual","permalink":"https://andrew-rey.github.io/tags/schedual/"},{"name":"Math","slug":"Math","permalink":"https://andrew-rey.github.io/tags/Math/"},{"name":"differentiation","slug":"differentiation","permalink":"https://andrew-rey.github.io/tags/differentiation/"},{"name":"paper","slug":"paper","permalink":"https://andrew-rey.github.io/tags/paper/"},{"name":"palm","slug":"palm","permalink":"https://andrew-rey.github.io/tags/palm/"},{"name":"Number Theory","slug":"Number-Theory","permalink":"https://andrew-rey.github.io/tags/Number-Theory/"},{"name":"Racket","slug":"Racket","permalink":"https://andrew-rey.github.io/tags/Racket/"},{"name":"Functional Programming","slug":"Functional-Programming","permalink":"https://andrew-rey.github.io/tags/Functional-Programming/"},{"name":"Full stack","slug":"Full-stack","permalink":"https://andrew-rey.github.io/tags/Full-stack/"},{"name":"Framework","slug":"Framework","permalink":"https://andrew-rey.github.io/tags/Framework/"},{"name":"RuoYi","slug":"RuoYi","permalink":"https://andrew-rey.github.io/tags/RuoYi/"},{"name":"welcome","slug":"welcome","permalink":"https://andrew-rey.github.io/tags/welcome/"},{"name":"clip","slug":"clip","permalink":"https://andrew-rey.github.io/tags/clip/"},{"name":"DL","slug":"DL","permalink":"https://andrew-rey.github.io/tags/DL/"},{"name":"Multimodule","slug":"Multimodule","permalink":"https://andrew-rey.github.io/tags/Multimodule/"},{"name":"CS","slug":"CS","permalink":"https://andrew-rey.github.io/tags/CS/"},{"name":"SE","slug":"SE","permalink":"https://andrew-rey.github.io/tags/SE/"},{"name":"Java","slug":"Java","permalink":"https://andrew-rey.github.io/tags/Java/"},{"name":"Disgn Pattern","slug":"Disgn-Pattern","permalink":"https://andrew-rey.github.io/tags/Disgn-Pattern/"},{"name":"Design Pattern","slug":"Design-Pattern","permalink":"https://andrew-rey.github.io/tags/Design-Pattern/"},{"name":"Full Stack","slug":"Full-Stack","permalink":"https://andrew-rey.github.io/tags/Full-Stack/"},{"name":"Springboot","slug":"Springboot","permalink":"https://andrew-rey.github.io/tags/Springboot/"},{"name":"Vue","slug":"Vue","permalink":"https://andrew-rey.github.io/tags/Vue/"},{"name":"Project","slug":"Project","permalink":"https://andrew-rey.github.io/tags/Project/"},{"name":"Microservice","slug":"Microservice","permalink":"https://andrew-rey.github.io/tags/Microservice/"},{"name":"reverse analysis","slug":"reverse-analysis","permalink":"https://andrew-rey.github.io/tags/reverse-analysis/"},{"name":"Unity","slug":"Unity","permalink":"https://andrew-rey.github.io/tags/Unity/"},{"name":"PCG","slug":"PCG","permalink":"https://andrew-rey.github.io/tags/PCG/"},{"name":"Terrain","slug":"Terrain","permalink":"https://andrew-rey.github.io/tags/Terrain/"},{"name":"C++","slug":"C","permalink":"https://andrew-rey.github.io/tags/C/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://andrew-rey.github.io/tags/Deep-Learning/"},{"name":"Programming","slug":"Programming","permalink":"https://andrew-rey.github.io/tags/Programming/"},{"name":"Algorithm","slug":"Algorithm","permalink":"https://andrew-rey.github.io/tags/Algorithm/"},{"name":"Blog","slug":"Blog","permalink":"https://andrew-rey.github.io/tags/Blog/"},{"name":"Life","slug":"Life","permalink":"https://andrew-rey.github.io/tags/Life/"}]}