<!DOCTYPE html>
<html lang="en">
  <head><meta charset="UTF-8"/>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="description" content="Image Semantic Segmentation based on UNet"/><meta name="keywords" content="CS, Deep Learning, Andrew-Rey" /><link rel="alternate" href="/disable" title="Andrew-Rey" ><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=2.11.1" />
<link rel="canonical" href="https://Andrew-Rey.github.io/2022/08/21/ML/UNet/"/>

<link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" /><script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<link rel="stylesheet" type="text/css" href="/css/style.css?v=2.11.1" />

<script>
  window.config = {"leancloud":{"app_id":null,"app_key":null},"toc":true,"fancybox":true,"pjax":"","latex":true};
</script>

    <title>Image Semantic Segmentation based on UNet - Andrew-Rey</title>
  <meta name="generator" content="Hexo 6.1.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="mobile-header-logo">
    <a href="/." class="logo">Andrew-Rey</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list"><a href="/">
        <li class="mobile-menu-item">Home
          </li>
      </a><a href="/archives/">
        <li class="mobile-menu-item">Archives
          </li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags
          </li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories
          </li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About
          </li>
      </a></ul>
</nav>
<div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Andrew-Rey</a>
</div>

<nav class="site-navbar"><ul id="menu" class="menu"><li class="menu-item">
          <a class="menu-item-link" href="/">
            Home
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            Archives
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/tags/">
            Tags
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/categories/">
            Categories
            </a>
        </li>
      <li class="menu-item">
          <a class="menu-item-link" href="/about/">
            About
            </a>
        </li>
      </ul></nav>
</header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content"><article class="post">
    <header class="post-header">
      <h1 class="post-title">Image Semantic Segmentation based on UNet
        </h1>

      <div class="post-meta">
        <span class="post-time">
          2022-08-21
        </span><span class="post-category">
            <a href="/categories/ML/">ML</a>
            </span>
        </div>
    </header>

    <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">Contents</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#unet-structure"><span class="toc-text">UNet Structure</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#operator-definitions"><span class="toc-text">Operator Definitions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#network-definition"><span class="toc-text">Network Definition</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#code"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#result"><span class="toc-text">Result</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#former-model"><span class="toc-text">Former Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model-upgrade"><span class="toc-text">Model Upgrade</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#analysis"><span class="toc-text">Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#u-net"><span class="toc-text">U-Net</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nested-unet"><span class="toc-text">Nested UNet</span></a></li></ol></li></ol>
    </div>
  </div><div class="post-content"><p>"Semantic segmentation of images, use UNet model."</p>
<span id="more"></span>
<h1 id="abstract">Abstract</h1>
<p>In this project, we realize an basic UNet model and UNet++ model,
then we apply them on image semantic segmentation. We show our basic
theory of UNet and an improvement of it, and we provide main code of
this program. Finally, we give the result of segmentation images,
loss-curve and accuracy-curve on both training and validation set.</p>
<p>The copyright of this program is owned by our team mentioned on the
end of this blog.</p>
<h1 id="unet-structure">UNet Structure</h1>
<p>The <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.04597">paper</a> published in
2015 propose a noval network structure, whose shape is similar with the
captal "U". The idea comes from FCNN. U-Net is one of the classes of
"Encoder-Decoder" structure.</p>
<figure>
<img src="unet-structure.png" alt="U-Net Structure" />
<figcaption aria-hidden="true">U-Net Structure</figcaption>
</figure>
<p>The front half of the network is "encoder". The input image passes
covolutional kernel, and then passes the pooling layer (or other
dimension-decreasing layer). The opposite of that is the back part of
UNet, the "decoder". The input of decoder is a sequence of feature maps
with highly contracted pixels. The output of the decoder (or the whole
network) is an image with the same shape of input image, where each
pixel has its own class.</p>
<p>In this project, we decrease the number of convolutional layers so
that there are only two convolutional layers in each convolutional
kernel as the dataset includes images with shape <span
class="math inline">\(128\times 256\)</span>.</p>
<h2 id="operator-definitions">Operator Definitions</h2>
<p><strong>Convolutional Kernel:</strong></p>
<p>We define the basic convolutional kernel as follow:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">self.layer = nn.Sequential(</span><br><span class="line">    <span class="comment"># in_channel, out_channel, kernel_size, stride, padding</span></span><br><span class="line">    <span class="comment"># batch size * channel * height * weight</span></span><br><span class="line">    nn.Conv2d(C_in, C_out, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="number">1</span>),  <span class="comment"># 64 64 128 256</span></span><br><span class="line">    nn.BatchNorm2d(C_out),</span><br><span class="line">    nn.Dropout(<span class="number">0.2</span>),</span><br><span class="line">    nn.LeakyReLU(),</span><br><span class="line"></span><br><span class="line">    nn.Conv2d(C_out, C_out, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="number">1</span>),  <span class="comment"># 64 64 128 256</span></span><br><span class="line">    nn.BatchNorm2d(C_out),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nn.LeakyReLU(),</span><br></pre></td></tr></table></figure>
<p>It includes two convolution operations.</p>
<p><strong>Down Sampling Kernel:</strong></p>
<p>As for downsampling kernel, we replace conditional pooling layer to
convolutional layer with stride equaling to 2, which means the shape
will be shrunk to <span class="math inline">\(\frac{1}{2}\)</span> while
remaining the same channels.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.Down = nn.Sequential(</span><br><span class="line">    nn.Conv2d(C, C, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="number">1</span>),  <span class="comment"># 64 64 64 128</span></span><br><span class="line">    nn.LeakyReLU()</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p><strong>Up Sampling Kernel:</strong></p>
<p>The basic structure of up-sampling contains only one convolutional
layer with <span class="math inline">\(1\times 1\)</span> convolutional
kernel size and half out-channel. The feature map should pass an
interpolation layer before getting into the convolutional layer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, C</span>):</span><br><span class="line">    <span class="built_in">super</span>(UpSampling, self).__init__()</span><br><span class="line">    <span class="comment"># out-channel = 1/2 in-channel</span></span><br><span class="line">    self.Up = nn.Conv2d(C, C // <span class="number">2</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, r</span>):</span><br><span class="line">    <span class="comment"># neighbor interpolation</span></span><br><span class="line">    up = F.interpolate(x, scale_factor=<span class="number">2</span>, mode=<span class="string">&quot;nearest&quot;</span>)</span><br><span class="line">    x = self.Up(up)</span><br><span class="line">    <span class="comment"># concatenate the feature map in encoder and </span></span><br><span class="line">    <span class="comment"># the feature map in corrsponding decoder layer, in channel dimension</span></span><br><span class="line">    res = torch.cat((x, r), <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>The interpolation mode we choose is "nearest". The function
<code>torch.cat(dim=1)</code> is used to concatenate two feature maps in
channel dimension.</p>
<h2 id="network-definition">Network Definition</h2>
<p>Based on the operators defined above, we link these blocks together
like UNet structure.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(UNet, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># down sampling</span></span><br><span class="line">    self.C1 = Conv(<span class="number">3</span>, <span class="number">64</span>)</span><br><span class="line">    self.D1 = DownSampling(<span class="number">64</span>)</span><br><span class="line">    self.C2 = Conv(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">    self.D2 = DownSampling(<span class="number">128</span>)</span><br><span class="line">    self.C3 = Conv(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">    self.D3 = DownSampling(<span class="number">256</span>)</span><br><span class="line">    self.C4 = Conv(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">    self.D4 = DownSampling(<span class="number">512</span>)</span><br><span class="line">    self.C5 = Conv(<span class="number">512</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># up sampling</span></span><br><span class="line">    self.U1 = UpSampling(<span class="number">1024</span>)</span><br><span class="line">    self.C6 = Conv(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">    self.U2 = UpSampling(<span class="number">512</span>)</span><br><span class="line">    self.C7 = Conv(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line">    self.U3 = UpSampling(<span class="number">256</span>)</span><br><span class="line">    self.C8 = Conv(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">    self.U4 = UpSampling(<span class="number">128</span>)</span><br><span class="line">    self.C9 = Conv(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    self.C10 = torch.nn.Conv2d(<span class="number">64</span>, <span class="number">3</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="number">1</span>)</span><br><span class="line">    self.pred = torch.nn.Conv2d(<span class="number">3</span>, <span class="number">34</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    self.Th = torch.nn.Sigmoid()</span><br></pre></td></tr></table></figure>
<p>Like U-Net mentioned in that paper, we designed 4 layer deep
network.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># part 1: down sampling, decreasing dimension</span></span><br><span class="line">        R1 = self.C1(x)</span><br><span class="line">        R2 = self.C2(self.D1(R1))</span><br><span class="line">        R3 = self.C3(self.D2(R2))</span><br><span class="line">        R4 = self.C4(self.D3(R3))</span><br><span class="line">        Y1 = self.C5(self.D4(R4))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># part 2: up sampling, connect priori knowledge</span></span><br><span class="line">        O1 = self.C6(self.U1(Y1, R4))</span><br><span class="line">        O2 = self.C7(self.U2(O1, R3))</span><br><span class="line">        O3 = self.C8(self.U3(O2, R2))</span><br><span class="line">        O4 = self.C9(self.U4(O3, R1))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># part 3: active function</span></span><br><span class="line">        <span class="keyword">return</span> self.Th(self.pred(self.C10(O4)))</span><br></pre></td></tr></table></figure>
<p>As you can see, the difference between U-Net and other networks
before U-Net is that U-Net conbines the former information from encoder
and current information from decoder.</p>
<h1 id="code">Code</h1>
<p>During the training process, we want to keep some information of loss
values and accuracy values on training set and validation set so that we
can analyze the variance.</p>
<p>In the function named <code>train()</code>, we take
<code>optimizer</code> and <code>loss</code> as two parameters used in
training process. The outputs of this function are loss and accuracy on
both training set and validation set. If we get the data about training
set and validation set, we can draw the curves. If both training and
validation loss values decrease during training process, we can conclude
that our model converges and does not overfit on training set.</p>
<p>The training code is shown as follow:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> self.train_loader:</span><br><span class="line">    batch_num += <span class="number">1</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    rgbs, segs = batch</span><br><span class="line">    s, _, m, n = segs.shape</span><br><span class="line">    segs = torch.reshape(segs, (s, m, n))</span><br><span class="line">    pred_segs = self.model(rgbs).to(self.device)</span><br><span class="line">    loss_val = loss(pred_segs, segs)</span><br><span class="line">    loss_val.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>The data collecting code can be written as follow:</p>
<p><strong>Statistic data of training set</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> ... :</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> batch_num % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            logging.info(<span class="string">f&quot;batch num <span class="subst">&#123;batch_num&#125;</span>, loss <span class="subst">&#123;loss_val&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># delete or add comments when needed</span></span><br><span class="line">        train_loss += loss_val</span><br><span class="line">        <span class="comment"># statistic valid classified samples</span></span><br><span class="line">        total_pix += s * m * n</span><br><span class="line">        idx = torch.argmax(pred_segs, dim=<span class="number">1</span>)</span><br><span class="line">        train_valid_pix += torch.eq(idx, segs).<span class="built_in">sum</span>().<span class="built_in">float</span>().item()</span><br><span class="line">torch.cuda.empty_cache()</span><br><span class="line">epoch_acc = train_valid_pix / total_pix</span><br><span class="line">train_epoch_loss.append(train_loss / batch_num)</span><br><span class="line">train_epoch_acc.append(epoch_acc)</span><br></pre></td></tr></table></figure>
<p><strong>Statistic data of validation set</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">self.model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> valid_batch <span class="keyword">in</span> self.valid_loader:</span><br><span class="line">        valid_batch_num += <span class="number">1</span></span><br><span class="line">        rgbs, segs = valid_batch</span><br><span class="line">        s, _, m, n = segs.shape</span><br><span class="line">        segs = torch.reshape(segs, (s, m, n))</span><br><span class="line">        pred_segs = self.model(rgbs).to(self.device)</span><br><span class="line">        loss_val = loss(pred_segs, segs)</span><br><span class="line">        valid_loss += loss_val</span><br><span class="line">        valid_total_pix += s * m * n</span><br><span class="line">        idx = torch.argmax(pred_segs, dim=<span class="number">1</span>)</span><br><span class="line">        valid_valid_pix += torch.eq(idx, segs).<span class="built_in">sum</span>().<span class="built_in">float</span>().item()</span><br><span class="line">epoch_acc = valid_valid_pix / valid_total_pix</span><br><span class="line">valid_epoch_loss.append(valid_loss / valid_batch_num)</span><br><span class="line">valid_epoch_acc.append(epoch_acc)</span><br></pre></td></tr></table></figure>
<p>The point you should pay attention to is that you should use
<code>with torch.no_grad()</code> before you do some work that have no
relation with training process, otherwise your GPU memory will be full
or even overflow.</p>
<h1 id="result">Result</h1>
<p>After a long time training, we get the satisfying result with U-Net
model.</p>
<h2 id="former-model">Former Model</h2>
<p>The "former model" infers the U-Net model, and you will see we use
other upgraded model named "UNet++" which will be introduced later.</p>
<p>We output the segmentation results and their uncertainties.</p>
<figure>
<img src="pic1-unet.png" alt="picture 1 result-UNet" />
<figcaption aria-hidden="true">picture 1 result-UNet</figcaption>
</figure>
<h2 id="model-upgrade">Model Upgrade</h2>
<p>For some reasons, we try another U-Net-like model, Nested UNet,
namely UNet++. It has a nested convolutional blocks like a pyramid and
there is a chain passing connectivity between each convolutional block
every layer.</p>
<figure>
<img src="nested.png" alt="Neseted UNet" />
<figcaption aria-hidden="true">Neseted UNet</figcaption>
</figure>
<p>The black nodes are the same with U-Net model. The green nodes are
what Nested UNet newly added. Both green and blue lines are skip
pathways that pass connectivities from encoder to decoder.</p>
<p>The use of Nested UNet gives us a little improvement on final
results.</p>
<figure>
<img src="pic1-nested.png" alt="pictrue 1 result-Nested UNet" />
<figcaption aria-hidden="true">pictrue 1 result-Nested UNet</figcaption>
</figure>
<h1 id="analysis">Analysis</h1>
<h2 id="u-net">U-Net</h2>
<p>We analyze the loss value and accuracy on both training and
validation set:</p>
<figure>
<img src="unet_loss.png" alt="unet loss" />
<figcaption aria-hidden="true">unet loss</figcaption>
</figure>
<p>We find that after 100 epochs, the model has not convergenced yet,
but the loss on validation decreases to the bottom.</p>
<figure>
<img src="unet_acc.png" alt="unet accuracy" />
<figcaption aria-hidden="true">unet accuracy</figcaption>
</figure>
<p>From the accuracy curves, we find that both training set and
validation set have increasing accuracy, which means our model does not
overfit.</p>
<h2 id="nested-unet">Nested UNet</h2>
<p>Meanwhile, we analyze the loss and accuracy of Nested UNet model on
both training and validation set.</p>
<figure>
<img src="nested_loss.png" alt="nested loss" />
<figcaption aria-hidden="true">nested loss</figcaption>
</figure>
<p>We find that Nested UNet has a faster convergency speed than UNet. It
uses only about 60 epochs. But to our surprise, we find that Neseted
UNet overfit after about only 20 epochs because the validation loss does
not decrease anymore.</p>
<figure>
<img src="nested_acc.png" alt="nested accuracy" />
<figcaption aria-hidden="true">nested accuracy</figcaption>
</figure>
<p>The performance on validation accuracy stays the same with UNet
model.</p>

      </div>
      <div class="post-copyright">
    <p class="copyright-item">
      <span>Author: </span>
      <a href="https://Andrew-Rey.github.io"></a>
    </p>
    <p class="copyright-item">
      <span>Link: </span>
      <a href="https://andrew-rey.github.io/2022/08/21/ML/UNet/">https://andrew-rey.github.io/2022/08/21/ML/UNet/</a>
    </p>
    <p class="copyright-item">
      <span>License: </span><a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>
    </p>
  </div>
      <footer class="post-footer">
        <div class="post-tags">
            <a href="/tags/CS/">CS</a>
            <a href="/tags/Deep-Learning/">Deep Learning</a>
            </div>
        
        <nav class="post-nav"><a class="prev" href="/2022/09/01/Android/AndroidBasic/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Android Basic</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    <a class="next" href="/2022/08/10/CS/CMakeTutorial/">
        <span class="next-text nav-default">CMake Tutorial</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    </nav></footer>
    </article></div><div class="comments" id="comments"></div></div>
      </main>

      <footer id="footer" class="footer"><div class="social-links"><a href="mailto:andrewrey.cc@gmail.com" class="iconfont icon-email" title="email"></a>
        <a target="_blank" rel="noopener" href="https://github.com/Andrew-Rey" class="iconfont icon-github" title="github"></a>
        <a target="_blank" rel="noopener" href="https://space.bilibili.com/631454496" class="iconfont icon-bilibili-line" title="bilibili-line"></a>
        </div><div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" target="_blank" rel="noopener" href="https://github.com/ahonn/hexo-theme-even">Even</a>
  </span>

  <span class="copyright-year">&copy;2015 - 2023<span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author"></span>
  </span>
</div>
</footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div><script type="text/javascript" src="/lib/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/src/even.js?v=2.11.1"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
